{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "machine_shape": "hm",
      "gpuType": "A100",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/JoshuaNalla/SNN-with-IFA/blob/main/DFA_version_of_Snn2.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "CbNYPZ5_wvtf"
      },
      "outputs": [],
      "source": [
        "\n",
        "# Import core libraries\n",
        "import math\n",
        "import numpy as np\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "from torchvision import datasets, transforms\n",
        "from torch.utils.data import DataLoader\n",
        "\n",
        "# for evaluation purposes, importing scikit.metric\n",
        "from sklearn.metrics import confusion_matrix"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# def compute_weight_stats(num_neurons, v_mean=8.0, v_second_moment=164.0, alpha=0.066):\n",
        "#     \"\"\"\n",
        "#     Compute weight initialization statistics.\n",
        "#     Paper reference: Appendix C, Equations A.7 and A.8\n",
        "\n",
        "#     Args:\n",
        "#         num_neurons: Number of neurons in layer (N)\n",
        "#         v_mean: Mean input value (v̄)\n",
        "#         v_second_moment: Second moment of input (v̄̄)\n",
        "#         alpha: Constant (0.066)\n",
        "\n",
        "#     Returns:\n",
        "#         w_mean: Mean weight value (W̄_n)\n",
        "#         w_std: Standard deviation (σ_{W_n})\n",
        "#     \"\"\"\n",
        "#     # Equation A.7: W̄_n = (v̄ - 0.8) / (α · N · v̄)\n",
        "#     w_mean = (v_mean - 0.8) / (alpha * num_neurons * v_mean)\n",
        "\n",
        "#     # Equation A.8: W̄̄_n for computing standard deviation\n",
        "#     numerator = (v_second_moment +\n",
        "#                  alpha**2 * (num_neurons - num_neurons**2) * w_mean**2 * v_mean**2 -\n",
        "#                  1.6 * alpha * num_neurons * v_mean * w_mean -\n",
        "#                  0.64)\n",
        "#     denominator = alpha**2 * num_neurons * v_second_moment\n",
        "\n",
        "#     w_second_moment = numerator / denominator\n",
        "\n",
        "#     # Calculate standard deviation from second moment\n",
        "#     w_std = np.sqrt(w_second_moment - w_mean**2)\n",
        "\n",
        "#     return w_mean, w_std\n",
        "\n",
        "\n",
        "# def initialize_feedback_matrix(output_shape, input_shape,\n",
        "#                                 w_mean_next, w_std_next,\n",
        "#                                 gamma=0.1,\n",
        "#                                 num_downstream_layers=1):\n",
        "#     \"\"\"\n",
        "#     Initialize fixed random feedback matrix B.\n",
        "#     Paper reference: Appendix B, Equation A.5\n",
        "\n",
        "#     Args:\n",
        "#         output_shape: Number of neurons this layer projects to\n",
        "#         input_shape: Number of neurons in this layer\n",
        "#         w_mean_next: Mean of weights in next layer\n",
        "#         w_std_next: Std of weights in next layer\n",
        "#         gamma: Scale factor (0.0338 in paper) but using other values\n",
        "#         num_downstream_layers: Number of layers between this and output (D)\n",
        "\n",
        "#     Returns:\n",
        "#         B: Fixed random feedback matrix [input_shape, output_shape]\n",
        "#     \"\"\"\n",
        "#     # Generate random matrix with paper's specific distribution\n",
        "#     # B_n = γ · [W̄_{n+1} + 2√3 · σ_{W_{n+1}} · (rand - 0.5)]\n",
        "\n",
        "#     rand_values = np.random.uniform(0, 1, size=(input_shape, output_shape))\n",
        "\n",
        "#     # Apply paper's formula\n",
        "#     B = w_mean_next + 2 * np.sqrt(3) * w_std_next * (rand_values - 0.5)\n",
        "\n",
        "#     # Apply scaling factor\n",
        "#     B = gamma * B\n",
        "\n",
        "#     # Note: Paper mentions product over downstream layers (∏)\n",
        "#     # For simplicity with 2-layer network, we use single multiplication\n",
        "#     # For deeper networks, you'd multiply B matrices from all downstream layers\n",
        "\n",
        "#     return B.astype(np.float32)\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "feedback_init"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class DFA_LIFLayer(nn.Module):\n",
        "    \"\"\"\n",
        "    LIF layer that outputs spikes over time.\n",
        "    Has fixed random feedback matrix B for DFA training.\n",
        "    \"\"\"\n",
        "    def __init__(self, in_features, units, output_size,\n",
        "                 tau=20.0, dt=0.25, threshold=0.1, t_ref=1.0,\n",
        "                 use_dfa=True, gamma=0.0338):\n",
        "        super().__init__()\n",
        "        self.in_features = in_features\n",
        "        self.units = units\n",
        "        self.output_size = output_size\n",
        "\n",
        "        self.tau = tau\n",
        "        self.dt = dt\n",
        "        self.threshold = threshold\n",
        "        self.t_ref = t_ref\n",
        "        self.use_dfa = use_dfa\n",
        "        self.gamma = gamma\n",
        "\n",
        "        # Trainable weights (like add_weight trainable=True)\n",
        "        self.w = nn.Parameter(torch.empty(in_features, units))\n",
        "        self.b = nn.Parameter(torch.zeros(units))\n",
        "        nn.init.constant_(self.b, 0.2)\n",
        "\n",
        "        # Init roughly like Keras Glorot-ish (you can match your stats init if you want)\n",
        "        nn.init.xavier_uniform_(self.w)\n",
        "\n",
        "        # Refractory steps\n",
        "        self.ref_steps = int(round(self.t_ref / self.dt))\n",
        "\n",
        "        # Fixed random feedback matrix B (like add_weight trainable=False)\n",
        "        if use_dfa:\n",
        "            B_init = torch.randn(units, output_size) * self.gamma\n",
        "            self.register_buffer(\"B\", B_init)  # NOT a Parameter -> not trainable\n",
        "\n",
        "    def forward(self, inputs):\n",
        "        \"\"\"\n",
        "        inputs: [batch, time, in_features]\n",
        "        returns spikes: [batch, time, units]\n",
        "        \"\"\"\n",
        "        batch_size, time_steps, _ = inputs.shape\n",
        "\n",
        "        v = torch.zeros(batch_size, self.units, device=inputs.device)\n",
        "        ref_count = torch.zeros(batch_size, self.units, device=inputs.device)\n",
        "\n",
        "        spikes_out = []\n",
        "\n",
        "        alpha = 1.0 - (self.dt / self.tau)\n",
        "        beta  = self.dt / self.tau\n",
        "\n",
        "        v_max = torch.tensor(0.0, device=inputs.device)\n",
        "        v_mean_accum = 0.0\n",
        "\n",
        "        for t in range(time_steps):\n",
        "            x_t = inputs[:, t, :]\n",
        "\n",
        "            I_t = (x_t @ self.w) + self.b\n",
        "            v = alpha * v + beta * I_t\n",
        "            # print once per forward call (you can comment this out after)\n",
        "\n",
        "            # refractory\n",
        "            v = torch.where(ref_count > 0, torch.zeros_like(v), v)\n",
        "\n",
        "            # spike + reset\n",
        "            spikes = (v >= self.threshold).float()\n",
        "\n",
        "            v = v * (1.0 - spikes)\n",
        "\n",
        "            # update refractory counter\n",
        "            ref_count = ref_count - 1.0\n",
        "            ref_count = torch.where(spikes > 0, torch.ones_like(ref_count) * self.ref_steps, ref_count)\n",
        "            ref_count = torch.clamp(ref_count, min=0.0)\n",
        "\n",
        "            spikes_out.append(spikes)\n",
        "\n",
        "\n",
        "        return torch.stack(spikes_out, dim=1)"
      ],
      "metadata": {
        "id": "3wdjozUNxfMj"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# @title\n",
        "# math function (PyTorch version)\n",
        "\n",
        "import torch\n",
        "import numpy as np\n",
        "\n",
        "def surrogate_gradient_exact(a, h_th=0.1, t_ref=1.0, tau=20.0): # original threshold was 0,4\n",
        "    \"\"\"\n",
        "    Exact surrogate gradient from paper (Appendix D, Equation A.9).\n",
        "    PyTorch version.\n",
        "    \"\"\"\n",
        "    eps = 1e-8\n",
        "\n",
        "    # Compute ratio a / (a - h_th)\n",
        "    ratio = a / (a - h_th + eps)\n",
        "\n",
        "    # Numerator: h_th * t_ref * tau / [a * (a - h_th)]\n",
        "    numerator = h_th * t_ref * tau / (a * (a - h_th) + eps)\n",
        "\n",
        "    # Denominator: [t_ref + tau * log(ratio)]^2\n",
        "    log_term = torch.log(ratio + eps)\n",
        "    denominator = (t_ref + tau * log_term) ** 2 + eps\n",
        "\n",
        "    grad = numerator / denominator\n",
        "\n",
        "    # Only non-zero where a > threshold\n",
        "    grad = torch.where(a > h_th, grad, torch.zeros_like(grad))\n",
        "\n",
        "    return grad\n",
        "\n",
        "\n",
        "def surrogate_gradient_fast_sigmoid(a, threshold=0.1, alpha=5): # original threshold was 0,4\n",
        "    \"\"\"\n",
        "    Fast sigmoid surrogate gradient (PyTorch version).\n",
        "    \"\"\"\n",
        "    shifted = a - threshold\n",
        "    grad = 1.0 / (1.0 + torch.abs(alpha * shifted)) ** 2\n",
        "    return grad"
      ],
      "metadata": {
        "id": "6HQjJ0LhMW6P"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Cell A: PyTorch model (matches your Keras topology)\n",
        "class DFASNN(nn.Module):\n",
        "    def __init__(self, time_steps, input_size, hidden_size, hidden_size2, output_size):\n",
        "        super().__init__()\n",
        "        self.time_steps = time_steps\n",
        "\n",
        "        # Hidden layer with DFA feedback matrix B\n",
        "        self.hidden_layer = DFA_LIFLayer(\n",
        "            in_features=input_size,\n",
        "            units=hidden_size,\n",
        "            output_size=output_size,\n",
        "            tau=20.0,\n",
        "            dt=0.25,\n",
        "            threshold=0.08, # play with this (original was 0.4)\n",
        "            t_ref=1.0,\n",
        "            use_dfa=True,\n",
        "            gamma=0.0338\n",
        "        )\n",
        "\n",
        "        # second hidden layer\n",
        "        self.hidden_layer2 = DFA_LIFLayer(\n",
        "            in_features=hidden_size,\n",
        "            units=hidden_size2,\n",
        "            output_size=output_size,\n",
        "            tau=20.0,\n",
        "            dt=0.25,\n",
        "            threshold=0.05,\n",
        "            t_ref=1.0,\n",
        "            use_dfa=True,\n",
        "            gamma=0.0338\n",
        "        )\n",
        "\n",
        "\n",
        "        # Output layer (no B matrix cus not trained)\n",
        "        self.output_layer = DFA_LIFLayer(\n",
        "            in_features=hidden_size2,\n",
        "            units=output_size,\n",
        "            output_size=output_size,\n",
        "            tau=20.0,\n",
        "            dt=0.25,\n",
        "            threshold=0.2, # maybe play with this (original was 0.4)\n",
        "            t_ref=1.0,\n",
        "            use_dfa=False\n",
        "        )\n",
        "\n",
        "    def forward(self, x_time):\n",
        "        \"\"\"\n",
        "        x_time: [batch, time, 784]  (matches your Keras input exactly)\n",
        "        returns:\n",
        "          h_spikes: [batch, time, hidden]\n",
        "          h2_spikes: [batch, time, hidden]\n",
        "          y_spikes: [batch, time, 10]\n",
        "        \"\"\"\n",
        "        h_spikes = self.hidden_layer(x_time)\n",
        "        h2_spikes = self.hidden_layer2(h_spikes)\n",
        "        y_spikes = self.output_layer(h2_spikes)\n",
        "\n",
        "\n",
        "        return h_spikes, h2_spikes, y_spikes"
      ],
      "metadata": {
        "id": "qlY7nl0nAokQ"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Cell B: PyTorch DFA trainer (manual DFA updates, no autograd needed)\n",
        "class DFATrainerTorch:\n",
        "    def __init__(self, model, learning_rate=0.1, use_exact_gradient=True):\n",
        "        self.model = model\n",
        "        self.lr = learning_rate\n",
        "        self.use_exact_gradient = use_exact_gradient\n",
        "\n",
        "    @staticmethod\n",
        "    def compute_spike_rate(spikes):\n",
        "        # match your TF version: sum over time\n",
        "        return spikes.sum(dim=1)  # [batch, units]\n",
        "\n",
        "    def train_step(self, x_time, y_onehot):\n",
        "      \"\"\"\n",
        "      x_time:   [b,t,784]\n",
        "      y_onehot: [b,10]\n",
        "      returns: (loss_float, acc_float)\n",
        "      \"\"\"\n",
        "      self.model.train()\n",
        "      x_time = x_time.to(device)\n",
        "      y_onehot = y_onehot.to(device)\n",
        "\n",
        "      # Forward\n",
        "      h_spikes, h2_spikes, y_spikes = self.model(x_time)   # [b,t,H1], [b,t,H2], [b,t,10]\n",
        "      out_rates = self.compute_spike_rate(y_spikes)        # [b,10]\n",
        "      out_probs = F.softmax(out_rates, dim=1)              # [b,10]\n",
        "\n",
        "      eps = 1e-9\n",
        "      loss = -(y_onehot * torch.log(out_probs + eps)).sum(dim=1).mean()\n",
        "\n",
        "      # Debug: spike activity\n",
        "      # with torch.no_grad():\n",
        "      #     print(\"mean h1 spike\", h_spikes.mean().item(),\n",
        "      #           \"mean h2 spike\", h2_spikes.mean().item(),\n",
        "      #           \"mean y spike\",  y_spikes.mean().item())\n",
        "\n",
        "      # Global error (broadcast to time)\n",
        "      e_global = out_probs - y_onehot                      # [b,10]\n",
        "      e_time = e_global.unsqueeze(1).repeat(1, y_spikes.size(1), 1)  # [b,t,10]\n",
        "\n",
        "      bsz, T, _ = y_spikes.shape\n",
        "      denom = float(bsz * T)\n",
        "\n",
        "      with torch.no_grad():\n",
        "          # =========================================================\n",
        "          # 1) OUTPUT LAYER UPDATE (uses real output error)\n",
        "          # =========================================================\n",
        "          out_layer = self.model.output_layer\n",
        "          out_input = h2_spikes                              # [b,t,H2]\n",
        "\n",
        "          a_out = (out_input @ out_layer.w) + out_layer.b    # [b,t,10]\n",
        "\n",
        "          if self.use_exact_gradient:\n",
        "              fprime_out = surrogate_gradient_exact(\n",
        "                  a_out, h_th=out_layer.threshold, t_ref=out_layer.t_ref, tau=out_layer.tau\n",
        "              )\n",
        "          else:\n",
        "              fprime_out = surrogate_gradient_fast_sigmoid(\n",
        "                  a_out, threshold=out_layer.threshold, alpha=10.0\n",
        "              )\n",
        "\n",
        "          e_out = e_time * fprime_out                        # [b,t,10]\n",
        "\n",
        "          grad_w_out = torch.einsum(\"bti,btj->ij\", out_input, e_out) / denom\n",
        "          grad_b_out = e_out.sum(dim=(0, 1)) / denom\n",
        "\n",
        "          out_layer.w -= self.lr * grad_w_out\n",
        "          out_layer.b -= self.lr * grad_b_out\n",
        "\n",
        "          # =========================================================\n",
        "          # 2) HIDDEN LAYER 2 DFA UPDATE (projects output error via B2)\n",
        "          # =========================================================\n",
        "          hid2_layer = self.model.hidden_layer2\n",
        "\n",
        "          # e_proj2: [b,t,10] x [H2,10] -> [b,t,H2]\n",
        "          e_proj2 = torch.einsum(\"bto,ho->bth\", e_time, hid2_layer.B)\n",
        "\n",
        "          # pre-activation for hidden2 uses input = h_spikes (layer1 spikes)\n",
        "          a_hid2 = (h_spikes @ hid2_layer.w) + hid2_layer.b  # [b,t,H2]\n",
        "\n",
        "          if self.use_exact_gradient:\n",
        "              fprime_hid2 = surrogate_gradient_exact(\n",
        "                  a_hid2, h_th=hid2_layer.threshold, t_ref=hid2_layer.t_ref, tau=hid2_layer.tau\n",
        "              )\n",
        "          else:\n",
        "              fprime_hid2 = surrogate_gradient_fast_sigmoid(\n",
        "                  a_hid2, threshold=hid2_layer.threshold, alpha=10.0\n",
        "              )\n",
        "\n",
        "          e_hid2 = e_proj2 * fprime_hid2                     # [b,t,H2]\n",
        "\n",
        "          grad_w_hid2 = torch.einsum(\"bti,btj->ij\", h_spikes, e_hid2) / denom\n",
        "          grad_b_hid2 = e_hid2.sum(dim=(0, 1)) / denom\n",
        "\n",
        "          hid2_layer.w -= self.lr * grad_w_hid2\n",
        "          hid2_layer.b -= self.lr * grad_b_hid2\n",
        "\n",
        "          # =========================================================\n",
        "          # 3) HIDDEN LAYER 1 DFA UPDATE (projects output error via B1)\n",
        "          # =========================================================\n",
        "          hid1_layer = self.model.hidden_layer\n",
        "\n",
        "          # e_proj1: [b,t,10] x [H1,10] -> [b,t,H1]\n",
        "          e_proj1 = torch.einsum(\"bto,ho->bth\", e_time, hid1_layer.B)\n",
        "\n",
        "          a_hid1 = (x_time @ hid1_layer.w) + hid1_layer.b    # [b,t,H1]\n",
        "\n",
        "          if self.use_exact_gradient:\n",
        "              fprime_hid1 = surrogate_gradient_exact(\n",
        "                  a_hid1, h_th=hid1_layer.threshold, t_ref=hid1_layer.t_ref, tau=hid1_layer.tau\n",
        "              )\n",
        "          else:\n",
        "              fprime_hid1 = surrogate_gradient_fast_sigmoid(\n",
        "                  a_hid1, threshold=hid1_layer.threshold, alpha=10.0\n",
        "              )\n",
        "\n",
        "          e_hid1 = e_proj1 * fprime_hid1                     # [b,t,H1]\n",
        "\n",
        "          grad_w_hid1 = torch.einsum(\"bti,btj->ij\", x_time, e_hid1) / denom\n",
        "          grad_b_hid1 = e_hid1.sum(dim=(0, 1)) / denom\n",
        "\n",
        "          hid1_layer.w -= self.lr * grad_w_hid1\n",
        "          hid1_layer.b -= self.lr * grad_b_hid1\n",
        "\n",
        "        # Accuracy\n",
        "      preds = torch.argmax(out_probs, dim=1)\n",
        "      true = torch.argmax(y_onehot, dim=1)\n",
        "      acc = (preds == true).float().mean().item()\n",
        "\n",
        "      return loss.item(), acc\n",
        "\n",
        "    def fit(self, x_time_all, y_onehot_all, epochs=20, batch_size=128, verbose=1):\n",
        "      history = {\"loss\": [], \"accuracy\": []}\n",
        "      n = x_time_all.shape[0]\n",
        "\n",
        "      for ep in range(epochs):\n",
        "          perm = torch.randperm(n)\n",
        "          ep_loss = 0.0\n",
        "          ep_acc = 0.0\n",
        "          nb = 0\n",
        "\n",
        "          for i in range(0, n, batch_size):\n",
        "              idx = perm[i:i+batch_size]\n",
        "              xb_time = x_time_all[idx].to(device)\n",
        "              yb_onehot = y_onehot_all[idx].to(device)\n",
        "\n",
        "              loss, acc = self.train_step(xb_time, yb_onehot)\n",
        "\n",
        "\n",
        "              ep_loss += loss\n",
        "              ep_acc += acc\n",
        "              nb += 1\n",
        "\n",
        "          history[\"loss\"].append(ep_loss / nb)\n",
        "          history[\"accuracy\"].append(ep_acc / nb)\n",
        "\n",
        "          if verbose:\n",
        "              print(f\"Epoch {ep+1}/{epochs} - loss: {history['loss'][-1]:.4f} - acc: {history['accuracy'][-1]:.4f}\")\n",
        "\n",
        "      return history"
      ],
      "metadata": {
        "id": "VZku5B2bApSu"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# @title\n",
        "# PyTorch version of that ONE cell (data prep + build + train + plot)\n",
        "# Assumes you already defined: DFA_LIFLayer, DFASNN, DFATrainerTorch (from earlier port steps)\n",
        "\n",
        "import numpy as np\n",
        "import torch\n",
        "import torch.nn.functional as F\n",
        "from torchvision import datasets, transforms\n",
        "from torch.utils.data import DataLoader, Subset\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# ----------------------------\n",
        "# Load MNIST data (PyTorch)\n",
        "# ----------------------------\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "transform = transforms.Compose([\n",
        "    transforms.ToTensor(),                  # -> float32 in [0,1], shape [1,28,28]\n",
        "    transforms.Lambda(lambda x: x.view(-1))  # -> [784]\n",
        "])\n",
        "\n",
        "train_ds_full = datasets.MNIST(root=\"./data\", train=True, download=True, transform=transform)\n",
        "test_ds_full  = datasets.MNIST(root=\"./data\", train=False, download=True, transform=transform)\n",
        "\n",
        "# Convert to tensors (to mirror your \"x_train, y_train\" arrays)\n",
        "# Note: this iterates through the dataset once; fine for MNIST.\n",
        "x_train = torch.stack([train_ds_full[i][0] for i in range(len(train_ds_full))]).float()  # [60000,784]\n",
        "y_train_int = torch.tensor([train_ds_full[i][1] for i in range(len(train_ds_full))], dtype=torch.long)  # [60000]\n",
        "\n",
        "x_test  = torch.stack([test_ds_full[i][0] for i in range(len(test_ds_full))]).float()    # [10000,784]\n",
        "y_test_int  = torch.tensor([test_ds_full[i][1] for i in range(len(test_ds_full))], dtype=torch.long)   # [10000]\n",
        "\n",
        "# One-hot encode labels (to match your Keras cell exactly)\n",
        "y_train = F.one_hot(y_train_int, num_classes=10).float()  # [60000,10]\n",
        "y_test  = F.one_hot(y_test_int, num_classes=10).float()   # [10000,10]\n",
        "\n",
        "print(f\"Training samples: {len(x_train)}\")\n",
        "print(f\"Test samples: {len(x_test)}\")\n",
        "print(f\"Input shape: {x_train.shape[1]}\")\n",
        "print(f\"Output classes: {y_train.shape[1]}\")\n",
        "\n",
        "TIME_STEPS = 25   # Reduced from 100 for faster training\n",
        "HIDDEN_SIZE = 1000\n",
        "HIDDEN_SIZE2 = 250\n",
        "OUTPUT_SIZE = 10\n",
        "INPUT_SIZE = 784\n",
        "\n",
        "# INCREASED: Use larger dataset\n",
        "TRAIN_SAMPLES = 30000\n",
        "x_train_small = x_train[:TRAIN_SAMPLES]          # [30000,784]\n",
        "y_train_small = y_train[:TRAIN_SAMPLES]          # [30000,10]\n",
        "y_train_small_int = y_train_int[:TRAIN_SAMPLES]  # [30000]\n",
        "\n",
        "# Expand to time dimension (paper uses \"direct mapping\")\n",
        "# Just repeat the input for each time step\n",
        "# (Matches your np.tile logic, but in torch)\n",
        "x_train_spikes = x_train_small.unsqueeze(1).repeat(1, TIME_STEPS, 1)  # [30000,25,784]\n",
        "\n",
        "print(f\"\\nInput shape with time: {tuple(x_train_spikes.shape)}\")\n",
        "print(f\"  [batch={x_train_spikes.shape[0]}, \"\n",
        "      f\"time={x_train_spikes.shape[1]}, \"\n",
        "      f\"features={x_train_spikes.shape[2]}]\")\n",
        "\n",
        "# ----------------------------\n",
        "# Build DFA-SNN model (PyTorch)\n",
        "# ----------------------------\n",
        "print(\"\\n\" + \"=\"*60)\n",
        "print(\"BUILDING DFA-SNN MODEL (PyTorch)\")\n",
        "print(\"=\"*60)\n",
        "\n",
        "# Safety checks (so you get a clear error if earlier port cells aren't run yet)\n",
        "missing = []\n",
        "for name in [\"DFA_LIFLayer\", \"DFASNN\", \"DFATrainerTorch\"]:\n",
        "    if name not in globals():\n",
        "        missing.append(name)\n",
        "if missing:\n",
        "    raise NameError(\n",
        "        \"You haven't defined these yet in earlier cells: \"\n",
        "        + \", \".join(missing)\n",
        "        + \"\\nRun the cells where you ported the layer/model/trainer first.\"\n",
        "    )\n",
        "\n",
        "model = DFASNN(\n",
        "    time_steps=TIME_STEPS,\n",
        "    input_size=INPUT_SIZE,\n",
        "    hidden_size=HIDDEN_SIZE,\n",
        "    hidden_size2=HIDDEN_SIZE2,\n",
        "    output_size=OUTPUT_SIZE\n",
        ").to(device)\n",
        "\n",
        "with torch.no_grad():\n",
        "        model.hidden_layer2.b.fill_(1.0)\n",
        "        model.output_layer.b.fill_(1.0)\n",
        "print(model)\n",
        "\n",
        "# ----------------------------\n",
        "# Create DFA trainer (PyTorch)\n",
        "# ----------------------------\n",
        "print(\"\\n\" + \"=\"*60)\n",
        "print(\"INITIALIZING DFA TRAINER (PyTorch)\")\n",
        "print(\"=\"*60)\n",
        "\n",
        "trainer = DFATrainerTorch(\n",
        "    model=model,\n",
        "    learning_rate=0.1  # matches your cell\n",
        ")\n",
        "\n",
        "# Mirror your \"dfa layers found\" printout as closely as possible\n",
        "dfa_layers = []\n",
        "if hasattr(model, \"hidden_layer\"):\n",
        "    dfa_layers.append(model.hidden_layer)\n",
        "if hasattr(model, \"hidden_layer2\"):\n",
        "    dfa_layers.append(model.hidden_layer2)\n",
        "if hasattr(model, \"output_layer\"):\n",
        "    dfa_layers.append(model.output_layer)\n",
        "\n",
        "print(f\"\\nDFA layers found: {len(dfa_layers)}\")\n",
        "for layer in dfa_layers:\n",
        "    B_shape = tuple(layer.B.shape) if hasattr(layer, \"B\") else None\n",
        "    print(f\"  {layer.__class__.__name__}:\")\n",
        "    print(f\"    Units: {layer.units}\")\n",
        "    print(f\"    B shape: {B_shape}\")\n",
        "    print(f\"    Learning rate: {trainer.lr:.6f}\")\n",
        "\n",
        "model.hidden_layer.debug_name = \"h1\"\n",
        "model.hidden_layer2.debug_name = \"h2\"\n",
        "model.output_layer.debug_name = \"out\"\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "# We already built x_train_spikes as a full tensor [30000,25,784]\n",
        "# To keep this as a single-cell replica, we’ll train on it directly in batches.\n",
        "\n",
        "history = trainer.fit(x_train_spikes, y_train_small, epochs=200, batch_size=128, verbose=1)\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "\n",
        "epochs = np.arange(1, len(history[\"accuracy\"]) + 1)\n",
        "\n",
        "plt.figure(figsize=(8, 5))\n",
        "plt.plot(epochs, history[\"accuracy\"], marker='o', linewidth=2)\n",
        "\n",
        "plt.xlabel(\"Epoch\")\n",
        "plt.ylabel(\"Accuracy\")\n",
        "plt.title(\"Training Accuracy vs Epochs (DFA-SNN)\")\n",
        "plt.grid(True, alpha=0.3)\n",
        "\n",
        "plt.xticks(epochs if len(epochs) <= 20 else None)\n",
        "plt.ylim(0, 1.0)\n",
        "\n",
        "plt.show()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "eBfUTSjSHxk6",
        "outputId": "ead36d28-0993-41c5-f1f3-5cb9afcb3a39"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Training samples: 60000\n",
            "Test samples: 10000\n",
            "Input shape: 784\n",
            "Output classes: 10\n",
            "\n",
            "Input shape with time: (30000, 25, 784)\n",
            "  [batch=30000, time=25, features=784]\n",
            "\n",
            "============================================================\n",
            "BUILDING DFA-SNN MODEL (PyTorch)\n",
            "============================================================\n",
            "DFASNN(\n",
            "  (hidden_layer): DFA_LIFLayer()\n",
            "  (hidden_layer2): DFA_LIFLayer()\n",
            "  (output_layer): DFA_LIFLayer()\n",
            ")\n",
            "\n",
            "============================================================\n",
            "INITIALIZING DFA TRAINER (PyTorch)\n",
            "============================================================\n",
            "\n",
            "DFA layers found: 3\n",
            "  DFA_LIFLayer:\n",
            "    Units: 1000\n",
            "    B shape: (1000, 10)\n",
            "    Learning rate: 0.100000\n",
            "  DFA_LIFLayer:\n",
            "    Units: 250\n",
            "    B shape: (250, 10)\n",
            "    Learning rate: 0.100000\n",
            "  DFA_LIFLayer:\n",
            "    Units: 10\n",
            "    B shape: None\n",
            "    Learning rate: 0.100000\n",
            "Epoch 1/200 - loss: 2.3041 - acc: 0.0987\n",
            "Epoch 2/200 - loss: 2.3153 - acc: 0.0989\n",
            "Epoch 3/200 - loss: 2.3051 - acc: 0.0990\n",
            "Epoch 4/200 - loss: 2.3034 - acc: 0.0930\n",
            "Epoch 5/200 - loss: 2.2969 - acc: 0.0776\n",
            "Epoch 6/200 - loss: 2.2788 - acc: 0.1115\n",
            "Epoch 7/200 - loss: 2.2376 - acc: 0.1721\n",
            "Epoch 8/200 - loss: 2.1929 - acc: 0.1995\n",
            "Epoch 9/200 - loss: 2.1608 - acc: 0.2083\n",
            "Epoch 10/200 - loss: 2.1061 - acc: 0.2205\n",
            "Epoch 11/200 - loss: 1.9875 - acc: 0.2650\n",
            "Epoch 12/200 - loss: 1.8821 - acc: 0.2846\n",
            "Epoch 13/200 - loss: 1.8030 - acc: 0.3076\n",
            "Epoch 14/200 - loss: 1.7431 - acc: 0.3430\n",
            "Epoch 15/200 - loss: 1.6884 - acc: 0.3938\n",
            "Epoch 16/200 - loss: 1.6279 - acc: 0.4437\n",
            "Epoch 17/200 - loss: 1.5636 - acc: 0.4834\n",
            "Epoch 18/200 - loss: 1.5098 - acc: 0.5160\n",
            "Epoch 19/200 - loss: 1.4613 - acc: 0.5483\n",
            "Epoch 20/200 - loss: 1.4234 - acc: 0.5705\n",
            "Epoch 21/200 - loss: 1.3910 - acc: 0.5967\n",
            "Epoch 22/200 - loss: 1.3600 - acc: 0.6187\n",
            "Epoch 23/200 - loss: 1.3381 - acc: 0.6347\n",
            "Epoch 24/200 - loss: 1.3163 - acc: 0.6509\n",
            "Epoch 25/200 - loss: 1.2952 - acc: 0.6710\n",
            "Epoch 26/200 - loss: 1.2759 - acc: 0.6883\n",
            "Epoch 27/200 - loss: 1.2583 - acc: 0.7027\n",
            "Epoch 28/200 - loss: 1.2418 - acc: 0.7166\n",
            "Epoch 29/200 - loss: 1.2277 - acc: 0.7280\n",
            "Epoch 30/200 - loss: 1.2086 - acc: 0.7423\n",
            "Epoch 31/200 - loss: 1.1893 - acc: 0.7532\n",
            "Epoch 32/200 - loss: 1.1732 - acc: 0.7605\n",
            "Epoch 33/200 - loss: 1.1610 - acc: 0.7676\n",
            "Epoch 34/200 - loss: 1.1500 - acc: 0.7742\n",
            "Epoch 35/200 - loss: 1.1369 - acc: 0.7810\n",
            "Epoch 36/200 - loss: 1.1243 - acc: 0.7844\n",
            "Epoch 37/200 - loss: 1.1067 - acc: 0.7900\n",
            "Epoch 38/200 - loss: 1.0887 - acc: 0.7953\n",
            "Epoch 39/200 - loss: 1.0704 - acc: 0.7969\n",
            "Epoch 40/200 - loss: 1.0516 - acc: 0.8013\n",
            "Epoch 41/200 - loss: 1.0352 - acc: 0.8076\n",
            "Epoch 42/200 - loss: 1.0164 - acc: 0.8088\n",
            "Epoch 43/200 - loss: 0.9989 - acc: 0.8124\n",
            "Epoch 44/200 - loss: 0.9854 - acc: 0.8171\n",
            "Epoch 45/200 - loss: 0.9724 - acc: 0.8211\n",
            "Epoch 46/200 - loss: 0.9572 - acc: 0.8247\n",
            "Epoch 47/200 - loss: 0.9442 - acc: 0.8272\n",
            "Epoch 48/200 - loss: 0.9293 - acc: 0.8322\n",
            "Epoch 49/200 - loss: 0.9174 - acc: 0.8343\n",
            "Epoch 50/200 - loss: 0.9060 - acc: 0.8352\n",
            "Epoch 51/200 - loss: 0.8947 - acc: 0.8382\n",
            "Epoch 52/200 - loss: 0.8849 - acc: 0.8397\n",
            "Epoch 53/200 - loss: 0.8747 - acc: 0.8419\n",
            "Epoch 54/200 - loss: 0.8663 - acc: 0.8438\n",
            "Epoch 55/200 - loss: 0.8573 - acc: 0.8451\n",
            "Epoch 56/200 - loss: 0.8503 - acc: 0.8453\n",
            "Epoch 57/200 - loss: 0.8419 - acc: 0.8453\n",
            "Epoch 58/200 - loss: 0.8354 - acc: 0.8477\n",
            "Epoch 59/200 - loss: 0.8293 - acc: 0.8480\n",
            "Epoch 60/200 - loss: 0.8230 - acc: 0.8493\n",
            "Epoch 61/200 - loss: 0.8184 - acc: 0.8495\n",
            "Epoch 62/200 - loss: 0.8129 - acc: 0.8500\n",
            "Epoch 63/200 - loss: 0.8077 - acc: 0.8511\n",
            "Epoch 64/200 - loss: 0.8023 - acc: 0.8510\n",
            "Epoch 65/200 - loss: 0.7973 - acc: 0.8529\n",
            "Epoch 66/200 - loss: 0.7944 - acc: 0.8534\n",
            "Epoch 67/200 - loss: 0.7893 - acc: 0.8540\n",
            "Epoch 68/200 - loss: 0.7853 - acc: 0.8547\n",
            "Epoch 69/200 - loss: 0.7815 - acc: 0.8539\n",
            "Epoch 70/200 - loss: 0.7780 - acc: 0.8551\n",
            "Epoch 71/200 - loss: 0.7743 - acc: 0.8558\n",
            "Epoch 72/200 - loss: 0.7703 - acc: 0.8558\n",
            "Epoch 73/200 - loss: 0.7674 - acc: 0.8560\n",
            "Epoch 74/200 - loss: 0.7642 - acc: 0.8570\n",
            "Epoch 75/200 - loss: 0.7618 - acc: 0.8561\n",
            "Epoch 76/200 - loss: 0.7582 - acc: 0.8572\n",
            "Epoch 77/200 - loss: 0.7560 - acc: 0.8575\n",
            "Epoch 78/200 - loss: 0.7528 - acc: 0.8592\n",
            "Epoch 79/200 - loss: 0.7501 - acc: 0.8598\n",
            "Epoch 80/200 - loss: 0.7468 - acc: 0.8603\n",
            "Epoch 81/200 - loss: 0.7447 - acc: 0.8617\n",
            "Epoch 82/200 - loss: 0.7415 - acc: 0.8618\n",
            "Epoch 83/200 - loss: 0.7405 - acc: 0.8626\n",
            "Epoch 84/200 - loss: 0.7379 - acc: 0.8628\n",
            "Epoch 85/200 - loss: 0.7357 - acc: 0.8626\n",
            "Epoch 86/200 - loss: 0.7338 - acc: 0.8634\n",
            "Epoch 87/200 - loss: 0.7314 - acc: 0.8637\n",
            "Epoch 88/200 - loss: 0.7288 - acc: 0.8648\n",
            "Epoch 89/200 - loss: 0.7281 - acc: 0.8640\n",
            "Epoch 90/200 - loss: 0.7262 - acc: 0.8643\n",
            "Epoch 91/200 - loss: 0.7234 - acc: 0.8658\n",
            "Epoch 92/200 - loss: 0.7219 - acc: 0.8657\n",
            "Epoch 93/200 - loss: 0.7202 - acc: 0.8664\n",
            "Epoch 94/200 - loss: 0.7174 - acc: 0.8669\n",
            "Epoch 95/200 - loss: 0.7151 - acc: 0.8679\n",
            "Epoch 96/200 - loss: 0.7127 - acc: 0.8685\n",
            "Epoch 97/200 - loss: 0.7115 - acc: 0.8689\n",
            "Epoch 98/200 - loss: 0.7089 - acc: 0.8697\n",
            "Epoch 99/200 - loss: 0.7084 - acc: 0.8696\n",
            "Epoch 100/200 - loss: 0.7071 - acc: 0.8701\n",
            "Epoch 101/200 - loss: 0.7050 - acc: 0.8700\n",
            "Epoch 102/200 - loss: 0.7031 - acc: 0.8700\n",
            "Epoch 103/200 - loss: 0.7009 - acc: 0.8708\n",
            "Epoch 104/200 - loss: 0.6998 - acc: 0.8712\n",
            "Epoch 105/200 - loss: 0.6975 - acc: 0.8726\n",
            "Epoch 106/200 - loss: 0.6964 - acc: 0.8721\n",
            "Epoch 107/200 - loss: 0.6943 - acc: 0.8738\n",
            "Epoch 108/200 - loss: 0.6921 - acc: 0.8732\n",
            "Epoch 109/200 - loss: 0.6907 - acc: 0.8729\n",
            "Epoch 110/200 - loss: 0.6893 - acc: 0.8735\n",
            "Epoch 111/200 - loss: 0.6872 - acc: 0.8748\n",
            "Epoch 112/200 - loss: 0.6861 - acc: 0.8747\n",
            "Epoch 113/200 - loss: 0.6848 - acc: 0.8748\n",
            "Epoch 114/200 - loss: 0.6824 - acc: 0.8749\n",
            "Epoch 115/200 - loss: 0.6812 - acc: 0.8759\n",
            "Epoch 116/200 - loss: 0.6794 - acc: 0.8764\n",
            "Epoch 117/200 - loss: 0.6778 - acc: 0.8768\n",
            "Epoch 118/200 - loss: 0.6762 - acc: 0.8767\n",
            "Epoch 119/200 - loss: 0.6745 - acc: 0.8778\n",
            "Epoch 120/200 - loss: 0.6727 - acc: 0.8777\n",
            "Epoch 121/200 - loss: 0.6712 - acc: 0.8780\n",
            "Epoch 122/200 - loss: 0.6697 - acc: 0.8782\n",
            "Epoch 123/200 - loss: 0.6677 - acc: 0.8798\n",
            "Epoch 124/200 - loss: 0.6664 - acc: 0.8799\n",
            "Epoch 125/200 - loss: 0.6653 - acc: 0.8797\n",
            "Epoch 126/200 - loss: 0.6643 - acc: 0.8803\n",
            "Epoch 127/200 - loss: 0.6631 - acc: 0.8804\n",
            "Epoch 128/200 - loss: 0.6611 - acc: 0.8806\n",
            "Epoch 129/200 - loss: 0.6598 - acc: 0.8804\n",
            "Epoch 130/200 - loss: 0.6585 - acc: 0.8819\n",
            "Epoch 131/200 - loss: 0.6575 - acc: 0.8809\n",
            "Epoch 132/200 - loss: 0.6556 - acc: 0.8822\n",
            "Epoch 133/200 - loss: 0.6534 - acc: 0.8826\n",
            "Epoch 134/200 - loss: 0.6517 - acc: 0.8828\n",
            "Epoch 135/200 - loss: 0.6512 - acc: 0.8827\n",
            "Epoch 136/200 - loss: 0.6500 - acc: 0.8830\n",
            "Epoch 137/200 - loss: 0.6484 - acc: 0.8834\n",
            "Epoch 138/200 - loss: 0.6475 - acc: 0.8830\n",
            "Epoch 139/200 - loss: 0.6457 - acc: 0.8836\n",
            "Epoch 140/200 - loss: 0.6439 - acc: 0.8844\n",
            "Epoch 141/200 - loss: 0.6434 - acc: 0.8844\n",
            "Epoch 142/200 - loss: 0.6418 - acc: 0.8841\n",
            "Epoch 143/200 - loss: 0.6406 - acc: 0.8841\n",
            "Epoch 144/200 - loss: 0.6393 - acc: 0.8850\n",
            "Epoch 145/200 - loss: 0.6379 - acc: 0.8851\n",
            "Epoch 146/200 - loss: 0.6373 - acc: 0.8846\n",
            "Epoch 147/200 - loss: 0.6353 - acc: 0.8858\n",
            "Epoch 148/200 - loss: 0.6342 - acc: 0.8859\n",
            "Epoch 149/200 - loss: 0.6324 - acc: 0.8863\n",
            "Epoch 150/200 - loss: 0.6320 - acc: 0.8859\n",
            "Epoch 151/200 - loss: 0.6301 - acc: 0.8868\n",
            "Epoch 152/200 - loss: 0.6303 - acc: 0.8865\n",
            "Epoch 153/200 - loss: 0.6288 - acc: 0.8866\n",
            "Epoch 154/200 - loss: 0.6265 - acc: 0.8876\n",
            "Epoch 155/200 - loss: 0.6259 - acc: 0.8875\n",
            "Epoch 156/200 - loss: 0.6253 - acc: 0.8877\n",
            "Epoch 157/200 - loss: 0.6237 - acc: 0.8880\n",
            "Epoch 158/200 - loss: 0.6223 - acc: 0.8883\n",
            "Epoch 159/200 - loss: 0.6217 - acc: 0.8885\n",
            "Epoch 160/200 - loss: 0.6204 - acc: 0.8886\n",
            "Epoch 161/200 - loss: 0.6190 - acc: 0.8892\n",
            "Epoch 162/200 - loss: 0.6173 - acc: 0.8897\n",
            "Epoch 163/200 - loss: 0.6150 - acc: 0.8900\n",
            "Epoch 164/200 - loss: 0.6153 - acc: 0.8896\n",
            "Epoch 165/200 - loss: 0.6143 - acc: 0.8899\n",
            "Epoch 166/200 - loss: 0.6133 - acc: 0.8903\n",
            "Epoch 167/200 - loss: 0.6118 - acc: 0.8904\n",
            "Epoch 168/200 - loss: 0.6109 - acc: 0.8903\n",
            "Epoch 169/200 - loss: 0.6102 - acc: 0.8905\n",
            "Epoch 170/200 - loss: 0.6088 - acc: 0.8912\n",
            "Epoch 171/200 - loss: 0.6077 - acc: 0.8910\n",
            "Epoch 172/200 - loss: 0.6065 - acc: 0.8919\n",
            "Epoch 173/200 - loss: 0.6060 - acc: 0.8914\n",
            "Epoch 174/200 - loss: 0.6052 - acc: 0.8917\n",
            "Epoch 175/200 - loss: 0.6038 - acc: 0.8920\n",
            "Epoch 176/200 - loss: 0.6031 - acc: 0.8914\n",
            "Epoch 177/200 - loss: 0.6023 - acc: 0.8912\n",
            "Epoch 178/200 - loss: 0.6002 - acc: 0.8924\n",
            "Epoch 179/200 - loss: 0.6004 - acc: 0.8929\n",
            "Epoch 180/200 - loss: 0.5989 - acc: 0.8930\n",
            "Epoch 181/200 - loss: 0.5977 - acc: 0.8923\n",
            "Epoch 182/200 - loss: 0.5967 - acc: 0.8929\n",
            "Epoch 183/200 - loss: 0.5962 - acc: 0.8919\n",
            "Epoch 184/200 - loss: 0.5952 - acc: 0.8932\n",
            "Epoch 185/200 - loss: 0.5936 - acc: 0.8933\n",
            "Epoch 186/200 - loss: 0.5930 - acc: 0.8927\n",
            "Epoch 187/200 - loss: 0.5919 - acc: 0.8938\n",
            "Epoch 188/200 - loss: 0.5906 - acc: 0.8938\n",
            "Epoch 189/200 - loss: 0.5894 - acc: 0.8939\n",
            "Epoch 190/200 - loss: 0.5895 - acc: 0.8940\n",
            "Epoch 191/200 - loss: 0.5877 - acc: 0.8936\n",
            "Epoch 192/200 - loss: 0.5864 - acc: 0.8941\n",
            "Epoch 193/200 - loss: 0.5855 - acc: 0.8944\n",
            "Epoch 194/200 - loss: 0.5848 - acc: 0.8946\n",
            "Epoch 195/200 - loss: 0.5844 - acc: 0.8939\n",
            "Epoch 196/200 - loss: 0.5831 - acc: 0.8951\n",
            "Epoch 197/200 - loss: 0.5820 - acc: 0.8952\n",
            "Epoch 198/200 - loss: 0.5817 - acc: 0.8952\n",
            "Epoch 199/200 - loss: 0.5796 - acc: 0.8960\n",
            "Epoch 200/200 - loss: 0.5795 - acc: 0.8954\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 800x500 with 1 Axes>"
            ],
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAArMAAAHWCAYAAABkNgFvAAAAOnRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjEwLjAsIGh0dHBzOi8vbWF0cGxvdGxpYi5vcmcvlHJYcgAAAAlwSFlzAAAPYQAAD2EBqD+naQAAXR9JREFUeJzt3Xlc1NX+x/H3gLKIoiAgaApoVpK7JtlmlmuGWXYzs1zqZpmWxV1M08i6v7TNlptZWVY3Tc3uzbKFMpe6lam5VGSaJmopuJGAKKDM+f3hZXKcGRh0mAVez8eDh8yZ8535zOGL8+Y753u+FmOMEQAAABCAgnxdAAAAAHC6CLMAAAAIWIRZAAAABCzCLAAAAAIWYRYAAAABizALAACAgEWYBQAAQMAizAIAACBgEWYBAAAQsAizQAAYOXKkkpKSTmvbhx56SBaLxbMFAV4ycuRI1a9fv9qf59dff1VYWJi++uqran+umuTFF19UixYtVFJS4utSUIsRZoEzYLFY3PpauXKlr0v1uRtuuEEWi0UTJkzwdSk4yciRI13ut2FhYb4uz2sefvhhpaam6uKLL7a1nTo29evXV8uWLXX99dfr3//+t6xWq8PjXH755S7Hc/PmzXZ9u3XrJovFolmzZlWp1tLSUj377LPq1KmTIiMj1ahRI51//vkaPXq03XO8/vrrtp/j7t27ndbatm1bu7akpCRZLBbdfffdDv1Xrlwpi8Wid955x26MSktL9dJLL1XpNQCeVMfXBQCB7M0337S7/a9//UtLly51aG/Tps0ZPc/s2bOdvnG6Y/Lkybr//vvP6PnPVEFBgZYsWaKkpCTNnz9f06dP52ixHwkNDdUrr7zi0B4cHOyDarxv//79euONN/TGG2843Hfy2Bw9elQ7d+7UkiVLdP311+vyyy/Xe++9p8jISLttzjrrLE2bNs3hsZo2bWr7fuvWrVq7dq2SkpI0b948jRkzxu16Bw8erI8//lhDhw7V7bffrmPHjmnz5s364IMPdNFFF+m8886z619SUqLp06frn//8p9vPMXv2bE2cONGuZmfCwsI0YsQIzZgxQ3fffTe/1/ANA8Bjxo4da9z5tSoqKvJCNf5jzpw5pm7dumb58uVGklm5cqWvS3LKarWaI0eO+LoMrxoxYoSJiIjwdRkueaO+GTNmmPDwcFNYWOj2c0+bNs1IMjfccINde48ePcz5559f6XM++OCDJi4uzvz73/82FovFZGdnu1XrmjVrjCTzf//3fw73HT9+3Bw4cMB2+7XXXjOSTMeOHU1oaKjZvXt3pbUmJiaa888/39SpU8fcfffddvetWLHCSDKLFi2ya//222+NJLNs2TK3XgPgaUwzAKpZ+Ud569at02WXXaZ69epp0qRJkqT33ntPAwYMUNOmTRUaGqpWrVrpkUceUVlZmd1jnDpndseOHbJYLHryySf18ssvq1WrVgoNDdUFF1ygtWvX2m3rbM6sxWLRuHHjtHjxYrVt21ahoaE6//zzlZmZ6VD/ypUr1bVrV4WFhalVq1Z66aWXqjwPd968eerdu7d69uypNm3aaN68eU77bd68WTfccINiY2MVHh6uc889Vw888IBdn927d+u2226zjVlycrLGjBmj0tJSl69X+uMj1x07dtjakpKSdPXVV+uTTz5R165dFR4ebvu49LXXXtMVV1yhuLg4hYaGKiUlxeXHwR9//LF69OihBg0aKDIyUhdccIHeeustSVJGRobq1q2r/fv3O2w3evRoNWrUSMXFxU4f98knn5TFYtHOnTsd7ps4caJCQkL0+++/SzpxpG/w4MGKj49XWFiYzjrrLN14443Kz893+thVVT5+X3zxhe644w41btxYkZGRGj58uK2Gk73wwgs6//zzFRoaqqZNm2rs2LE6dOiQQ7/Vq1frqquuUlRUlCIiItS+fXs9++yzDv12796tQYMGqX79+oqNjdVf//pXh9+TBQsWqEuXLrafQ7t27Zw+1qkWL16s1NTUKs3Nvf/++9WnTx8tWrRIP//8s9vblXvrrbd0/fXX6+qrr1bDhg1t+0tlfvnlF0mymw5RLjg4WI0bN3ZonzRpksrKyjR9+nS3niMpKUnDhw/X7NmztWfPnkr7d+nSRdHR0XrvvffcenzA0wizgBccPHhQ/fv3V8eOHfXMM8+oZ8+ekk4EhPr16ys9PV3PPvusunTpogcffNDtaQFvvfWWnnjiCd1xxx36xz/+oR07dui6667TsWPHKt32yy+/1F133aUbb7xRjz/+uIqLizV48GAdPHjQ1mfDhg3q16+fDh48qKlTp+q2227Tww8/rMWLF7v92vfs2aMVK1Zo6NChkqShQ4fqnXfesYXPct9//71SU1O1fPly3X777Xr22Wc1aNAgLVmyxO6xunXrpgULFmjIkCF67rnndMstt+jzzz/XkSNH3K7pZFu2bNHQoUPVu3dvPfvss+rYsaMkadasWUpMTNSkSZP01FNPqXnz5rrrrrs0c+ZMu+1ff/11DRgwQHl5eZo4caKmT5+ujh072v4wuOWWW3T8+HEtXLjQbrvS0lK98847Gjx4sMu5qeXzjN9++22H+95++2316dNHUVFRKi0tVd++ffXNN9/o7rvv1syZMzV69Ght377daYB05sCBAw5fBQUFDv3GjRunn376SQ899JCGDx+uefPmadCgQTLG2Po89NBDGjt2rJo2baqnnnpKgwcP1ksvvaQ+ffrY7ZtLly7VZZddpk2bNmn8+PF66qmn1LNnT33wwQd2z1lWVqa+ffuqcePGevLJJ9WjRw899dRTevnll+0ea+jQoYqKitJjjz2m6dOn6/LLL6/0hK5jx45p7dq16ty5s1vjdLJbbrlFxhgtXbrUod5Tx/Lw4cO2+1evXq1t27Zp6NChCgkJ0XXXXefyD7xTJSYmSjrxB+Lx48fd2iY5OblK4VSSHnjgAR0/ftztANy5c2dOnoPv+PrQMFCTOJtm0KNHDyPJvPjiiw79nX2kfccdd5h69eqZ4uJiW9uIESNMYmKi7XZ2draRZBo3bmzy8vJs7e+9956RZJYsWWJry8jIcKhJkgkJCTHbtm2ztX333XdGkvnnP/9pa0tLSzP16tWz+3hy69atpk6dOm5NpzDGmCeffNKEh4ebgoICY4wxP//8s5Fk3n33Xbt+l112mWnQoIHZuXOnXbvVarV9P3z4cBMUFGTWrl3r8Dzl/Zy9XmP++Mj15I9zExMTjSSTmZnp0N/Zz6Zv376mZcuWttuHDh0yDRo0MKmpqebo0aMu6+7evbtJTU21u/8///mPkWRWrFjh8Dwn6969u+nSpYtdW/lHzf/617+MMcZs2LDB6ce/7hgxYoSR5PSrb9++tn7l49elSxdTWlpqa3/88ceNJPPee+8ZY4zZt2+fCQkJMX369DFlZWW2fs8//7yRZObMmWOMOfGReHJysklMTDS///67XU0nj115fQ8//LBdn06dOtmNy/jx401kZKQ5fvx4lV7/tm3bHPb7k5+7oikO5eN+33332drKf99P/RoxYoStz7hx40zz5s1tr/PTTz81ksyGDRsqrddqtdqeo0mTJmbo0KFm5syZDr83xvzxM1u7dq355ZdfTJ06dcw999xjV6uzaQYDBgwwxhgzatQoExYWZvbs2WOMcT3NwBhjRo8ebcLDwyutH6gOHJkFvCA0NFSjRo1yaA8PD7d9X1hYqAMHDujSSy/VkSNHHM58dmbIkCGKioqy3b700kslSdu3b6902169eqlVq1a22+3bt1dkZKRt27KyMn322WcaNGiQ3UkgZ599tvr371/p45ebN2+eBgwYoAYNGkiSWrdurS5dutgdidq/f7+++OIL3XrrrWrRooXd9uVTBqxWqxYvXqy0tDR17drV4XlO98ST5ORk9e3b16H95J9Nfn6+Dhw4oB49emj79u22j+6XLl2qwsJC3X///Q5HV0+uZ/jw4Vq9erXtI2LpxLg0b95cPXr0qLC+IUOGaN26dXbbLly4UKGhobrmmmskSQ0bNpQkffLJJ6d1hDosLExLly51+HJ2VG706NGqW7eu7faYMWNUp04dffTRR5Kkzz77TKWlpbr33nsVFPTHW8ztt9+uyMhIffjhh5JOHPXPzs7Wvffeq0aNGtk9h7Of5Z133ml3+9JLL7Xbzxs1aqSioiKHo6SVKf8k4uTfI3eVT0soLCy0a09KSnIYy7///e+SZDtKP2TIENvrLJ/O4s7RWYvFok8++UT/+Mc/FBUVpfnz52vs2LFKTEzUkCFDXB6Jb9mypW655Ra9/PLLysnJcev1TZ482e2js1FRUTp69Ohpf0ICnAnCLOAFzZo1U0hIiEP7jz/+qGuvvVYNGzZUZGSkYmNjdfPNN0uSW3MdTw1+5W/IzuYwVrZt+fbl2+7bt09Hjx7V2Wef7dDPWZszP/30kzZs2KCLL75Y27Zts31dfvnl+uCDD2wfY5eHklOXCTrZ/v37VVBQUGGf05GcnOy0/auvvlKvXr0UERGhRo0aKTY21jbXufxnUx4wK6tpyJAhCg0NtYWV/Px8ffDBBxo2bFilIfxPf/qTgoKCbNMUjDFatGiR+vfvbzuLPjk5Wenp6XrllVcUExOjvn37aubMmW7Plw0ODlavXr0cvsqnXJysdevWdrfr16+vhIQE21zk8vm95557rl2/kJAQtWzZ0na/u2MnnQjbsbGxdm0n76uSdNddd+mcc85R//79ddZZZ+nWW291OgfcFXPSNAl3lU8dKP9DrVxERITDWKakpEiSPv30U+3fv1/dunWz/T5kZ2erZ8+emj9/vm3Vkvz8fOXm5tq+8vLybI8fGhqqBx54QD/99JP27Nmj+fPn68ILL9Tbb7+tcePGuay3KuFUqloALh8/VjOALxBmAS84+ShfuUOHDqlHjx767rvv9PDDD2vJkiVaunSpHnvsMUlyaykuV0snufPGfCbbumvu3LmSpPvuu0+tW7e2fT311FMqLi7Wv//9b489VzlXb6annixUztnP5pdfftGVV16pAwcOaMaMGfrwww+1dOlS3XfffZLc+9mcLCoqSldffbUtzL7zzjsqKSmx/eFSkaZNm+rSSy+1zZv95ptvtGvXLg0ZMsSu31NPPaXvv/9ekyZN0tGjR3XPPffo/PPP12+//ValWv2RO0uExcXFaePGjXr//fc1cOBArVixQv3799eIESMq3K78hCl3/gA8VVZWliT3/7iTZNsHbrjhBrvfiYULF2r37t36/PPPJUnjx49XQkKC7eu6665z+ngJCQm68cYb9cUXX6h169Z6++23Xc6lbdmypW6++eYqHZ0tnztb/v+SK7///rvq1avn9PcJqG6sMwv4yMqVK3Xw4EH95z//0WWXXWZrz87O9mFVf4iLi1NYWJi2bdvmcJ+ztlMZY/TWW2+pZ8+euuuuuxzuf+SRRzRv3jyNGjVKLVu2lPRHOHAmNjZWkZGRFfaR/jg6fejQIbuPr52tCODKkiVLVFJSovfff9/uCPaKFSvs+pVP08jKyqo00AwfPlzXXHON1q5dq3nz5qlTp046//zz3apnyJAhuuuuu7RlyxYtXLhQ9erVU1pamkO/du3aqV27dpo8ebK+/vprXXzxxXrxxRf1j3/8w63nccfWrVttJzBKJ45O5uTk6KqrrpL0xwlKW7Zssf1cpRMnvGVnZ6tXr16S7MeuvO1MhYSEKC0tTWlpabJarbrrrrv00ksvacqUKS5/Pi1atFB4ePhp/d69+eabslgs6t27t1v9i4qK9N5772nIkCG6/vrrHe6/5557NG/ePPXs2VN///vf7f7YqWwaRN26ddW+fXtt3bpVBw4cUHx8vNN+kydP1ty5cysNp+VatWqlm2++WS+99JJSU1Nd9svOzj7j9bSB08WRWcBHyo82nXwktLS0VC+88IKvSrJT/tHz4sWL7c6A3rZtmz7++ONKt//qq6+0Y8cOjRo1Stdff73D15AhQ7RixQrt2bNHsbGxuuyyyzRnzhzt2rXL7nHKxycoKMi2usG3337r8Hzl/cpD0hdffGG7r6ioyOmC+BW99pMfUzrxse9rr71m169Pnz5q0KCBpk2b5rC81qlHuPv376+YmBg99thj+vzzz906Kltu8ODBCg4O1vz587Vo0SJdffXVioiIsN1fUFDgcDSuXbt2CgoK8vhlRl9++WW7FQlmzZql48eP2+ZR9+rVSyEhIXruuefsxuDVV19Vfn6+BgwYIOnE2e/Jycl65plnHOZ5ns6nAyevwiGd2F/at28vSRWOQd26ddW1a1en+1RFpk+frk8//VRDhgxxmHrhyrvvvquioiKNHTvW6e/E1VdfrX//+98qKSlRSkqK3TSFLl26SDrxx8SpvyPSiT/eVq1apaioKIcpGSc7OZzm5ua6VffkyZN17NgxPf744y77rF+/XhdddJFbjwd4GkdmAR+56KKLFBUVpREjRuiee+6RxWLRm2++6dGP+c/UQw89pE8//VQXX3yxxowZo7KyMj3//PNq27atNm7cWOG28+bNU3BwsC28nGrgwIF64IEHtGDBAqWnp+u5557TJZdcos6dO2v06NFKTk7Wjh079OGHH9qe69FHH9Wnn36qHj16aPTo0WrTpo1ycnK0aNEiffnll2rUqJH69OmjFi1a6LbbbtPf/vY3BQcHa86cOYqNjXUaApzp06eP7SjfHXfcocOHD2v27NmKi4uz+3g2MjJSTz/9tP785z/rggsu0E033aSoqCh99913OnLkiF2Arlu3rm688UY9//zzCg4Oti1V5o64uDj17NlTM2bMUGFhocMUg+XLl2vcuHH605/+pHPOOUfHjx/Xm2++qeDgYA0ePLjSxz9+/LhtSsiprr32WrvgXFpaqiuvvFI33HCDtmzZohdeeEGXXHKJBg4cKOnEEfSJEydq6tSp6tevnwYOHGjrd8EFF9hCfFBQkGbNmqW0tDR17NhRo0aNUkJCgjZv3qwff/xRn3zyidvjI0l//vOflZeXpyuuuEJnnXWWdu7cqX/+85/q2LFjpUcMr7nmGj3wwAMqKChwuJrXyWNTXFysnTt36v3339f333+vnj172i0PVpl58+apcePGLkPfwIEDNXv2bH344YcupxV89913uummm9S/f39deumlio6O1u7du/XGG29oz549euaZZyqdlvHAAw/ozTff1JYtW9z6dKA8ALv6g3DdunXKy8uznZAIeJ0vllAAaipXS3O5uiLQV199ZS688EITHh5umjZtav7+97+bTz75xGHJJldLcz3xxBMOjynJZGRk2G67Wppr7NixDtsmJibaLSFkjDHLli0znTp1MiEhIaZVq1bmlVdeMX/5y19MWFiYi1EwprS01DRu3NhceumlLvsYY0xycrLp1KmT7XZWVpa59tprTaNGjUxYWJg599xzzZQpU+y22blzpxk+fLiJjY01oaGhpmXLlmbs2LGmpKTE1mfdunUmNTXVhISEmBYtWpgZM2a4XJqrfBmiU73//vumffv2JiwszCQlJZnHHnvMzJkzx+ExyvtedNFFJjw83ERGRppu3bqZ+fPnOzxm+ZJaffr0qXBcnJk9e7aRZBo0aOCwDNj27dvNrbfealq1amXCwsJMdHS06dmzp/nss88qfdyKluY6+bWWj9/nn39uRo8ebaKiokz9+vXNsGHDzMGDBx0e9/nnnzfnnXeeqVu3rmnSpIkZM2aMwxJcxhjz5Zdfmt69e5sGDRqYiIgI0759e7tlslwtj3Xqfv3OO++YPn36mLi4ONvP/Y477jA5OTmVjsHevXtNnTp1zJtvvlnh2NSrV88kJSWZwYMHm3feecdu6bFyrn7fy5/jlltucVnHkSNHTL169cy1115bYa3Tp083PXr0MAkJCaZOnTomKirKXHHFFeadd96x63vy0lynKn9tFS3NdbKtW7ea4OBgp0tzTZgwwbRo0cJuSTXAmyzG+NFhIAABYdCgQfrxxx+1detWX5cSUL777jt17NhR//rXv3TLLbf4upwqef311zVq1CitXbvW6dJoge62227Tzz//rP/+97++LiWglJSUKCkpSffff7/Gjx/v63JQSzFnFkCFjh49and769at+uijj3T55Zf7pqAANnv2bNWvX9/lR8jwnYyMDK1du5arWFXRa6+9prp16zqsAwx4E3NmAVSoZcuWGjlypG2N0FmzZikkJMS2CDwqt2TJEm3atEkvv/yyxo0bZzcHFf6hRYsWDifxoXJ33nknQRY+R5gFUKF+/fpp/vz5ys3NVWhoqLp3765HH33U7TO4Id19993au3evrrrqKk2dOtXX5QBAjeLTObNffPGFnnjiCa1bt045OTl69913NWjQoAq3WblypdLT0/Xjjz+qefPmmjx5skaOHOmVegEAAOBffDpntqioSB06dNDMmTPd6p+dna0BAwaoZ8+e2rhxo+699179+c9/rvISLgAAAKgZ/GY1A4vFUumR2QkTJujDDz+0uwLQjTfeqEOHDlXpGtwAAACoGQJqzuyqVascLnvYt29f3XvvvS63KSkpsbv6i9VqVV5enho3buzyGu4AAADwHWOMCgsL1bRpUwUFVTyRIKDCbG5urpo0aWLX1qRJExUUFOjo0aMKDw932GbatGmccAEAABCAfv31V5111lkV9gmoMHs6Jk6cqPT0dNvt/Px8tWjRQjt37nS4bKEnWa1WHThwQDExMZX+RVGbMC6uMTauMTauMTauMTbOMS6uMTaueXtsCgoKlJiYqAYNGlTaN6DCbHx8vPbu3WvXtnfvXkVGRjo9KitJoaGhCg0NdWhv1KhRtYfZ0tJSNWrUiF+IkzAurjE2rjE2rjE2rjE2zjEurjE2rnl7bMqfw50poQH1k+revbuWLVtm17Z06VJ1797dRxUBAADAl3waZg8fPqyNGzdq48aNkk4svbVx40bt2rVL0okpAsOHD7f1v/POO7V9+3b9/e9/1+bNm/XCCy/o7bff1n333eeL8gEAAOBjPg2z3377rTp16qROnTpJktLT09WpUyc9+OCDkqScnBxbsJWk5ORkffjhh1q6dKk6dOigp556Sq+88or69u3rk/oBAADgWz6dM3v55ZeromVuX3/9dafbbNiwoRqrAgAAQKAIqDmzAAAAwMkIswAAAAhYhFkAAAAELMIsAAAAAhZhFgAAAAGLMAsAAICARZgFAABAwCLMAgAAIGARZgEAABCwCLMAAAAIWIRZAAAABCzCLAAAAAIWYRYAAAABizALAACAgEWYBQAAQMAizAIAACBgEWYBAAAQsAizAAAACFiEWQAAAAQswiwAAAACFmEWAAAAAYswCwAAgIBFmAUAAEDAIswCAAAgYBFmAQAAELAIswAAAAhYhFkAAAAELMIsAAAAAlYdXxcAAAAAzymzGq3JztO+wmLFRIRKFunA4RLFNQhTt+RoBQdZnPbZV1CsvKJSRdcPVVz9P7aLiQiVMVZt35Onsw8HK7VljIKDLL5+mTaEWQAAUOOdHN7KQ50ku0BXWWCragB093tPPsZvh47qvY17lFdU6nQcoiPqqlPzRtrwa77LPhXLVkLDMGWkpahf24TT2N7zCLMAAMArysNgbv5RrwbAZT/t1eJTAl790GAZIxWVljmpNFvREXV1bcdmuuK8Ji4fIxDlFR3Tss37z+gxcvOLNWbues26ubNfBFrCLAAANdzJRxRj64cosZ5RmdVodfZBh4+iq+soor+FwcMlzkLsH/KKjunVr3bo1a92eKegAGIkWSRNXbJJvVPifT7lgDALAICPuJrbWN0hMrxOkIKDLZUGOsAVIyknv1hrsvPUvVVjn9ZCmAUA4H/cmVfpqY/EfXmk8uhxq3Tc60+LGmhfYbGvSyDMAgACV0VHNstP5mlZECRLUFClRzarPq8SQFyDMF+XQJgFAHhPVT5W99yRzezTrpeP4QHnLJLiG/7x6YUvEWYBAJKcf8R+8okdZxpEK1syCEBgKP9fISMtxecnf0mEWQCosaoSPp0d5YyOqKtrOjTVWVH1CKIAbOJZZxYA4EpV1+E8dV5oefvaHXl6/esdOnT02GnXkld0TK99vdODrw6At7izTu6pfSq/AtgBnX1WLFcAA4Ca7nQ/jj+zs9tPf14oUFu4uvpVo/C6GnFRoi5IjNIH63fq062HKryClrsB0BdXAIuuH6r4SMdpQhefHaMHBqRUOJWoIlarVWdHWhUX11hBfhRkJcIsALh0OqHUE0dEgZrs5Okr3gyAJ4c3V/PDywPbI9d31rc7Dzn87lc1APqb4CCLz9eErQ6EWQA13uksTM8cUdQ0zpYZO/UoY3UfRfSXMFhZqKupoa+mIswCCHgVzTP1t0toAqdyN1B6IkRK0urtB7Ttt/1+OfcROB2EWQBe5+xIaeUnHjie5ERYRXWobF5lt+TGHvtI3BdHKi9s2Vgt65f55dxH4HQQZgFUSVWDKCc5obq5OrJZfvZ1y6Yxbl0BzN15lQD8C2EWgE1lc0s5CgpP8OQ8zYpC5h9nX8coKCioynUybxIIDIRZoIarLKBywhNO5uoj9pPvP5Mg6mzJIAA4E4RZIIBxJBXucHeup7OP2E8+qY4gCsAfEWaBAFRmNXp++Ta99lU265nWQFVZh/PUeaGeWgqJj9gBBArCLODHnB15XfbTXr397W86XHLc1+WhEqdz9vvpX5Xn9OaFAkCgI8wCfqbMarQ6+6CWbsplioAfOJ05ov60ODwA1HSEWcAPlFmNVm8/qPe//VWfbv2BAFsNTmdheuaIAoD/I8wCXnbqiTWsIuAZFc0z5SgpANRchFnAizKzcjR1ySbl5Bf7uhSfO/VIaeVXAPP8SU4AgMBHmAW8oHz1gac/+9nXpXhEVYIoJzkBAKoTYRaoJuXTCZZuytW7G3br9yOBsYRWZXNLOQoKAPAnhFmgGvjbdIKKAionPAEAAhlhFvAgX00nqGg9U46kAgBqMsIscIZ8NZ2g/Ghrr5R4wioAoNYizAJnwBvTCZxNEeBoKwAAJxBmgdNQ3dMJOOoKAIB7CLNAFZSH2Dlfbld+8XGPPnZ0RF31PidK13RJUmrLGAIsAABuIMwClTh5Tuzb3/6mwyWeC7HXd26mi1vHKj4yTF0TG+nggf2Ki2usIIIsAABuIcwCFaiuObEJDcOUkZaifm0TbG1Wq9WjzwEAQG1AmAWcqI45scyDBQDA8wizwCkys3L00Ps/KregxGOPeV+v1hp3RWsCLAAAHubzi5zPnDlTSUlJCgsLU2pqqtasWVNh/2eeeUbnnnuuwsPD1bx5c913330qLvaPqywhsJVZjZ79bKvunLveY0E2oWGYXry5s8b3OocgCwBANfDpkdmFCxcqPT1dL774olJTU/XMM8+ob9++2rJli+Li4hz6v/XWW7r//vs1Z84cXXTRRfr55581cuRIWSwWzZgxwwevAIGuOi54wHQCAAC8x6dhdsaMGbr99ts1atQoSdKLL76oDz/8UHPmzNH999/v0P/rr7/WxRdfrJtuukmSlJSUpKFDh2r16tVerRs1Q3Wc3MV0AgAAvMtnYba0tFTr1q3TxIkTbW1BQUHq1auXVq1a5XSbiy66SHPnztWaNWvUrVs3bd++XR999JFuueUWl89TUlKikpI/PjIuKCiQdOLM8eo8e9xqtcoYwxnqp/CHcSmzGs1csU3PLNvmsceMqldX/zeorfq1jZdkZLWaKj+GP4yNv2JsXGNsXGNsnGNcXGNsXPP22FTleXwWZg8cOKCysjI1adLErr1JkybavHmz021uuukmHThwQJdccomMMTp+/LjuvPNOTZo0yeXzTJs2TVOnTnVo379/f7XOtbVarcrPz5cxRkFBPp+a7Dd8PS4rtv2uGSt2aX+RZ9aKjQwN1pBOcRrZLUHBQRbt27fvtB/L12Pjzxgb1xgb1xgb5xgX1xgb17w9NoWFhW73DajVDFauXKlHH31UL7zwglJTU7Vt2zaNHz9ejzzyiKZMmeJ0m4kTJyo9Pd12u6CgQM2bN1dsbKwiIyOrrVar1SqLxaLY2Fh+IU7iy3H56IccTfxg+xk/TnREXV3Toal6pzTRBUmemxPLPuMaY+MaY+MaY+Mc4+IaY+Oat8cmLCzM7b4+C7MxMTEKDg7W3r177dr37t2r+Ph4p9tMmTJFt9xyi/785z9Lktq1a6eioiKNHj1aDzzwgNPBDQ0NVWhoqEN7UFBQtf8wLBaLV54n0HhzXMpP8Prkxxy9sWrnGT9edc+JZZ9xjbFxjbFxjbFxjnFxjbFxzZtjU5Xn8FmYDQkJUZcuXbRs2TINGjRI0onUv2zZMo0bN87pNkeOHHF4ccHBwZIkY6o+RxE1mydP8HJ2xS4AAOB7Pp1mkJ6erhEjRqhr167q1q2bnnnmGRUVFdlWNxg+fLiaNWumadOmSZLS0tI0Y8YMderUyTbNYMqUKUpLS7OFWkA6EWTHzF0vT/yJwwoFAAD4L5+G2SFDhmj//v168MEHlZubq44dOyozM9N2UtiuXbvsjsROnjxZFotFkydP1u7duxUbG6u0tDT93//9n69eAvxQ6XGrJr2bdcZBlqOxAAD4P5+fADZu3DiX0wpWrlxpd7tOnTrKyMhQRkaGFypDIMrMytGkd39QXtHpXfyACx4AABBYfB5mAU8osxo9v3ybnv7s59N+DKYTAAAQeAizCHiZWTl66P0flVtQUnlnJ4Is0vNDO+uq9kwnAAAg0BBmEdA8caLX80M7EWQBAAhQhFkErDM90YsTvAAACHyEWQSk0z3RKzKsjv7U5SxO8AIAoIYgzCLgnO7UguiIuvpmYi+F1OGqLgAA1BS8qyOgnO7UAoukR69tR5AFAKCG4Z0dASMzK0cXTvtMeUWlVdouoWGYZt3cmbmxAADUQEwzQEA43akFrB0LAEDNRpiF3yuzGk1dsqlKQbZxRIj+79q2HI0FAKCGI8zC763JzlNOfrHb/aMj6mrVxCuZHwsAQC3Auz383mebct3uy4leAADULrzjw69lZuXo1a92uNW3cUQIJ3oBAFDLMM0Afqt8rqw7mFoAAEDtxDs//FZV5soytQAAgNqJd3/4rdwC94LsrRcnMbUAAIBaijALv5SZlaNHPvjRrb69U+KruRoAAOCvmDMLv+PuBRIskuIbhqlbcrQ3ygIAAH6II7PwK+5eIKH8el4ZaSlc3QsAgFqMMAu/4u5JX9EswwUAAESYhZ9x96SvyQPaEGQBAABhFv6jKid9xTcMr+ZqAABAIOAEMPgFTvoCAACngyOz8DlO+gIAAKeLMAuf46QvAABwugiz8DlO+gIAAKeLMAuf4qQvAABwJjgBDD7DSV8AAOBMcWQWPsFJXwAAwBMIs/CJtTs46QsAAJw5wix8Yl9hiVv9OOkLAABUhDALn4iJCHGrHyd9AQCAinACGLxuxbbf9cznuyvsw0lfAADAHYRZeFVmVq4mfrC9wj6c9AUAANxFmIXXlFmNHv7gp0r7xTcMU0ZaCnNlAQBApQiz8Jo12XluXe3ryes76OLWMV6oCAAABDpOAIPX7Ct077K1B4rcW+kAAACAMAuviakf6la/uAZh1VwJAACoKZhmAK/IzMrRQ+//WGEfVjAAAABVRZhFtcvMytGYuesrvHQtKxgAAIDTQZhFtSqzGk1dsqnCICuxggEAADg9hFlUqzXZecrJZwUDAABQPTgBDNWKFQwAAEB1IsyiWrm7MgErGAAAgNNBmEW16pYcrYSGYXJ1SpdFUgIrGAAAgNNEmEW1Cg6yaMqAFKcngLGCAQAAOFOcAIZqlZmVo0c+3OT0PlYwAAAAZ4owi2pT2fqyUwa0IcgCAIAzwjQDVIvK1pe1SHrkw59UZq1sBVoAAADXCLOoFpWtL2sk5eQXa012nveKAgAANQ5hFtXC3fVl3e0HAADgDGEW1SKmfqhb/VhfFgAAnAlOAIPHZWbl6KH3f6ywj0UnVjNgfVkAAHAmCLPwqMpWMDgZ68sCAIAzxTQDeExlKxiUi6tfVzNv6sSyXAAA4IwRZuExla1gUG5KnyT1axvvhYoAAEBNR5iFx7i7MsHvR45XcyUAAKC2IMzCY9xdmaBxRN1qrgQAANQWhFl4TJfEKEVHhLi83yIpoWGYOjar772iAABAjUaYhUdkZuWoxxMrlFdU6vT+8jULpgxowwoGAADAYwizOGPly3FVdPJXfMMwzbq5Myd+AQAAj2KdWZwRd5bjio6oq8//1lMhdYJktVq9VhsAAKj5ODKLM+LOclx5Rce0bufvXqoIAADUJoRZnBF3l+Nytx8AAEBVEGZxRtxdjsvdfgAAAFVBmMUZ6ZYcrYSGroNq+XJc3ZKjvVcUAACoNXweZmfOnKmkpCSFhYUpNTVVa9asqbD/oUOHNHbsWCUkJCg0NFTnnHOOPvroIy9VC2duvKC50/byBbgy0lJYjgsAAFQLn65msHDhQqWnp+vFF19UamqqnnnmGfXt21dbtmxRXFycQ//S0lL17t1bcXFxeuedd9SsWTPt3LlTjRo18n7xUGZWjqYu2eTyBLD4hmHKSEtRv7YJXq4MAADUFj4NszNmzNDtt9+uUaNGSZJefPFFffjhh5ozZ47uv/9+h/5z5sxRXl6evv76a9Wte+KSqElJSd4sGf9TvrasqyW57uvVWuOuaM0RWQAAUK18FmZLS0u1bt06TZw40dYWFBSkXr16adWqVU63ef/999W9e3eNHTtW7733nmJjY3XTTTdpwoQJCg4OdrpNSUmJSkpKbLcLCgokSVartVrXPLVarTLG1Mh1VcusRg+973ptWYukBWt/1V2Xt5JF9mG2Jo/LmWJsXGNsXGNsXGNsnGNcXGNsXPP22FTleXwWZg8cOKCysjI1adLErr1JkybavHmz0222b9+u5cuXa9iwYfroo4+0bds23XXXXTp27JgyMjKcbjNt2jRNnTrVoX3//v0qLq6+5aKsVqvy8/NljFFQkM+nJnvUul8LlVvgeuyMpJz8Yn26Ybu6NG9gd19NHpczxdi4xti4xti4xtg4x7i4xti45u2xKSwsdLtvQF0BzGq1Ki4uTi+//LKCg4PVpUsX7d69W0888YTLMDtx4kSlp6fbbhcUFKh58+aKjY1VZGRktdZqsVgUGxtb434hjuUcd69fnXCHuc81eVzOFGPjGmPjGmPjGmPjHOPiGmPjmrfHJizM/SU9fRZmY2JiFBwcrL1799q17927V/Hx8U63SUhIUN26de2mFLRp00a5ubkqLS1VSEiIwzahoaEKDQ11aA8KCqr2H4bFYvHK83hbk8hwt/s5e+01dVw8gbFxjbFxjbFxjbFxjnFxjbFxzZtjU5Xn8NlPKiQkRF26dNGyZctsbVarVcuWLVP37t2dbnPxxRdr27ZtdvMofv75ZyUkJDgNsqge5WvLujq1i7VlAQCAt/j0z4709HTNnj1bb7zxhn766SeNGTNGRUVFttUNhg8fbneC2JgxY5SXl6fx48fr559/1ocffqhHH31UY8eO9dVLqJWCgyzKSEtxegIYa8sCAABv8umc2SFDhmj//v168MEHlZubq44dOyozM9N2UtiuXbvsDjM3b95cn3zyie677z61b99ezZo10/jx4zVhwgRfvYRaq1/bBF3cqrG++uWgXTtrywIAAG/y+Qlg48aN07hx45zet3LlSoe27t2765tvvqnmqlCRMqvRmuyD+nHPiWXO6gRJ0we3V7NG9dQtOZojsgAAwGt8HmYRWJxd9SsoKEj1Q+uoe6vGPqwMAADURpyqB7eVX/Xr1MvXlh63aszc9crMyvFRZQAAoLYizMItZVajqUtcX/VLkqYu2aQya0U9AAAAPKvKYTYpKUkPP/ywdu3aVR31wE+tyc5zOCJ7svKrfq3JzvNeUQAAoNarcpi999579Z///EctW7ZU7969tWDBApWUlFRHbfAj+wrdu/Svu/0AAAA84bTC7MaNG7VmzRq1adNGd999txISEjRu3DitX7++OmqEH4hr4N5l5dztBwAA4AmnPWe2c+fOeu6557Rnzx5lZGTolVde0QUXXKCOHTtqzpw5Moa5kzUJV/0CAAD+6LTD7LFjx/T2229r4MCB+stf/qKuXbvqlVde0eDBgzVp0iQNGzbMk3XCx7jqFwAA8EdVXmd2/fr1eu211zR//nwFBQVp+PDhevrpp3XeeefZ+lx77bW64IILPFoofKvMatQwPETJMfWUfeCI3X1c9QsAAPhKlcPsBRdcoN69e2vWrFkaNGiQ6tat69AnOTlZN954o0cKhO85u1CCJI28KFF9z0/gql8AAMBnqhxmt2/frsTExAr7RERE6LXXXjvtouA/yi+U4Gx6wRtf79SFLRsTZAEAgM9Uec7svn37tHr1aof21atX69tvv/VIUfAPXCgBAAD4uyqH2bFjx+rXX391aN+9e7fGjh3rkaLgH7hQAgAA8HdVDrObNm1S586dHdo7deqkTZs2eaQo+AculAAAAPxdlcNsaGio9u7d69Cek5OjOnWqPAUXfowLJQAAAH9X5TDbp08fTZw4Ufn5+ba2Q4cOadKkSerdu7dHi4NvcaEEAADg76ocZp988kn9+uuvSkxMVM+ePdWzZ08lJycrNzdXTz31VHXUCB8pv1CCM1woAQAA+IMqh9lmzZrp+++/1+OPP66UlBR16dJFzz77rH744Qc1b968OmqED/Vrm6Dpg9s7tMc3DNOsmztzoQQAAOBTpzXJNSIiQqNHj/Z0LfBTjer9cWGMK9vE6c+XtORCCQAAwC+c9hlbmzZt0q5du1RaWmrXPnDgwDMuCv5l/c7fbd8P6dpc3Vs19mE1AAAAfzitK4Bde+21+uGHH2SxWGTMiQXzLZYTR+nKyso8WyF8qsxqtGLLPtvtDs0b+a4YAACAU1R5zuz48eOVnJysffv2qV69evrxxx/1xRdfqGvXrlq5cmU1lAhfyczK0cXTl+nnvYdtbYNmfqXMrBwfVgUAAPCHKofZVatW6eGHH1ZMTIyCgoIUFBSkSy65RNOmTdM999xTHTXCBzKzcjRm7nrlFpTYtefmF2vM3PUEWgAA4BeqHGbLysrUoEEDSVJMTIz27NkjSUpMTNSWLVs8Wx18osxqNHXJJhkn95W3TV2ySWVWZz0AAAC8p8pzZtu2bavvvvtOycnJSk1N1eOPP66QkBC9/PLLatmyZXXUCC9bk52nnHzXl6g1knLyi7UmO4+TwQAAgE9VOcxOnjxZRUVFkqSHH35YV199tS699FI1btxYCxcu9HiB8L59ha6D7On0AwAAqC5VDrN9+/a1fX/22Wdr8+bNysvLU1RUlG1FAwS2uAZhHu0HAABQXao0Z/bYsWOqU6eOsrKy7Nqjo6MJsjVIt+RoJTR0HVQtkhIahqlbcrT3igIAAHCiSmG2bt26atGiBWvJ1nDBQRZlpKU4va/8T5aMtBSuAAYAAHyuyqsZPPDAA5o0aZLy8vKqox74iX5tE9Q8KtyhPb5hmGbd3Fn92ib4oCoAAAB7VZ4z+/zzz2vbtm1q2rSpEhMTFRERYXf/+vXrPVYcfKeo5Lh2HzoqSWoRXU9/6XOO4hqcmFrAEVkAAOAvqhxmBw0aVA1lwN989+shlS8je9k5MbqmYzPfFgQAAOBElcNsRkZGddQBP7Nu5++277skRvmwEgAAANeqPGcWNV+Z1eizzXtttzueRZgFAAD+qcphNigoSMHBwS6/ENgys3J08fTl+u7XfFvb0Fe+UWZWjg+rAgAAcK7K0wzeffddu9vHjh3Thg0b9MYbb2jq1KkeKwzel5mVozFz18uc0r43v1hj5q5nFQMAAOB3qhxmr7nmGoe266+/Xueff74WLlyo2267zSOFwbvKrEZTl2xyCLKSZHRifdmpSzapd0o8qxkAAAC/4bE5sxdeeKGWLVvmqYeDl63JzlNOfrHL+42knPxirclmfWEAAOA/PBJmjx49queee07NmrF8U6DaV+g6yJ5OPwAAAG+o8jSDqKgoWSx/fMxsjFFhYaHq1aunuXPnerQ4eE9cgzCP9gMAAPCGKofZp59+2i7MBgUFKTY2VqmpqYqKYgmnQNUtOVoJDcNcTjWw6MSlbLslR3u3MAAAgApUOcyOHDmyGsqArwUHWZSRlqI75zpejrj8T5eMtBRO/gIAAH6lynNmX3vtNS1atMihfdGiRXrjjTc8UhR8o1/bBPVOaeLQHt8wjGW5AACAX6pymJ02bZpiYmIc2uPi4vToo496pCj4TvGxMtv3D19zvubffqG+nHAFQRYAAPilKk8z2LVrl5KTkx3aExMTtWvXLo8UBd8wxmjTngJJUuOIEN1yYaLd/GgAAAB/U+Ujs3Fxcfr+++8d2r/77js1btzYI0XBN/YXluhgUakkqU1CJEEWAAD4vSqH2aFDh+qee+7RihUrVFZWprKyMi1fvlzjx4/XjTfeWB01wks25RTYvk9pGunDSgAAANxT5WkGjzzyiHbs2KErr7xSdeqc2NxqtWr48OHMmQ1wJ4fZNgkNfFgJAACAe6ocZkNCQrRw4UL94x//0MaNGxUeHq527dopMTGxOuqDl5RZjf679YDt9rlNODILAAD8X5XDbLnWrVurdevWnqwFPpKZlaOpSzbZXTDh1tfX6qGBKaxiAAAA/FqV58wOHjxYjz32mEP7448/rj/96U8eKQrek5mVozFz1ztc+WtvQbHGzF2vzKwcH1UGAABQuSqH2S+++EJXXXWVQ3v//v31xRdfeKQoeEeZ1Wjqkk0yTu4rb5u6ZJPKrM56AAAA+F6Vw+zhw4cVEhLi0F63bl0VFBQ42QL+ak12nsMR2ZMZSTn5xVqTnee9ogAAAKqgymG2Xbt2WrhwoUP7ggULlJKS4pGi4B37Cl0H2dPpBwAA4G1VPgFsypQpuu666/TLL7/oiiuukCQtW7ZMb731lt555x2PF4jqE9cgzKP9AAAAvK3KYTYtLU2LFy/Wo48+qnfeeUfh4eHq0KGDli9frujo6OqoEdWkW3K0EhqGKTe/2Om8WYuk+IZh6pbMzxUAAPinKk8zkKQBAwboq6++UlFRkbZv364bbrhBf/3rX9WhQwdP14dqFBxkUUaa86kh5ReyzUhLUXAQl7UFAAD+6bTCrHRiVYMRI0aoadOmeuqpp3TFFVfom2++8WRt8IJ+bRP08DVtHdrjG4Zp1s2dWWcWAAD4tSpNM8jNzdXrr7+uV199VQUFBbrhhhtUUlKixYsXc/JXAGseHW77/so2cfrzJS3VLTmaI7IAAMDvuX1kNi0tTeeee66+//57PfPMM9qzZ4/++c9/Vmdt8JIdB4ps3/dNiVf3Vo0JsgAAICC4fWT2448/1j333KMxY8ZwGdsaJvukMJsUE+HDSgAAAKrG7SOzX375pQoLC9WlSxelpqbq+eef14EDB6qzNnhJ9sEjtu+TYur5sBIAAICqcTvMXnjhhZo9e7ZycnJ0xx13aMGCBWratKmsVquWLl2qwsLC6qwT1Sj7wGFJUv3QOoqtH+rjagAAANxX5dUMIiIidOutt+rLL7/UDz/8oL/85S+aPn264uLiNHDgwOqoEdWo9LhVu38/KunEUVmLhbmyAAAgcJz20lySdO655+rxxx/Xb7/9pvnz53uqJnjRrrwjsv7vignJMfV9WwwAAEAVnVGYLRccHKxBgwbp/fff98TDwYtOPvkruTHzZQEAQGDxSJg9UzNnzlRSUpLCwsKUmpqqNWvWuLXdggULZLFYNGjQoOotsAY7eVmu5FhWMgAAAIHF52F24cKFSk9PV0ZGhtavX68OHTqob9++2rdvX4Xb7dixQ3/961916aWXeqnSmqfMarRq+0Hb7eZRHJkFAACBxedhdsaMGbr99ts1atQopaSk6MUXX1S9evU0Z84cl9uUlZVp2LBhmjp1qlq2bOnFamuOzKwcXfLYci3f/McfDWPfWq/MrBwfVgUAAFA1VbqcraeVlpZq3bp1mjhxoq0tKChIvXr10qpVq1xu9/DDDysuLk633Xab/vvf/1b4HCUlJSopKbHdLigokCRZrVZZrdYzfAWuWa1WGWOq9TlOV2ZWrsa+tUHmlPZ9BSUaM3e9Zt7USf3axlfLc/vzuPgaY+MaY+MaY+MaY+Mc4+IaY+Oat8emKs/j0zB74MABlZWVqUmTJnbtTZo00ebNm51u8+WXX+rVV1/Vxo0b3XqOadOmaerUqQ7t+/fvV3FxcZVrdpfValV+fr6MMQoK8vkBcJsyq9FD72c5BFlJtrap72epQ4ylWi5p66/j4g8YG9cYG9cYG9cYG+cYF9cYG9e8PTZVuX6BT8NsVRUWFuqWW27R7NmzFRMT49Y2EydOVHp6uu12QUGBmjdvrtjYWEVGRlZXqbJarbJYLIqNjfWrX4hvth/UvsPHKuyz9/Ax7TxSRxe2bOzx5/fXcfEHjI1rjI1rjI1rjI1zjItrjI1r3h6bsLAwt/v6NMzGxMQoODhYe/futWvfu3ev4uMdP+b+5ZdftGPHDqWlpdnayg9D16lTR1u2bFGrVq3stgkNDVVoqONVrYKCgqr9h2GxWLzyPFWx/3Cp2/2qq25/HBd/wdi4xti4xti4xtg4x7i4xti45s2xqcpz+PQnFRISoi5dumjZsmW2NqvVqmXLlql79+4O/c877zz98MMP2rhxo+1r4MCB6tmzpzZu3KjmzZt7s/yAFNfAvb903O0HAADgSz6fZpCenq4RI0aoa9eu6tatm5555hkVFRVp1KhRkqThw4erWbNmmjZtmsLCwtS2bVu77Rs1aiRJDu1wrltytBIahik3v9jpvFmLpPiGYeqWHO3t0gAAAKrM52F2yJAh2r9/vx588EHl5uaqY8eOyszMtJ0UtmvXLg71e1BwkEUZaSkaM3e9w33lp3tlpKVUy8lfAAAAnubzMCtJ48aN07hx45zet3Llygq3ff311z1fUA3Xr22CZt3cWXfNWy/rSYdn4xuGKSMtRf3aJviuOAAAgCrwizAL77vo7BhbkG0VG6F/DGqnbsnRHJEFAAABhTBbS+04UGT7/oKkaHVv5flluAAAAKobk1FrqeyTwmxSTIQPKwEAADh9hNla6uQwm0yYBQAAAYowW0sRZgEAQE1AmK2lyufMWixSi+h6Pq4GAADg9BBmayFjjLb/L8w2bRiusLrBPq4IAADg9BBma6G8olIVFh+XxBQDAAAQ2AiztRDzZQEAQE1BmK2FWJYLAADUFITZWqbMavTVtgO220mc/AUAAAIYYbYWyczK0SWPLdfijXtsbff/5wdlZuX4sCoAAIDTR5itJTKzcjRm7nrl5BfbtR84XKIxc9cTaAEAQEAizNYCZVajqUs2yTi5r7xt6pJNKrM66wEAAOC/CLO1wJrsPIcjsiczknLyi7UmO897RQEAAHgAYbYW2FfoOsieTj8AAAB/QZitBeIahHm0HwAAgL8gzNYC3ZKjldAwTBYX91skJTQMU7fkaG+WBQAAcMYIs7VAcJBFGWkpTu8rD7gZaSkKDnIVdwEAAPwTYbaW6Nc2QRkDHQNtfMMwzbq5s/q1TfBBVQAAAGemjq8LgPc0Cg+xfX9V23jd0j1J3ZKjOSILAAACFmG2Fvnut0O274d0a6HurRr7rhgAAAAPYJpBLfLdr4ds37dv1tB3hQAAAHgIR2ZrgTKr0apfDuj73/IlSc2jwhUVEVLJVgAAAP6PMFvDZWblaOqSTXZXANt/uESZWTmc9AUAAAIe0wxqsMysHI2Zu97hUrbFx6waM3e9MrNyfFQZAACAZxBma6gyq9HUJZtkKugzdckmlVkr6gEAAODfCLM11JrsPIcjsiczknLyi7UmO897RQEAAHgYYbaG2lfoOsieTj8AAAB/RJitoeIahHm0HwAAgD8izNZQ3ZKjldAwTK6u7WWRlNAwTN2So71ZFgAAgEcRZmuo4CCLMtJSnN5XHnAz0lK4lC0AAAhohNkarHdKvPq2beLQHt8wTLNu7sw6swAAIOBx0YQaytnFEuqHBuv2S1tq3BWtOSILAABqBI7M1kCuLpZQVFKmZz7bqqWbcn1UGQAAgGcRZmuYii6WUN7GxRIAAEBNQZitYbhYAgAAqE0IszUMF0sAAAC1CWG2huFiCQAAoDYhzNYw5RdLcIWLJQAAgJqEMFsD9XOytqzExRIAAEDNwzqzNYiztWVPFt8wTBlpKVwsAQAA1BiE2RqifG1ZVwtu3derNRdLAAAANQ7TDGqAitaWlU5ML1iw9ldvlgQAAOAVhNkagLVlAQBAbUWYrQFYWxYAANRWhNkagLVlAQBAbUWYrQFYWxYAANRWhNkaIDjIooy0FKf3sbYsAACoyQizNUS/tgka0rW5Q3t8wzDNurkza8sCAIAaiXVma5C6df448nrvla2V2rKxuiVHc0QWAADUWITZGmTr3sO270denKRG9UJ8WA0AAED1Y5pBDbJt34kwG9sglCALAABqBcJsDXHwcIkOFpVKks5pUt/H1QAAAHgHYbaG2LrvjykGreMa+LASAAAA7yHM1hAnh9mz4zgyCwAAagfCbA2xdW+h7ftzmnBkFgAA1A6E2RqgzGr07Y7fbbdbxkT4sBoAAADvIcwGuMysHF3y2HJtyimwtaU9/6Uys3J8WBUAAIB3EGYDWGZWjsbMXa+c/GK79tz8Yo2Zu55ACwAAajzCbIAqsxpNXbJJxsl95W1Tl2xSmdVZDwAAgJqBMBug1mTnORyRPZmRlJNfrDXZed4rCgAAwMsIswEqt8B1kD3ZvkL3+gEAAAQiwmwAyszK0SMf/OhW37gGYdVcDQAAgO/U8XUBqJryk74qmwlrkRTfMEzdkqO9URYAAIBPcGQ2gJQet2rSu1luBVlJykhLUXCQpcK+AAAAgcwvwuzMmTOVlJSksLAwpaamas2aNS77zp49W5deeqmioqIUFRWlXr16Vdi/psjMytGF0z5TXlFppX2jI0I06+bO6tc2wQuVAQAA+I7Pw+zChQuVnp6ujIwMrV+/Xh06dFDfvn21b98+p/1XrlypoUOHasWKFVq1apWaN2+uPn36aPfu3V6u3HvKpxbkFR1zq//kAW0IsgAAoFbweZidMWOGbr/9do0aNUopKSl68cUXVa9ePc2ZM8dp/3nz5umuu+5Sx44ddd555+mVV16R1WrVsmXLvFy5d1S0nqwr8Q3Dq60eAAAAf+LTE8BKS0u1bt06TZw40dYWFBSkXr16adWqVW49xpEjR3Ts2DFFRzs/0amkpEQlJSW22wUFJy77arVaZbVaz6D6ilmtVhljzvg5Vm8/WOF6sicrP+mra2Kjan1tZ8JT41ITMTauMTauMTauMTbOMS6uMTaueXtsqvI8Pg2zBw4cUFlZmZo0aWLX3qRJE23evNmtx5gwYYKaNm2qXr16Ob1/2rRpmjp1qkP7/v37VVxcfWuwWq1W5efnyxijoKDTPwC+7Tf3L3pgJN1zaVMdPLD/tJ+vunlqXGoixsY1xsY1xsY1xsY5xsU1xsY1b49NYWGh230Demmu6dOna8GCBVq5cqXCwpyvpzpx4kSlp6fbbhcUFKh58+aKjY1VZGRktdVmtVplsVgUGxt7Rj/0sw8HS8qutF/jiBA9cs356tc2/rSfyxs8NS41EWPjGmPjGmPjGmPjHOPiGmPjmrfHxlWuc8anYTYmJkbBwcHau3evXfvevXsVH19xKHvyySc1ffp0ffbZZ2rfvr3LfqGhoQoNDXVoDwoKqvYfhsViOePnSW0Zo4SGYcrNL3Y5bzY6oq5WTbxSIXUC4xfPE+NSUzE2rjE2rjE2rjE2zjEurjE2rnlzbKryHD79SYWEhKhLly52J2+Vn8zVvXt3l9s9/vjjeuSRR5SZmamuXbt6o1SfCQ6yKCMtxWmQtfzv69Fr2wVMkAUAAPAknyeg9PR0zZ49W2+88YZ++uknjRkzRkVFRRo1apQkafjw4XYniD322GOaMmWK5syZo6SkJOXm5io3N1eHDx/21Uuodv3aJuiWCxMd2uMbhrGeLAAAqNV8Pmd2yJAh2r9/vx588EHl5uaqY8eOyszMtJ0UtmvXLrtDzbNmzVJpaamuv/56u8fJyMjQQw895M3SvaLMarQmO08/7sm3td3bq7VSkxurW3I0V/gCAAC1ms/DrCSNGzdO48aNc3rfypUr7W7v2LGj+gvyE5lZOZq6ZJPd0lwWSa3j6qt7q8a+KwwAAMBP+EWYhaPyq36dOlfWSBr31gYFB1mYXgAAAGo9n8+ZhSN3rvo1dckmlVmrcl0wAACAmocw64fWZOdVeNUvIyknv1hrst2/oAIAAEBNRJj1Q/sK3bsymbv9AAAAairCrB+Ka+DeVS/c7QcAAFBTEWb9ULfkaCU0DJOrRbcskhIahqlbcrQ3ywIAAPA7hFk/VH7VL2fKA25GWgprzAIAgFqPMOun+rVN0KybO6tBmP3qaVz1CwAA4A+sM+vH+rVN0Dfb8/T61zskSQ9e3UYjLkrmiCwAAMD/cGTWz+XkH7V93+f8eIIsAADASQizfm73oRNhNjjIovhIVi8AAAA4GWHWz/32+4kwGx8ZpjrB/LgAAABORjryY0Ulx3XoyDFJUrNG4T6uBgAAwP8QZv1Y+RQDSTorijALAABwKsKsH9v9+x9hthlhFgAAwAFh1o/9dtKRWaYZAAAAOCLM+jGOzAIAAFSMMOvHdnNkFgAAoEKEWT+2+/cjtu+bEmYBAAAcEGb9WPmR2Zj6oQqrG+zjagAAAPwPYdZPlRwv096CEknMlwUAAHCFMOuncg4V274/iykGAAAAThFm/dSveX/Mlw2ySGVW48NqAAAA/BNh1g9lZuVo3PwNtttLvs/RJY8tV2ZWjg+rAgAA8D+EWT+TmZWjMXPXK//oMbv23PxijZm7nkALAABwEsKsHymzGk1dsknOJhSUt01dsokpBwAAAP9DmPUja7LzlJNf7PJ+Iyknv1hrsvO8VxQAAIAfq+PrAmqyMqvRN9sPav/hUsVEhEoW6cDhErvv4xqEqVtytIKDLNpX6DrInszdfgAAADUdYbaaZGbl6qH3s7Tv8LFK+yY0DFNGWoriGoS59dju9gMAAKjpmGZQDTKzcjT2rQ1uBVnpxNSBO+eu1zfbDyg+0nVQtehE8O2WHO2hSgEAAAIbYdbDKjqJqzLPLtum4mNlTu+z/O/fjLQUBQdZnPYBAACobZhm4GGVncRVmUNHnR/Njf/fVIR+bRNO+7EBAABqGsKsh3ny5KykxuG6r/e5dieJAQAA4A+EWQ/z5MlZZ8c10DUdm3ns8QAAAGoa5sx6WLfkaCU0DJMnjqE2qeBkMAAAABBmPS44yKKMtBSPPFan5o088jgAAAA1FWG2GvRrm6CZN3VSXP26Z/Q4T376szKzcjxUFQAAQM3DnNlq0q9tvDrEWLTzSB2nVwBbuyNPzyzbWuFj7C0o1pi56zXr5s6sYgAAAOAEYbYaBQdZdGHLxgoKcjwAfnHrGJ2X0EAPvf+jcgtKnG5vdGJ92alLNql3SjyrGQAAAJyCaQY+1K9tgp66oWOFfYxOXCFsTXaeV2oCAAAIJIRZHztw2PlR2VN5cv1aAACAmoIw62PurkvryfVrAQAAagrCrI9Vti6tRVJCwxNXAAMAAIA9wqyPnbwu7amBtvx2RloKJ38BAAA4QZj1A/3aJmjWzZ0V39B+KkF8wzCW5QIAAKgAS3P5iX5tE9Q7JV5rsvO0r7BYcQ1OTC3giCwAAIBrhFk/EhxkUfdWjX1dBgAAQMBgmgEAAAACFmEWAAAAAYswCwAAgIBFmAUAAEDAIswCAAAgYBFmAQAAELAIswAAAAhYhFkAAAAELMIsAAAAAhZhFgAAAAGLMAsAAICARZgFAABAwCLMAgAAIGARZgEAABCwCLMAAAAIWIRZAAAABCzCLAAAAAIWYRYAAAABizALAACAgOUXYXbmzJlKSkpSWFiYUlNTtWbNmgr7L1q0SOedd57CwsLUrl07ffTRR16qFAAAAP7E52F24cKFSk9PV0ZGhtavX68OHTqob9++2rdvn9P+X3/9tYYOHarbbrtNGzZs0KBBgzRo0CBlZWV5uXIAAAD4ms/D7IwZM3T77bdr1KhRSklJ0Ysvvqh69eppzpw5Tvs/++yz6tevn/72t7+pTZs2euSRR9S5c2c9//zzXq4cAAAAvlbHl09eWlqqdevWaeLEiba2oKAg9erVS6tWrXK6zapVq5Senm7X1rdvXy1evNhp/5KSEpWUlNhu5+fnS5IOHTokq9V6hq/ANavVqoKCAoWEhCgoyOd/M/gNxsU1xsY1xsY1xsY1xsY5xsU1xsY1b49NQUGBJMkYU2lfn4bZAwcOqKysTE2aNLFrb9KkiTZv3ux0m9zcXKf9c3NznfafNm2apk6d6tCemJh4mlUDAADAGwoLC9WwYcMK+/g0zHrDxIkT7Y7kWq1W5eXlqXHjxrJYLNX2vAUFBWrevLl+/fVXRUZGVtvzBBrGxTXGxjXGxjXGxjXGxjnGxTXGxjVvj40xRoWFhWratGmlfX0aZmNiYhQcHKy9e/fate/du1fx8fFOt4mPj69S/9DQUIWGhtq1NWrU6PSLrqLIyEh+IZxgXFxjbFxjbFxjbFxjbJxjXFxjbFzz5thUdkS2nE8nhISEhKhLly5atmyZrc1qtWrZsmXq3r270226d+9u11+Sli5d6rI/AAAAai6fTzNIT0/XiBEj1LVrV3Xr1k3PPPOMioqKNGrUKEnS8OHD1axZM02bNk2SNH78ePXo0UNPPfWUBgwYoAULFujbb7/Vyy+/7MuXAQAAAB/weZgdMmSI9u/frwcffFC5ubnq2LGjMjMzbSd57dq1y+6suYsuukhvvfWWJk+erEmTJql169ZavHix2rZt66uX4FRoaKgyMjIcpjjUdoyLa4yNa4yNa4yNa4yNc4yLa4yNa/48NhbjzpoHAAAAgB9iETUAAAAELMIsAAAAAhZhFgAAAAGLMAsAAICARZitBjNnzlRSUpLCwsKUmpqqNWvW+Lokr5s2bZouuOACNWjQQHFxcRo0aJC2bNli1+fyyy+XxWKx+7rzzjt9VLH3PPTQQw6v+7zzzrPdX1xcrLFjx6px48aqX7++Bg8e7HChkJooKSnJYVwsFovGjh0rqXbtL1988YXS0tLUtGlTWSwWLV682O5+Y4wefPBBJSQkKDw8XL169dLWrVvt+uTl5WnYsGGKjIxUo0aNdNttt+nw4cNefBXVo6KxOXbsmCZMmKB27dopIiJCTZs21fDhw7Vnzx67x3C2r02fPt3Lr8TzKttvRo4c6fC6+/XrZ9enNu43kpz+32OxWPTEE0/Y+tTE/cad92p33pN27dqlAQMGqF69eoqLi9Pf/vY3HT9+3GuvgzDrYQsXLlR6eroyMjK0fv16dejQQX379tW+fft8XZpXff755xo7dqy++eYbLV26VMeOHVOfPn1UVFRk1+/2229XTk6O7evxxx/3UcXedf7559u97i+//NJ233333aclS5Zo0aJF+vzzz7Vnzx5dd911PqzWO9auXWs3JkuXLpUk/elPf7L1qS37S1FRkTp06KCZM2c6vf/xxx/Xc889pxdffFGrV69WRESE+vbtq+LiYlufYcOG6ccff9TSpUv1wQcf6IsvvtDo0aO99RKqTUVjc+TIEa1fv15TpkzR+vXr9Z///EdbtmzRwIEDHfo+/PDDdvvS3Xff7Y3yq1Vl+40k9evXz+51z58/3+7+2rjfSLIbk5ycHM2ZM0cWi0WDBw+261fT9ht33qsre08qKyvTgAEDVFpaqq+//lpvvPGGXn/9dT344IPeeyEGHtWtWzczduxY2+2ysjLTtGlTM23aNB9W5Xv79u0zksznn39ua+vRo4cZP36874rykYyMDNOhQwen9x06dMjUrVvXLFq0yNb2008/GUlm1apVXqrQP4wfP960atXKWK1WY0zt3V8kmXfffdd222q1mvj4ePPEE0/Y2g4dOmRCQ0PN/PnzjTHGbNq0yUgya9eutfX5+OOPjcViMbt37/Za7dXt1LFxZs2aNUaS2blzp60tMTHRPP3009VbnI85G5sRI0aYa665xuU27Dd/uOaaa8wVV1xh11Yb9ptT36vdeU/66KOPTFBQkMnNzbX1mTVrlomMjDQlJSVeqZsjsx5UWlqqdevWqVevXra2oKAg9erVS6tWrfJhZb6Xn58vSYqOjrZrnzdvnmJiYtS2bVtNnDhRR44c8UV5Xrd161Y1bdpULVu21LBhw7Rr1y5J0rp163Ts2DG7fei8885TixYtatU+VFpaqrlz5+rWW2+VxWKxtdfW/eVk2dnZys3NtdtHGjZsqNTUVNs+smrVKjVq1Ehdu3a19enVq5eCgoK0evVqr9fsS/n5+bJYLGrUqJFd+/Tp09W4cWN16tRJTzzxhFc/EvWllStXKi4uTueee67GjBmjgwcP2u5jvzlh7969+vDDD3Xbbbc53FfT95tT36vdeU9atWqV2rVrZ7vYlST17dtXBQUF+vHHH71St8+vAFaTHDhwQGVlZXY/UElq0qSJNm/e7KOqfM9qteree+/VxRdfbHeltptuukmJiYlq2rSpvv/+e02YMEFbtmzRf/7zHx9WW/1SU1P1+uuv69xzz1VOTo6mTp2qSy+9VFlZWcrNzVVISIjDG2+TJk2Um5vrm4J9YPHixTp06JBGjhxpa6ut+8upyvcDZ//PlN+Xm5uruLg4u/vr1Kmj6OjoWrUfFRcXa8KECRo6dKgiIyNt7ffcc486d+6s6Ohoff3115o4caJycnI0Y8YMH1Zb/fr166frrrtOycnJ+uWXXzRp0iT1799fq1atUnBwMPvN/7zxxhtq0KCBw/Sumr7fOHuvduc9KTc31+n/R+X3eQNhFtVu7NixysrKspsXKsluHla7du2UkJCgK6+8Ur/88otatWrl7TK9pn///rbv27dvr9TUVCUmJurtt99WeHi4DyvzH6+++qr69++vpk2b2tpq6/6C03Ps2DHdcMMNMsZo1qxZdvelp6fbvm/fvr1CQkJ0xx13aNq0aX55qU5PufHGG23ft2vXTu3bt1erVq20cuVKXXnllT6szL/MmTNHw4YNU1hYmF17Td9vXL1XBwKmGXhQTEyMgoODHc7y27t3r+Lj431UlW+NGzdOH3zwgVasWKGzzjqrwr6pqamSpG3btnmjNL/RqFEjnXPOOdq2bZvi4+NVWlqqQ4cO2fWpTfvQzp079dlnn+nPf/5zhf1q6/5Svh9U9P9MfHy8w0mnx48fV15eXq3Yj8qD7M6dO7V06VK7o7LOpKam6vjx49qxY4d3CvQTLVu2VExMjO13qLbvN5L03//+V1u2bKn0/x+pZu03rt6r3XlPio+Pd/r/Ufl93kCY9aCQkBB16dJFy5Yts7VZrVYtW7ZM3bt392Fl3meM0bhx4/Tuu+9q+fLlSk5OrnSbjRs3SpISEhKquTr/cvjwYf3yyy9KSEhQly5dVLduXbt9aMuWLdq1a1et2Ydee+01xcXFacCAARX2q637S3JysuLj4+32kYKCAq1evdq2j3Tv3l2HDh3SunXrbH2WL18uq9Vq+yOgpioPslu3btVnn32mxo0bV7rNxo0bFRQU5PARe03322+/6eDBg7bfodq835R79dVX1aVLF3Xo0KHSvjVhv6nsvdqd96Tu3bvrhx9+sPtDqPyPyJSUFK+9EHjQggULTGhoqHn99dfNpk2bzOjRo02jRo3szvKrDcaMGWMaNmxoVq5caXJycmxfR44cMcYYs23bNvPwww+bb7/91mRnZ5v33nvPtGzZ0lx22WU+rrz6/eUvfzErV6402dnZ5quvvjK9evUyMTExZt++fcYYY+68807TokULs3z5cvPtt9+a7t27m+7du/u4au8oKyszLVq0MBMmTLBrr237S2FhodmwYYPZsGGDkWRmzJhhNmzYYDsjf/r06aZRo0bmvffeM99//7255pprTHJysjl69KjtMfr162c6depkVq9ebb788kvTunVrM3ToUF+9JI+paGxKS0vNwIEDzVlnnWU2btxo939P+VnVX3/9tXn66afNxo0bzS+//GLmzp1rYmNjzfDhw338ys5cRWNTWFho/vrXv5pVq1aZ7Oxs89lnn5nOnTub1q1bm+LiYttj1Mb9plx+fr6pV6+emTVrlsP2NXW/qey92pjK35OOHz9u2rZta/r06WM2btxoMjMzTWxsrJk4caLXXgdhthr885//NC1atDAhISGmW7du5ptvvvF1SV4nyenXa6+9ZowxZteuXeayyy4z0dHRJjQ01Jx99tnmb3/7m8nPz/dt4V4wZMgQk5CQYEJCQkyzZs3MkCFDzLZt22z3Hz161Nx1110mKirK1KtXz1x77bUmJyfHhxV7zyeffGIkmS1btti117b9ZcWKFU5/f0aMGGGMObE815QpU0yTJk1MaGioufLKKx3G7ODBg2bo0KGmfv36JjIy0owaNcoUFhb64NV4VkVjk52d7fL/nhUrVhhjjFm3bp1JTU01DRs2NGFhYaZNmzbm0UcftQt0gaqisTly5Ijp06ePiY2NNXXr1jWJiYnm9ttvdzjQUhv3m3IvvfSSCQ8PN4cOHXLYvqbuN5W9Vxvj3nvSjh07TP/+/U14eLiJiYkxf/nLX8yxY8e89jos/3sxAAAAQMBhziwAAAACFmEWAAAAAYswCwAAgIBFmAUAAEDAIswCAAAgYBFmAQAAELAIswAAAAhYhFkAAAAELMIsANRSFotFixcv9nUZAHBGCLMA4AMjR46UxWJx+OrXr5+vSwOAgFLH1wUAQG3Vr18/vfbaa3ZtoaGhPqoGAAITR2YBwEdCQ0MVHx9v9xUVFSXpxBSAWbNmqX///goPD1fLli31zjvv2G3/ww8/6IorrlB4eLgaN26s0aNH6/Dhw3Z95syZo/PPP1+hoaFKSEjQuHHj7O4/cOCArr32WtWrV0+tW7fW+++/X70vGgA8jDALAH5qypQpGjx4sL777jsNGzZMN954o3766SdJUlFRkfr27auoqCitXbtWixYt0meffWYXVmfNmqWxY8dq9OjR+uGHH/T+++/r7LPPtnuOqVOn6oYbbtD333+vq666SsOGDVNeXp5XXycAnAmLMcb4uggAqG1GjhypuXPnKiwszK590qRJmjRpkiwWi+68807NmjXLdt+FF16ozp0764UXXtDs2bM1YcIE/frrr4qIiJAkffTRR0pLS9OePXvUpEkTNWvWTKNGjdI//vEPpzVYLBZNnjxZjzzyiKQTAbl+/fr6+OOPmbsLIGAwZxYAfKRnz552YVWSoqOjbd93797d7r7u3btr48aNkqSffvpJHTp0sAVZSbr44otltVq1ZcsWWSwW7dmzR1deeWWFNbRv3972fUREhCIjI7Vv377TfUkA4HWEWQDwkYiICIeP/T0lPDzcrX5169a1u22xWGS1WqujJACoFsyZBQA/9c033zjcbtOmjSSpTZs2+u6771RUVGS7/6uvvlJQUJDOPfdcNWjQQElJSVq2bJlXawYAb+PILAD4SElJiXJzc+3a6tSpo5iYGEnSokWL1LVrV11yySWaN2+e1qxZo1dffVWSNGzYMGVkZGjEiBF66KGHtH//ft1999265ZZb1KRJE0nSQw89pDvvvFNxcXHq37+/CgsL9dVXX+nuu+/27gsFgGpEmAUAH8nMzFRCQoJd27nnnqvNmzdLOrHSwIIFC3TXXXcpISFB8+fPV0pKiiSpXr16+uSTTzR+/HhdcMEFqlevngYPHqwZM2bYHmvEiBEqLi7W008/rb/+9a+KiYnR9ddf770XCABewGoGAOCHLBaL3n33XQ0aNMjXpQCAX2POLAAAAAIWYRYAAAABizmzAOCHmAEGAO7hyCwAAAACFmEWAAAAAYswCwAAgIBFmAUAAEDAIswCAAAgYBFmAQAAELAIswAAAAhYhFkAAAAErP8H7S4kPHKdM/4AAAAASUVORK5CYII=\n"
          },
          "metadata": {}
        }
      ]
    }
  ]
}