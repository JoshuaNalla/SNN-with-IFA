{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "machine_shape": "hm",
      "gpuType": "A100",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/JoshuaNalla/SNN-with-IFA/blob/main/DFA_version_of_Snn2.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 40,
      "metadata": {
        "id": "CbNYPZ5_wvtf"
      },
      "outputs": [],
      "source": [
        "\n",
        "# Import core libraries\n",
        "import math\n",
        "import numpy as np\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "from torchvision import datasets, transforms\n",
        "from torch.utils.data import DataLoader\n",
        "\n",
        "# for evaluation purposes, importing scikit.metric\n",
        "from sklearn.metrics import confusion_matrix"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# def compute_weight_stats(num_neurons, v_mean=8.0, v_second_moment=164.0, alpha=0.066):\n",
        "#     \"\"\"\n",
        "#     Compute weight initialization statistics.\n",
        "#     Paper reference: Appendix C, Equations A.7 and A.8\n",
        "\n",
        "#     Args:\n",
        "#         num_neurons: Number of neurons in layer (N)\n",
        "#         v_mean: Mean input value (v̄)\n",
        "#         v_second_moment: Second moment of input (v̄̄)\n",
        "#         alpha: Constant (0.066)\n",
        "\n",
        "#     Returns:\n",
        "#         w_mean: Mean weight value (W̄_n)\n",
        "#         w_std: Standard deviation (σ_{W_n})\n",
        "#     \"\"\"\n",
        "#     # Equation A.7: W̄_n = (v̄ - 0.8) / (α · N · v̄)\n",
        "#     w_mean = (v_mean - 0.8) / (alpha * num_neurons * v_mean)\n",
        "\n",
        "#     # Equation A.8: W̄̄_n for computing standard deviation\n",
        "#     numerator = (v_second_moment +\n",
        "#                  alpha**2 * (num_neurons - num_neurons**2) * w_mean**2 * v_mean**2 -\n",
        "#                  1.6 * alpha * num_neurons * v_mean * w_mean -\n",
        "#                  0.64)\n",
        "#     denominator = alpha**2 * num_neurons * v_second_moment\n",
        "\n",
        "#     w_second_moment = numerator / denominator\n",
        "\n",
        "#     # Calculate standard deviation from second moment\n",
        "#     w_std = np.sqrt(w_second_moment - w_mean**2)\n",
        "\n",
        "#     return w_mean, w_std\n",
        "\n",
        "\n",
        "# def initialize_feedback_matrix(output_shape, input_shape,\n",
        "#                                 w_mean_next, w_std_next,\n",
        "#                                 gamma=0.1,\n",
        "#                                 num_downstream_layers=1):\n",
        "#     \"\"\"\n",
        "#     Initialize fixed random feedback matrix B.\n",
        "#     Paper reference: Appendix B, Equation A.5\n",
        "\n",
        "#     Args:\n",
        "#         output_shape: Number of neurons this layer projects to\n",
        "#         input_shape: Number of neurons in this layer\n",
        "#         w_mean_next: Mean of weights in next layer\n",
        "#         w_std_next: Std of weights in next layer\n",
        "#         gamma: Scale factor (0.0338 in paper) but using other values\n",
        "#         num_downstream_layers: Number of layers between this and output (D)\n",
        "\n",
        "#     Returns:\n",
        "#         B: Fixed random feedback matrix [input_shape, output_shape]\n",
        "#     \"\"\"\n",
        "#     # Generate random matrix with paper's specific distribution\n",
        "#     # B_n = γ · [W̄_{n+1} + 2√3 · σ_{W_{n+1}} · (rand - 0.5)]\n",
        "\n",
        "#     rand_values = np.random.uniform(0, 1, size=(input_shape, output_shape))\n",
        "\n",
        "#     # Apply paper's formula\n",
        "#     B = w_mean_next + 2 * np.sqrt(3) * w_std_next * (rand_values - 0.5)\n",
        "\n",
        "#     # Apply scaling factor\n",
        "#     B = gamma * B\n",
        "\n",
        "#     # Note: Paper mentions product over downstream layers (∏)\n",
        "#     # For simplicity with 2-layer network, we use single multiplication\n",
        "#     # For deeper networks, you'd multiply B matrices from all downstream layers\n",
        "\n",
        "#     return B.astype(np.float32)\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "feedback_init"
      },
      "execution_count": 41,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class DFA_LIFLayer(nn.Module):\n",
        "    \"\"\"\n",
        "    LIF layer that outputs spikes over time.\n",
        "    Has fixed random feedback matrix B for DFA training.\n",
        "    \"\"\"\n",
        "    def __init__(self, in_features, units, output_size,\n",
        "                 tau=20.0, dt=0.25, threshold=0.1, t_ref=1.0,\n",
        "                 use_dfa=True, gamma=0.0338):\n",
        "        super().__init__()\n",
        "        self.in_features = in_features\n",
        "        self.units = units\n",
        "        self.output_size = output_size\n",
        "\n",
        "        self.tau = tau\n",
        "        self.dt = dt\n",
        "        self.threshold = threshold\n",
        "        self.t_ref = t_ref\n",
        "        self.use_dfa = use_dfa\n",
        "        self.gamma = gamma\n",
        "\n",
        "        # Trainable weights (like add_weight trainable=True)\n",
        "        self.w = nn.Parameter(torch.empty(in_features, units))\n",
        "        self.b = nn.Parameter(torch.zeros(units))\n",
        "        nn.init.constant_(self.b, 0.2)\n",
        "\n",
        "        # Init roughly like Keras Glorot-ish (you can match your stats init if you want)\n",
        "        nn.init.xavier_uniform_(self.w)\n",
        "\n",
        "        # Refractory steps\n",
        "        self.ref_steps = int(round(self.t_ref / self.dt))\n",
        "\n",
        "        # Fixed random feedback matrix B (like add_weight trainable=False)\n",
        "        if use_dfa:\n",
        "            B_init = torch.randn(units, output_size) * self.gamma\n",
        "            self.register_buffer(\"B\", B_init)  # NOT a Parameter -> not trainable\n",
        "\n",
        "    def forward(self, inputs):\n",
        "        \"\"\"\n",
        "        inputs: [batch, time, in_features]\n",
        "        returns spikes: [batch, time, units]\n",
        "        \"\"\"\n",
        "        batch_size, time_steps, _ = inputs.shape\n",
        "\n",
        "        v = torch.zeros(batch_size, self.units, device=inputs.device)\n",
        "        ref_count = torch.zeros(batch_size, self.units, device=inputs.device)\n",
        "\n",
        "        spikes_out = []\n",
        "\n",
        "        alpha = 1.0 - (self.dt / self.tau)\n",
        "        beta  = self.dt / self.tau\n",
        "\n",
        "        v_max = torch.tensor(0.0, device=inputs.device)\n",
        "        v_mean_accum = 0.0\n",
        "\n",
        "        for t in range(time_steps):\n",
        "            x_t = inputs[:, t, :]\n",
        "\n",
        "            I_t = (x_t @ self.w) + self.b\n",
        "            v = alpha * v + beta * I_t\n",
        "            # print once per forward call (you can comment this out after)\n",
        "\n",
        "            # refractory\n",
        "            v = torch.where(ref_count > 0, torch.zeros_like(v), v)\n",
        "\n",
        "            # spike + reset\n",
        "            spikes = (v >= self.threshold).float()\n",
        "\n",
        "            v = v * (1.0 - spikes)\n",
        "\n",
        "            # update refractory counter\n",
        "            ref_count = ref_count - 1.0\n",
        "            ref_count = torch.where(spikes > 0, torch.ones_like(ref_count) * self.ref_steps, ref_count)\n",
        "            ref_count = torch.clamp(ref_count, min=0.0)\n",
        "\n",
        "            spikes_out.append(spikes)\n",
        "\n",
        "\n",
        "        return torch.stack(spikes_out, dim=1)"
      ],
      "metadata": {
        "id": "3wdjozUNxfMj"
      },
      "execution_count": 42,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# @title\n",
        "# math function (PyTorch version)\n",
        "\n",
        "import torch\n",
        "import numpy as np\n",
        "\n",
        "def surrogate_gradient_exact(a, h_th=0.1, t_ref=1.0, tau=20.0): # original threshold was 0,4\n",
        "    \"\"\"\n",
        "    Exact surrogate gradient from paper (Appendix D, Equation A.9).\n",
        "    PyTorch version.\n",
        "    \"\"\"\n",
        "    eps = 1e-8\n",
        "\n",
        "    # Compute ratio a / (a - h_th)\n",
        "    ratio = a / (a - h_th + eps)\n",
        "\n",
        "    # Numerator: h_th * t_ref * tau / [a * (a - h_th)]\n",
        "    numerator = h_th * t_ref * tau / (a * (a - h_th) + eps)\n",
        "\n",
        "    # Denominator: [t_ref + tau * log(ratio)]^2\n",
        "    log_term = torch.log(ratio + eps)\n",
        "    denominator = (t_ref + tau * log_term) ** 2 + eps\n",
        "\n",
        "    grad = numerator / denominator\n",
        "\n",
        "    # Only non-zero where a > threshold\n",
        "    grad = torch.where(a > h_th, grad, torch.zeros_like(grad))\n",
        "\n",
        "    return grad\n",
        "\n",
        "\n",
        "def surrogate_gradient_fast_sigmoid(a, threshold=0.1, alpha=5): # original threshold was 0,4\n",
        "    \"\"\"\n",
        "    Fast sigmoid surrogate gradient (PyTorch version).\n",
        "    \"\"\"\n",
        "    shifted = a - threshold\n",
        "    grad = 1.0 / (1.0 + torch.abs(alpha * shifted)) ** 2\n",
        "    return grad"
      ],
      "metadata": {
        "id": "6HQjJ0LhMW6P"
      },
      "execution_count": 43,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Cell A: PyTorch model (matches your Keras topology)\n",
        "class DFASNN(nn.Module):\n",
        "    def __init__(self, time_steps, input_size, hidden_size, hidden_size2, output_size):\n",
        "        super().__init__()\n",
        "        self.time_steps = time_steps\n",
        "\n",
        "        # Hidden layer with DFA feedback matrix B\n",
        "        self.hidden_layer = DFA_LIFLayer(\n",
        "            in_features=input_size,\n",
        "            units=hidden_size,\n",
        "            output_size=output_size,\n",
        "            tau=20.0,\n",
        "            dt=0.25,\n",
        "            threshold=0.08, # play with this (original was 0.4)\n",
        "            t_ref=1.0,\n",
        "            use_dfa=True,\n",
        "            gamma=0.0338\n",
        "        )\n",
        "\n",
        "        # second hidden layer\n",
        "        self.hidden_layer2 = DFA_LIFLayer(\n",
        "            in_features=hidden_size,\n",
        "            units=hidden_size2,\n",
        "            output_size=output_size,\n",
        "            tau=20.0,\n",
        "            dt=0.25,\n",
        "            threshold=0.05,\n",
        "            t_ref=1.0,\n",
        "            use_dfa=True,\n",
        "            gamma=0.0338\n",
        "        )\n",
        "\n",
        "\n",
        "        # Output layer (no B matrix cus not trained)\n",
        "        self.output_layer = DFA_LIFLayer(\n",
        "            in_features=hidden_size2,\n",
        "            units=output_size,\n",
        "            output_size=output_size,\n",
        "            tau=20.0,\n",
        "            dt=0.25,\n",
        "            threshold=0.2, # maybe play with this (original was 0.4)\n",
        "            t_ref=1.0,\n",
        "            use_dfa=False\n",
        "        )\n",
        "\n",
        "    def forward(self, x_time):\n",
        "        \"\"\"\n",
        "        x_time: [batch, time, 784]  (matches your Keras input exactly)\n",
        "        returns:\n",
        "          h_spikes: [batch, time, hidden]\n",
        "          h2_spikes: [batch, time, hidden]\n",
        "          y_spikes: [batch, time, 10]\n",
        "        \"\"\"\n",
        "        h_spikes = self.hidden_layer(x_time)\n",
        "        h2_spikes = self.hidden_layer2(h_spikes)\n",
        "        y_spikes = self.output_layer(h2_spikes)\n",
        "\n",
        "\n",
        "        return h_spikes, h2_spikes, y_spikes"
      ],
      "metadata": {
        "id": "qlY7nl0nAokQ"
      },
      "execution_count": 44,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Cell B: PyTorch DFA trainer (manual DFA updates, no autograd needed)\n",
        "class DFATrainerTorch:\n",
        "    def __init__(self, model, learning_rate=0.1, use_exact_gradient=True):\n",
        "        self.model = model\n",
        "        self.lr = learning_rate\n",
        "        self.use_exact_gradient = use_exact_gradient\n",
        "\n",
        "    @staticmethod\n",
        "    def compute_spike_rate(spikes):\n",
        "        # match your TF version: sum over time\n",
        "        return spikes.sum(dim=1)  # [batch, units]\n",
        "\n",
        "    def train_step(self, x_time, y_onehot):\n",
        "      \"\"\"\n",
        "      x_time:   [b,t,784]\n",
        "      y_onehot: [b,10]\n",
        "      returns: (loss_float, acc_float)\n",
        "      \"\"\"\n",
        "      self.model.train()\n",
        "      x_time = x_time.to(device)\n",
        "      y_onehot = y_onehot.to(device)\n",
        "\n",
        "      # Forward\n",
        "      h_spikes, h2_spikes, y_spikes = self.model(x_time)   # [b,t,H1], [b,t,H2], [b,t,10]\n",
        "      out_rates = self.compute_spike_rate(y_spikes)        # [b,10]\n",
        "      out_probs = F.softmax(out_rates, dim=1)              # [b,10]\n",
        "\n",
        "      eps = 1e-9\n",
        "      loss = -(y_onehot * torch.log(out_probs + eps)).sum(dim=1).mean()\n",
        "\n",
        "      # Debug: spike activity\n",
        "      # with torch.no_grad():\n",
        "      #     print(\"mean h1 spike\", h_spikes.mean().item(),\n",
        "      #           \"mean h2 spike\", h2_spikes.mean().item(),\n",
        "      #           \"mean y spike\",  y_spikes.mean().item())\n",
        "\n",
        "      # Global error (broadcast to time)\n",
        "      e_global = out_probs - y_onehot                      # [b,10]\n",
        "      e_time = e_global.unsqueeze(1).repeat(1, y_spikes.size(1), 1)  # [b,t,10]\n",
        "\n",
        "      bsz, T, _ = y_spikes.shape\n",
        "      denom = float(bsz * T)\n",
        "\n",
        "      with torch.no_grad():\n",
        "          # =========================================================\n",
        "          # 1) OUTPUT LAYER UPDATE (uses real output error)\n",
        "          # =========================================================\n",
        "          out_layer = self.model.output_layer\n",
        "          out_input = h2_spikes                              # [b,t,H2]\n",
        "\n",
        "          a_out = (out_input @ out_layer.w) + out_layer.b    # [b,t,10]\n",
        "\n",
        "          if self.use_exact_gradient:\n",
        "              fprime_out = surrogate_gradient_exact(\n",
        "                  a_out, h_th=out_layer.threshold, t_ref=out_layer.t_ref, tau=out_layer.tau\n",
        "              )\n",
        "          else:\n",
        "              fprime_out = surrogate_gradient_fast_sigmoid(\n",
        "                  a_out, threshold=out_layer.threshold, alpha=10.0\n",
        "              )\n",
        "\n",
        "          e_out = e_time * fprime_out                        # [b,t,10]\n",
        "\n",
        "          grad_w_out = torch.einsum(\"bti,btj->ij\", out_input, e_out) / denom\n",
        "          grad_b_out = e_out.sum(dim=(0, 1)) / denom\n",
        "\n",
        "          out_layer.w -= self.lr * grad_w_out\n",
        "          out_layer.b -= self.lr * grad_b_out\n",
        "\n",
        "          # =========================================================\n",
        "          # 2) HIDDEN LAYER 2 DFA UPDATE (projects output error via B2)\n",
        "          # =========================================================\n",
        "          hid2_layer = self.model.hidden_layer2\n",
        "\n",
        "          # e_proj2: [b,t,10] x [H2,10] -> [b,t,H2]\n",
        "          e_proj2 = torch.einsum(\"bto,ho->bth\", e_time, hid2_layer.B)\n",
        "\n",
        "          # pre-activation for hidden2 uses input = h_spikes (layer1 spikes)\n",
        "          a_hid2 = (h_spikes @ hid2_layer.w) + hid2_layer.b  # [b,t,H2]\n",
        "\n",
        "          if self.use_exact_gradient:\n",
        "              fprime_hid2 = surrogate_gradient_exact(\n",
        "                  a_hid2, h_th=hid2_layer.threshold, t_ref=hid2_layer.t_ref, tau=hid2_layer.tau\n",
        "              )\n",
        "          else:\n",
        "              fprime_hid2 = surrogate_gradient_fast_sigmoid(\n",
        "                  a_hid2, threshold=hid2_layer.threshold, alpha=10.0\n",
        "              )\n",
        "\n",
        "          e_hid2 = e_proj2 * fprime_hid2                     # [b,t,H2]\n",
        "\n",
        "          grad_w_hid2 = torch.einsum(\"bti,btj->ij\", h_spikes, e_hid2) / denom\n",
        "          grad_b_hid2 = e_hid2.sum(dim=(0, 1)) / denom\n",
        "\n",
        "          hid2_layer.w -= self.lr * grad_w_hid2\n",
        "          hid2_layer.b -= self.lr * grad_b_hid2\n",
        "\n",
        "          # =========================================================\n",
        "          # 3) HIDDEN LAYER 1 DFA UPDATE (projects output error via B1)\n",
        "          # =========================================================\n",
        "          hid1_layer = self.model.hidden_layer\n",
        "\n",
        "          # e_proj1: [b,t,10] x [H1,10] -> [b,t,H1]\n",
        "          e_proj1 = torch.einsum(\"bto,ho->bth\", e_time, hid1_layer.B)\n",
        "\n",
        "          a_hid1 = (x_time @ hid1_layer.w) + hid1_layer.b    # [b,t,H1]\n",
        "\n",
        "          if self.use_exact_gradient:\n",
        "              fprime_hid1 = surrogate_gradient_exact(\n",
        "                  a_hid1, h_th=hid1_layer.threshold, t_ref=hid1_layer.t_ref, tau=hid1_layer.tau\n",
        "              )\n",
        "          else:\n",
        "              fprime_hid1 = surrogate_gradient_fast_sigmoid(\n",
        "                  a_hid1, threshold=hid1_layer.threshold, alpha=10.0\n",
        "              )\n",
        "\n",
        "          e_hid1 = e_proj1 * fprime_hid1                     # [b,t,H1]\n",
        "\n",
        "          grad_w_hid1 = torch.einsum(\"bti,btj->ij\", x_time, e_hid1) / denom\n",
        "          grad_b_hid1 = e_hid1.sum(dim=(0, 1)) / denom\n",
        "\n",
        "          hid1_layer.w -= self.lr * grad_w_hid1\n",
        "          hid1_layer.b -= self.lr * grad_b_hid1\n",
        "\n",
        "        # Accuracy\n",
        "      preds = torch.argmax(out_probs, dim=1)\n",
        "      true = torch.argmax(y_onehot, dim=1)\n",
        "      acc = (preds == true).float().mean().item()\n",
        "\n",
        "      return loss.item(), acc\n",
        "\n",
        "    def fit(self, x_time_all, y_onehot_all, epochs=20, batch_size=128, verbose=1):\n",
        "      history = {\"loss\": [], \"accuracy\": []}\n",
        "      n = x_time_all.shape[0]\n",
        "\n",
        "      for ep in range(epochs):\n",
        "          perm = torch.randperm(n)\n",
        "          ep_loss = 0.0\n",
        "          ep_acc = 0.0\n",
        "          nb = 0\n",
        "\n",
        "          for i in range(0, n, batch_size):\n",
        "              idx = perm[i:i+batch_size]\n",
        "              xb_time = x_time_all[idx].to(device)\n",
        "              yb_onehot = y_onehot_all[idx].to(device)\n",
        "\n",
        "              loss, acc = self.train_step(xb_time, yb_onehot)\n",
        "\n",
        "\n",
        "              ep_loss += loss\n",
        "              ep_acc += acc\n",
        "              nb += 1\n",
        "\n",
        "          history[\"loss\"].append(ep_loss / nb)\n",
        "          history[\"accuracy\"].append(ep_acc / nb)\n",
        "\n",
        "          if verbose:\n",
        "              print(f\"Epoch {ep+1}/{epochs} - loss: {history['loss'][-1]:.4f} - acc: {history['accuracy'][-1]:.4f}\")\n",
        "\n",
        "      return history"
      ],
      "metadata": {
        "id": "VZku5B2bApSu"
      },
      "execution_count": 45,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# @title\n",
        "# PyTorch version of that ONE cell (data prep + build + train + plot)\n",
        "# Assumes you already defined: DFA_LIFLayer, DFASNN, DFATrainerTorch (from earlier port steps)\n",
        "\n",
        "import numpy as np\n",
        "import torch\n",
        "import torch.nn.functional as F\n",
        "from torchvision import datasets, transforms\n",
        "from torch.utils.data import DataLoader, Subset\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# ----------------------------\n",
        "# Load MNIST data (PyTorch)\n",
        "# ----------------------------\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "transform = transforms.Compose([\n",
        "    transforms.ToTensor(),                  # -> float32 in [0,1], shape [1,28,28]\n",
        "    transforms.Lambda(lambda x: x.view(-1))  # -> [784]\n",
        "])\n",
        "\n",
        "train_ds_full = datasets.MNIST(root=\"./data\", train=True, download=True, transform=transform)\n",
        "test_ds_full  = datasets.MNIST(root=\"./data\", train=False, download=True, transform=transform)\n",
        "\n",
        "# Convert to tensors (to mirror your \"x_train, y_train\" arrays)\n",
        "# Note: this iterates through the dataset once; fine for MNIST.\n",
        "x_train = torch.stack([train_ds_full[i][0] for i in range(len(train_ds_full))]).float()  # [60000,784]\n",
        "y_train_int = torch.tensor([train_ds_full[i][1] for i in range(len(train_ds_full))], dtype=torch.long)  # [60000]\n",
        "\n",
        "x_test  = torch.stack([test_ds_full[i][0] for i in range(len(test_ds_full))]).float()    # [10000,784]\n",
        "y_test_int  = torch.tensor([test_ds_full[i][1] for i in range(len(test_ds_full))], dtype=torch.long)   # [10000]\n",
        "\n",
        "# One-hot encode labels (to match your Keras cell exactly)\n",
        "y_train = F.one_hot(y_train_int, num_classes=10).float()  # [60000,10]\n",
        "y_test  = F.one_hot(y_test_int, num_classes=10).float()   # [10000,10]\n",
        "\n",
        "print(f\"Training samples: {len(x_train)}\")\n",
        "print(f\"Test samples: {len(x_test)}\")\n",
        "print(f\"Input shape: {x_train.shape[1]}\")\n",
        "print(f\"Output classes: {y_train.shape[1]}\")\n",
        "\n",
        "TIME_STEPS = 25   # Reduced from 100 for faster training\n",
        "HIDDEN_SIZE = 1000\n",
        "HIDDEN_SIZE2 = 250\n",
        "OUTPUT_SIZE = 10\n",
        "INPUT_SIZE = 784\n",
        "\n",
        "# INCREASED: Use larger dataset\n",
        "TRAIN_SAMPLES = 30000\n",
        "x_train_small = x_train[:TRAIN_SAMPLES]          # [30000,784]\n",
        "y_train_small = y_train[:TRAIN_SAMPLES]          # [30000,10]\n",
        "y_train_small_int = y_train_int[:TRAIN_SAMPLES]  # [30000]\n",
        "\n",
        "# Expand to time dimension (paper uses \"direct mapping\")\n",
        "# Just repeat the input for each time step\n",
        "# (Matches your np.tile logic, but in torch)\n",
        "x_train_spikes = x_train_small.unsqueeze(1).repeat(1, TIME_STEPS, 1)  # [30000,25,784]\n",
        "\n",
        "print(f\"\\nInput shape with time: {tuple(x_train_spikes.shape)}\")\n",
        "print(f\"  [batch={x_train_spikes.shape[0]}, \"\n",
        "      f\"time={x_train_spikes.shape[1]}, \"\n",
        "      f\"features={x_train_spikes.shape[2]}]\")\n",
        "\n",
        "# ----------------------------\n",
        "# Build DFA-SNN model (PyTorch)\n",
        "# ----------------------------\n",
        "print(\"\\n\" + \"=\"*60)\n",
        "print(\"BUILDING DFA-SNN MODEL (PyTorch)\")\n",
        "print(\"=\"*60)\n",
        "\n",
        "# Safety checks (so you get a clear error if earlier port cells aren't run yet)\n",
        "missing = []\n",
        "for name in [\"DFA_LIFLayer\", \"DFASNN\", \"DFATrainerTorch\"]:\n",
        "    if name not in globals():\n",
        "        missing.append(name)\n",
        "if missing:\n",
        "    raise NameError(\n",
        "        \"You haven't defined these yet in earlier cells: \"\n",
        "        + \", \".join(missing)\n",
        "        + \"\\nRun the cells where you ported the layer/model/trainer first.\"\n",
        "    )\n",
        "\n",
        "model = DFASNN(\n",
        "    time_steps=TIME_STEPS,\n",
        "    input_size=INPUT_SIZE,\n",
        "    hidden_size=HIDDEN_SIZE,\n",
        "    hidden_size2=HIDDEN_SIZE2,\n",
        "    output_size=OUTPUT_SIZE\n",
        ").to(device)\n",
        "\n",
        "with torch.no_grad():\n",
        "        model.hidden_layer2.b.fill_(1.0)\n",
        "        model.output_layer.b.fill_(1.0)\n",
        "print(model)\n",
        "\n",
        "# ----------------------------\n",
        "# Create DFA trainer (PyTorch)\n",
        "# ----------------------------\n",
        "print(\"\\n\" + \"=\"*60)\n",
        "print(\"INITIALIZING DFA TRAINER (PyTorch)\")\n",
        "print(\"=\"*60)\n",
        "\n",
        "trainer = DFATrainerTorch(\n",
        "    model=model,\n",
        "    learning_rate=0.1  # matches your cell\n",
        ")\n",
        "\n",
        "# Mirror your \"dfa layers found\" printout as closely as possible\n",
        "dfa_layers = []\n",
        "if hasattr(model, \"hidden_layer\"):\n",
        "    dfa_layers.append(model.hidden_layer)\n",
        "if hasattr(model, \"hidden_layer2\"):\n",
        "    dfa_layers.append(model.hidden_layer2)\n",
        "if hasattr(model, \"output_layer\"):\n",
        "    dfa_layers.append(model.output_layer)\n",
        "\n",
        "print(f\"\\nDFA layers found: {len(dfa_layers)}\")\n",
        "for layer in dfa_layers:\n",
        "    B_shape = tuple(layer.B.shape) if hasattr(layer, \"B\") else None\n",
        "    print(f\"  {layer.__class__.__name__}:\")\n",
        "    print(f\"    Units: {layer.units}\")\n",
        "    print(f\"    B shape: {B_shape}\")\n",
        "    print(f\"    Learning rate: {trainer.lr:.6f}\")\n",
        "\n",
        "model.hidden_layer.debug_name = \"h1\"\n",
        "model.hidden_layer2.debug_name = \"h2\"\n",
        "model.output_layer.debug_name = \"out\"\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "# We already built x_train_spikes as a full tensor [30000,25,784]\n",
        "# To keep this as a single-cell replica, we’ll train on it directly in batches.\n",
        "\n",
        "history = trainer.fit(x_train_spikes, y_train_small, epochs=50, batch_size=128, verbose=1)\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "\n",
        "epochs = np.arange(1, len(history[\"accuracy\"]) + 1)\n",
        "\n",
        "plt.figure(figsize=(8, 5))\n",
        "plt.plot(epochs, history[\"accuracy\"], marker='o', linewidth=2)\n",
        "\n",
        "plt.xlabel(\"Epoch\")\n",
        "plt.ylabel(\"Accuracy\")\n",
        "plt.title(\"Training Accuracy vs Epochs (DFA-SNN)\")\n",
        "plt.grid(True, alpha=0.3)\n",
        "\n",
        "plt.xticks(epochs if len(epochs) <= 20 else None)\n",
        "plt.ylim(0, 1.0)\n",
        "\n",
        "plt.show()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "eBfUTSjSHxk6",
        "outputId": "5d87ebc2-b5b6-45fe-9f98-c1e050864ce2"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Training samples: 60000\n",
            "Test samples: 10000\n",
            "Input shape: 784\n",
            "Output classes: 10\n",
            "\n",
            "Input shape with time: (30000, 25, 784)\n",
            "  [batch=30000, time=25, features=784]\n",
            "\n",
            "============================================================\n",
            "BUILDING DFA-SNN MODEL (PyTorch)\n",
            "============================================================\n",
            "DFASNN(\n",
            "  (hidden_layer): DFA_LIFLayer()\n",
            "  (hidden_layer2): DFA_LIFLayer()\n",
            "  (output_layer): DFA_LIFLayer()\n",
            ")\n",
            "\n",
            "============================================================\n",
            "INITIALIZING DFA TRAINER (PyTorch)\n",
            "============================================================\n",
            "\n",
            "DFA layers found: 3\n",
            "  DFA_LIFLayer:\n",
            "    Units: 1000\n",
            "    B shape: (1000, 10)\n",
            "    Learning rate: 0.100000\n",
            "  DFA_LIFLayer:\n",
            "    Units: 250\n",
            "    B shape: (250, 10)\n",
            "    Learning rate: 0.100000\n",
            "  DFA_LIFLayer:\n",
            "    Units: 10\n",
            "    B shape: None\n",
            "    Learning rate: 0.100000\n",
            "Epoch 1/200 - loss: 2.3026 - acc: 0.0985\n",
            "Epoch 2/200 - loss: 2.3026 - acc: 0.0989\n",
            "Epoch 3/200 - loss: 2.3088 - acc: 0.0986\n",
            "Epoch 4/200 - loss: 2.3141 - acc: 0.0821\n",
            "Epoch 5/200 - loss: 2.3179 - acc: 0.0645\n",
            "Epoch 6/200 - loss: 2.2932 - acc: 0.1007\n",
            "Epoch 7/200 - loss: 2.2367 - acc: 0.1723\n",
            "Epoch 8/200 - loss: 2.1901 - acc: 0.1942\n",
            "Epoch 9/200 - loss: 2.1638 - acc: 0.2028\n",
            "Epoch 10/200 - loss: 2.1018 - acc: 0.2150\n",
            "Epoch 11/200 - loss: 1.9957 - acc: 0.2539\n",
            "Epoch 12/200 - loss: 1.8935 - acc: 0.2797\n",
            "Epoch 13/200 - loss: 1.8040 - acc: 0.3045\n",
            "Epoch 14/200 - loss: 1.7387 - acc: 0.3456\n",
            "Epoch 15/200 - loss: 1.6791 - acc: 0.3948\n",
            "Epoch 16/200 - loss: 1.6257 - acc: 0.4438\n",
            "Epoch 17/200 - loss: 1.5750 - acc: 0.4838\n",
            "Epoch 18/200 - loss: 1.5210 - acc: 0.5251\n",
            "Epoch 19/200 - loss: 1.4728 - acc: 0.5536\n",
            "Epoch 20/200 - loss: 1.4268 - acc: 0.5843\n",
            "Epoch 21/200 - loss: 1.3908 - acc: 0.6060\n",
            "Epoch 22/200 - loss: 1.3622 - acc: 0.6256\n",
            "Epoch 23/200 - loss: 1.3376 - acc: 0.6448\n",
            "Epoch 24/200 - loss: 1.3157 - acc: 0.6613\n",
            "Epoch 25/200 - loss: 1.2942 - acc: 0.6782\n",
            "Epoch 26/200 - loss: 1.2738 - acc: 0.6967\n",
            "Epoch 27/200 - loss: 1.2536 - acc: 0.7119\n",
            "Epoch 28/200 - loss: 1.2394 - acc: 0.7240\n",
            "Epoch 29/200 - loss: 1.2231 - acc: 0.7373\n",
            "Epoch 30/200 - loss: 1.2071 - acc: 0.7486\n",
            "Epoch 31/200 - loss: 1.1868 - acc: 0.7579\n",
            "Epoch 32/200 - loss: 1.1712 - acc: 0.7653\n",
            "Epoch 33/200 - loss: 1.1581 - acc: 0.7710\n",
            "Epoch 34/200 - loss: 1.1426 - acc: 0.7776\n",
            "Epoch 35/200 - loss: 1.1284 - acc: 0.7839\n",
            "Epoch 36/200 - loss: 1.1161 - acc: 0.7885\n",
            "Epoch 37/200 - loss: 1.0991 - acc: 0.7928\n",
            "Epoch 38/200 - loss: 1.0819 - acc: 0.7973\n",
            "Epoch 39/200 - loss: 1.0662 - acc: 0.7985\n",
            "Epoch 40/200 - loss: 1.0486 - acc: 0.8011\n",
            "Epoch 41/200 - loss: 1.0357 - acc: 0.8046\n",
            "Epoch 42/200 - loss: 1.0208 - acc: 0.8071\n",
            "Epoch 43/200 - loss: 1.0067 - acc: 0.8100\n",
            "Epoch 44/200 - loss: 0.9908 - acc: 0.8150\n",
            "Epoch 45/200 - loss: 0.9763 - acc: 0.8183\n",
            "Epoch 46/200 - loss: 0.9621 - acc: 0.8219\n",
            "Epoch 47/200 - loss: 0.9468 - acc: 0.8258\n",
            "Epoch 48/200 - loss: 0.9326 - acc: 0.8291\n",
            "Epoch 49/200 - loss: 0.9191 - acc: 0.8328\n",
            "Epoch 50/200 - loss: 0.9071 - acc: 0.8338\n",
            "Epoch 51/200 - loss: 0.8951 - acc: 0.8367\n",
            "Epoch 52/200 - loss: 0.8841 - acc: 0.8386\n",
            "Epoch 53/200 - loss: 0.8731 - acc: 0.8404\n",
            "Epoch 54/200 - loss: 0.8641 - acc: 0.8391\n",
            "Epoch 55/200 - loss: 0.8547 - acc: 0.8415\n",
            "Epoch 56/200 - loss: 0.8473 - acc: 0.8428\n",
            "Epoch 57/200 - loss: 0.8390 - acc: 0.8453\n",
            "Epoch 58/200 - loss: 0.8327 - acc: 0.8472\n",
            "Epoch 59/200 - loss: 0.8265 - acc: 0.8465\n",
            "Epoch 60/200 - loss: 0.8187 - acc: 0.8492\n",
            "Epoch 61/200 - loss: 0.8139 - acc: 0.8493\n",
            "Epoch 62/200 - loss: 0.8075 - acc: 0.8507\n",
            "Epoch 63/200 - loss: 0.8041 - acc: 0.8515\n",
            "Epoch 64/200 - loss: 0.7974 - acc: 0.8534\n",
            "Epoch 65/200 - loss: 0.7936 - acc: 0.8537\n",
            "Epoch 66/200 - loss: 0.7885 - acc: 0.8547\n",
            "Epoch 67/200 - loss: 0.7847 - acc: 0.8548\n",
            "Epoch 68/200 - loss: 0.7800 - acc: 0.8559\n",
            "Epoch 69/200 - loss: 0.7760 - acc: 0.8568\n",
            "Epoch 70/200 - loss: 0.7726 - acc: 0.8569\n",
            "Epoch 71/200 - loss: 0.7696 - acc: 0.8569\n",
            "Epoch 72/200 - loss: 0.7655 - acc: 0.8582\n",
            "Epoch 73/200 - loss: 0.7631 - acc: 0.8589\n",
            "Epoch 74/200 - loss: 0.7601 - acc: 0.8590\n",
            "Epoch 75/200 - loss: 0.7575 - acc: 0.8597\n",
            "Epoch 76/200 - loss: 0.7550 - acc: 0.8609\n",
            "Epoch 77/200 - loss: 0.7506 - acc: 0.8628\n",
            "Epoch 78/200 - loss: 0.7486 - acc: 0.8626\n",
            "Epoch 79/200 - loss: 0.7457 - acc: 0.8627\n"
          ]
        }
      ]
    }
  ]
}