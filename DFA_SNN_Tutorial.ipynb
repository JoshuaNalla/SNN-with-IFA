{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Direct Feedback Alignment (DFA) for Spiking Neural Networks\n",
        "\n",
        "## Tutorial: Implementing DFA for SNNs\n",
        "\n",
        "This notebook implements Direct Feedback Alignment (DFA) for training Spiking Neural Networks (SNNs) based on:\n",
        "\n",
        "**Paper:** \"Training Spiking Neural Networks via Augmented Direct Feedback Alignment\" by Zhang et al.\n",
        "\n",
        "---\n",
        "\n",
        "## Key Concepts:\n",
        "\n",
        "### 1. Standard Backpropagation (BP)\n",
        "- **Error propagation:** `e_n = [W^T_{n+1} · e_{n+1}] ⊙ f'(a_n)`\n",
        "- Requires layer-by-layer propagation\n",
        "- Needs precise transpose W^T and derivative f'\n",
        "\n",
        "### 2. Direct Feedback Alignment (DFA)\n",
        "- **Error propagation:** `e_n = [B_n · e] ⊙ f'(a_n)`\n",
        "- **B_n:** Fixed random feedback matrices (not trained)\n",
        "- **e:** Global error from output layer\n",
        "- Injects error **directly** to each layer, bypassing sequential propagation\n",
        "\n",
        "### 3. Augmented DFA (aDFA)\n",
        "- **Error propagation:** `e_n = [B_n · e] ⊙ g(a_n)`\n",
        "- Replaces f' with arbitrary function g\n",
        "- Most flexible, doesn't require accurate derivatives\n",
        "\n",
        "---\n",
        "\n",
        "## Why DFA for SNNs?\n",
        "\n",
        "1. **Biologically plausible:** No need for symmetric feedback weights\n",
        "2. **Hardware friendly:** Simpler to implement on neuromorphic chips\n",
        "3. **Gradient-free:** Works with non-differentiable spike functions\n",
        "4. **Scalable:** Direct error injection is more efficient"
      ],
      "metadata": {
        "id": "intro"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "---\n",
        "# CELL 1: Import Libraries\n",
        "---"
      ],
      "metadata": {
        "id": "cell1"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "imports"
      },
      "outputs": [],
      "source": [
        "import tensorflow as tf\n",
        "from tensorflow import keras\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "print(f\"TensorFlow version: {tf.__version__}\")\n",
        "print(\"✓ Libraries imported successfully\")"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "---\n",
        "# CELL 2: LIF Neuron Dynamics\n",
        "---\n",
        "\n",
        "## What we're implementing:\n",
        "\n",
        "The **Leaky Integrate-and-Fire (LIF)** neuron from the paper (Appendix A).\n",
        "\n",
        "### LIF Dynamics (Equation A.2 from paper):\n",
        "\n",
        "```\n",
        "h^(t)_i = (1 - Δt/τ) · h^(t-1)_i + (Δt/τ) · v^(t)_i + η^(t)\n",
        "```\n",
        "\n",
        "Where:\n",
        "- **h^(t)_i:** Membrane potential at time t\n",
        "- **τ:** Membrane time constant (controls decay rate)\n",
        "- **Δt:** Time step size\n",
        "- **v^(t)_i:** Input current (weighted sum of inputs)\n",
        "- **η^(t):** Reset mechanism (after spike)\n",
        "\n",
        "### Spike Generation (Equation A.3):\n",
        "\n",
        "```\n",
        "a^(t)_i = 1  if h^(t)_i ≥ h_th\n",
        "a^(t)_i = 0  if h^(t)_i < h_th\n",
        "```\n",
        "\n",
        "### Reset Mechanism (Equation A.4):\n",
        "\n",
        "After spiking, membrane potential is reset to 0 and neuron enters refractory period.\n",
        "\n",
        "## Why this matters for DFA:\n",
        "\n",
        "The spike function is **non-differentiable** (step function), which is why:\n",
        "1. Standard BP needs surrogate gradients\n",
        "2. DFA can bypass this with fixed random feedback"
      ],
      "metadata": {
        "id": "cell2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class LIFNeuron:\n",
        "    \"\"\"\n",
        "    Leaky Integrate-and-Fire neuron implementation.\n",
        "    \n",
        "    This class demonstrates the LIF dynamics before incorporating into layers.\n",
        "    Paper reference: Appendix A, Equations A.1-A.4\n",
        "    \"\"\"\n",
        "    \n",
        "    def __init__(self, tau=20.0, dt=0.25, threshold=0.4, t_ref=1.0):\n",
        "        \"\"\"\n",
        "        Args:\n",
        "            tau: Membrane time constant (ms) - controls leak rate\n",
        "            dt: Time step (ms) - discretization step\n",
        "            threshold: Spike threshold (h_th)\n",
        "            t_ref: Refractory period (ms) - time after spike when neuron can't fire\n",
        "        \n",
        "        Paper settings (Section B):\n",
        "            - tau = 20ms\n",
        "            - dt = 0.25ms\n",
        "            - threshold = 0.4\n",
        "            - t_ref = 1ms\n",
        "        \"\"\"\n",
        "        self.tau = tau\n",
        "        self.dt = dt\n",
        "        self.threshold = threshold\n",
        "        self.t_ref = t_ref\n",
        "        \n",
        "        # Calculate leak factor (1 - dt/tau)\n",
        "        self.alpha = 1.0 - (dt / tau)\n",
        "        # Calculate input scaling (dt/tau)\n",
        "        self.beta = dt / tau\n",
        "        \n",
        "        # State variables\n",
        "        self.v = 0.0  # Membrane potential\n",
        "        self.ref_count = 0  # Refractory counter (in time steps)\n",
        "        \n",
        "    def step(self, input_current):\n",
        "        \"\"\"\n",
        "        Single time step of LIF dynamics.\n",
        "        \n",
        "        Args:\n",
        "            input_current: Weighted sum of inputs at this time step\n",
        "            \n",
        "        Returns:\n",
        "            spike: 1 if neuron fired, 0 otherwise\n",
        "        \"\"\"\n",
        "        spike = 0\n",
        "        \n",
        "        # Check if in refractory period\n",
        "        if self.ref_count > 0:\n",
        "            self.ref_count -= 1\n",
        "            self.v = 0.0  # Keep voltage at 0 during refractory\n",
        "            return spike\n",
        "        \n",
        "        # Update membrane potential (Equation A.2)\n",
        "        self.v = self.alpha * self.v + self.beta * input_current\n",
        "        \n",
        "        # Check for spike (Equation A.3)\n",
        "        if self.v >= self.threshold:\n",
        "            spike = 1\n",
        "            self.v = 0.0  # Reset (Equation A.4)\n",
        "            # Enter refractory period\n",
        "            self.ref_count = int(self.t_ref / self.dt)\n",
        "            \n",
        "        return spike\n",
        "\n",
        "# Test the neuron\n",
        "neuron = LIFNeuron()\n",
        "print(\"✓ LIF Neuron class defined\")\n",
        "print(f\"  Leak factor (α): {neuron.alpha:.4f}\")\n",
        "print(f\"  Input scaling (β): {neuron.beta:.4f}\")\n",
        "print(f\"  Threshold: {neuron.threshold}\")"
      ],
      "metadata": {
        "id": "lif_neuron"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "---\n",
        "# CELL 3: Initialize Fixed Random Feedback Matrices (B)\n",
        "---\n",
        "\n",
        "## What we're implementing:\n",
        "\n",
        "The **fixed random feedback matrices** B_n that replace W^T in error backpropagation.\n",
        "\n",
        "### Initialization Formula (Appendix A.5):\n",
        "\n",
        "```\n",
        "B_n = γ · ∏[W̄_{n+1} + 2√3 · σ_{W_{n+1}} · (rand - 0.5)]\n",
        "```\n",
        "\n",
        "Where:\n",
        "- **γ:** Scale factor (0.0338 in paper)\n",
        "- **W̄_{n+1}:** Desired mean of weights (Equation A.7)\n",
        "- **σ_{W_{n+1}}:** Standard deviation of weights\n",
        "- **rand:** Uniform random [0, 1]\n",
        "\n",
        "### Weight Initialization (Appendix C, Equation A.6):\n",
        "\n",
        "```\n",
        "W_n = W̄_n + 2√3 · σ_{W_n} · (rand - 0.5)\n",
        "```\n",
        "\n",
        "Where W̄_n and σ_{W_n} are calculated from:\n",
        "\n",
        "```\n",
        "W̄_n = (v̄ - 0.8) / (α · N · v̄)\n",
        "```\n",
        "\n",
        "- **v̄:** Mean input value (8.0 in paper)\n",
        "- **α:** Constant (0.066)\n",
        "- **N:** Number of neurons in layer\n",
        "\n",
        "## Why this matters:\n",
        "\n",
        "1. **Fixed matrices:** B is initialized once and **never trained**\n",
        "2. **Random projection:** Provides approximate error direction\n",
        "3. **Biologically plausible:** No weight transport problem\n",
        "4. **Hardware friendly:** Simple random initialization"
      ],
      "metadata": {
        "id": "cell3"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def compute_weight_stats(num_neurons, v_mean=8.0, v_second_moment=164.0, alpha=0.066):\n",
        "    \"\"\"\n",
        "    Compute weight initialization statistics.\n",
        "    Paper reference: Appendix C, Equations A.7 and A.8\n",
        "    \n",
        "    Args:\n",
        "        num_neurons: Number of neurons in layer (N)\n",
        "        v_mean: Mean input value (v̄)\n",
        "        v_second_moment: Second moment of input (v̄̄)\n",
        "        alpha: Constant (0.066)\n",
        "    \n",
        "    Returns:\n",
        "        w_mean: Mean weight value (W̄_n)\n",
        "        w_std: Standard deviation (σ_{W_n})\n",
        "    \"\"\"\n",
        "    # Equation A.7: W̄_n = (v̄ - 0.8) / (α · N · v̄)\n",
        "    w_mean = (v_mean - 0.8) / (alpha * num_neurons * v_mean)\n",
        "    \n",
        "    # Equation A.8: W̄̄_n for computing standard deviation\n",
        "    numerator = (v_second_moment + \n",
        "                 alpha**2 * (num_neurons - num_neurons**2) * w_mean**2 * v_mean**2 - \n",
        "                 1.6 * alpha * num_neurons * v_mean * w_mean - \n",
        "                 0.64)\n",
        "    denominator = alpha**2 * num_neurons * v_second_moment\n",
        "    \n",
        "    w_second_moment = numerator / denominator\n",
        "    \n",
        "    # Calculate standard deviation from second moment\n",
        "    w_std = np.sqrt(w_second_moment - w_mean**2)\n",
        "    \n",
        "    return w_mean, w_std\n",
        "\n",
        "\n",
        "def initialize_feedback_matrix(output_shape, input_shape, \n",
        "                                w_mean_next, w_std_next,\n",
        "                                gamma=0.0338,\n",
        "                                num_downstream_layers=1):\n",
        "    \"\"\"\n",
        "    Initialize fixed random feedback matrix B.\n",
        "    Paper reference: Appendix B, Equation A.5\n",
        "    \n",
        "    Args:\n",
        "        output_shape: Number of neurons this layer projects to\n",
        "        input_shape: Number of neurons in this layer\n",
        "        w_mean_next: Mean of weights in next layer\n",
        "        w_std_next: Std of weights in next layer\n",
        "        gamma: Scale factor (0.0338 in paper)\n",
        "        num_downstream_layers: Number of layers between this and output (D)\n",
        "    \n",
        "    Returns:\n",
        "        B: Fixed random feedback matrix [input_shape, output_shape]\n",
        "    \"\"\"\n",
        "    # Generate random matrix with paper's specific distribution\n",
        "    # B_n = γ · [W̄_{n+1} + 2√3 · σ_{W_{n+1}} · (rand - 0.5)]\n",
        "    \n",
        "    rand_values = np.random.uniform(0, 1, size=(input_shape, output_shape))\n",
        "    \n",
        "    # Apply paper's formula\n",
        "    B = w_mean_next + 2 * np.sqrt(3) * w_std_next * (rand_values - 0.5)\n",
        "    \n",
        "    # Apply scaling factor\n",
        "    B = gamma * B\n",
        "    \n",
        "    # Note: Paper mentions product over downstream layers (∏)\n",
        "    # For simplicity with 2-layer network, we use single multiplication\n",
        "    # For deeper networks, you'd multiply B matrices from all downstream layers\n",
        "    \n",
        "    return B.astype(np.float32)\n",
        "\n",
        "\n",
        "# Example: Initialize feedback matrices for 784 → 1000 → 10 network\n",
        "print(\"=\" * 60)\n",
        "print(\"FEEDBACK MATRIX INITIALIZATION EXAMPLE\")\n",
        "print(\"=\" * 60)\n",
        "print(\"\\nNetwork architecture: 784 → 1000 → 10 (from paper)\\n\")\n",
        "\n",
        "# Layer sizes\n",
        "input_size = 784\n",
        "hidden_size = 1000\n",
        "output_size = 10\n",
        "\n",
        "# Compute weight statistics for each layer\n",
        "w_mean_hidden, w_std_hidden = compute_weight_stats(hidden_size)\n",
        "w_mean_output, w_std_output = compute_weight_stats(output_size)\n",
        "\n",
        "print(f\"Hidden layer weight stats:\")\n",
        "print(f\"  W̄ (mean):  {w_mean_hidden:.6f}\")\n",
        "print(f\"  σ_W (std): {w_std_hidden:.6f}\")\n",
        "print()\n",
        "print(f\"Output layer weight stats:\")\n",
        "print(f\"  W̄ (mean):  {w_mean_output:.6f}\")\n",
        "print(f\"  σ_W (std): {w_std_output:.6f}\")\n",
        "print()\n",
        "\n",
        "# Initialize feedback matrix for hidden layer\n",
        "# This projects global error (10-dim) to hidden layer (1000-dim)\n",
        "B_hidden = initialize_feedback_matrix(\n",
        "    output_shape=output_size,\n",
        "    input_shape=hidden_size,\n",
        "    w_mean_next=w_mean_output,\n",
        "    w_std_next=w_std_output,\n",
        "    gamma=0.0338\n",
        ")\n",
        "\n",
        "print(f\"Feedback matrix B (hidden layer):\")\n",
        "print(f\"  Shape: {B_hidden.shape} (projects {output_size}-dim error to {hidden_size}-dim)\")\n",
        "print(f\"  Mean: {B_hidden.mean():.6f}\")\n",
        "print(f\"  Std:  {B_hidden.std():.6f}\")\n",
        "print(f\"  Min:  {B_hidden.min():.6f}\")\n",
        "print(f\"  Max:  {B_hidden.max():.6f}\")\n",
        "print(\"\\n✓ Feedback matrices initialized (FIXED, not trainable)\")"
      ],
      "metadata": {
        "id": "feedback_init"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "---\n",
        "# CELL 4: DFA LIF Layer Implementation\n",
        "---\n",
        "\n",
        "## What we're implementing:\n",
        "\n",
        "A **custom LIF layer** that supports DFA training.\n",
        "\n",
        "### Key Differences from Standard BP:\n",
        "\n",
        "#### Standard BP:\n",
        "```\n",
        "e_n = [W^T_{n+1} · e_{n+1}] ⊙ f'(a_n)\n",
        "     ↑              ↑           ↑\n",
        "   exact        layer-by-    exact\n",
        " transpose      layer        derivative\n",
        "```\n",
        "\n",
        "#### DFA:\n",
        "```\n",
        "e_n = [B_n · e] ⊙ f'(a_n)\n",
        "     ↑      ↑       ↑\n",
        "   fixed  global  exact\n",
        "  random  error   derivative\n",
        "```\n",
        "\n",
        "### Implementation Strategy:\n",
        "\n",
        "1. **Forward pass:** Standard LIF dynamics (same as before)\n",
        "2. **Backward pass:** Custom gradient using DFA formula\n",
        "3. **Feedback matrix B:** Stored as non-trainable weight\n",
        "\n",
        "### Gradient Computation:\n",
        "\n",
        "We need to implement:\n",
        "```python\n",
        "∂L/∂W_n = e_n · x_n^T\n",
        "```\n",
        "\n",
        "Where:\n",
        "- `e_n = [B_n · e_global] ⊙ f'(a_n)`\n",
        "- `x_n` is the layer input\n",
        "- `e_global` is the error from output layer"
      ],
      "metadata": {
        "id": "cell4"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class DFA_LIFLayer(keras.layers.Layer):\n",
        "    \"\"\"\n",
        "    LIF layer with Direct Feedback Alignment (DFA) training.\n",
        "    \n",
        "    This layer implements DFA as described in the paper:\n",
        "    - Forward: Standard LIF dynamics\n",
        "    - Backward: DFA error propagation (e_n = [B_n · e] ⊙ f'(a_n))\n",
        "    \"\"\"\n",
        "    \n",
        "    def __init__(self, units, output_size,\n",
        "                 tau=20.0, dt=0.25, threshold=0.4, t_ref=1.0,\n",
        "                 use_dfa=True, gamma=0.0338,\n",
        "                 **kwargs):\n",
        "        \"\"\"\n",
        "        Args:\n",
        "            units: Number of neurons in this layer\n",
        "            output_size: Number of output neurons (for B matrix shape)\n",
        "            tau: Membrane time constant (ms)\n",
        "            dt: Time step (ms)\n",
        "            threshold: Spike threshold\n",
        "            t_ref: Refractory period (ms)\n",
        "            use_dfa: If True, use DFA; if False, use standard BP\n",
        "            gamma: Scale factor for B initialization\n",
        "        \"\"\"\n",
        "        super().__init__(**kwargs)\n",
        "        self.units = units\n",
        "        self.output_size = output_size\n",
        "        self.tau = tau\n",
        "        self.dt = dt\n",
        "        self.threshold = threshold\n",
        "        self.t_ref = t_ref\n",
        "        self.use_dfa = use_dfa\n",
        "        self.gamma = gamma\n",
        "        \n",
        "        # Calculate LIF dynamics parameters\n",
        "        self.alpha = 1.0 - (dt / tau)  # Leak factor\n",
        "        self.beta = dt / tau  # Input scaling\n",
        "        self.ref_steps = int(t_ref / dt)  # Refractory steps\n",
        "        \n",
        "    def build(self, input_shape):\n",
        "        \"\"\"\n",
        "        Build layer weights and feedback matrix.\n",
        "        \"\"\"\n",
        "        input_dim = input_shape[-1]\n",
        "        \n",
        "        # Compute weight initialization statistics (paper's method)\n",
        "        w_mean, w_std = compute_weight_stats(self.units)\n",
        "        \n",
        "        # Initialize forward weights W (trainable)\n",
        "        # Using paper's initialization: W = W̄ + 2√3·σ_W·(rand - 0.5)\n",
        "        w_init = keras.initializers.RandomUniform(\n",
        "            minval=w_mean - np.sqrt(3) * w_std,\n",
        "            maxval=w_mean + np.sqrt(3) * w_std\n",
        "        )\n",
        "        \n",
        "        self.w = self.add_weight(\n",
        "            name='weights',\n",
        "            shape=(input_dim, self.units),\n",
        "            initializer=w_init,\n",
        "            trainable=True  # W is trained\n",
        "        )\n",
        "        \n",
        "        # Initialize bias (paper uses 0.8)\n",
        "        self.b = self.add_weight(\n",
        "            name='bias',\n",
        "            shape=(self.units,),\n",
        "            initializer=keras.initializers.Constant(0.8),\n",
        "            trainable=True\n",
        "        )\n",
        "        \n",
        "        if self.use_dfa:\n",
        "            # Initialize fixed random feedback matrix B (NOT trainable)\n",
        "            # B projects global error (output_size) to this layer (units)\n",
        "            w_mean_next, w_std_next = compute_weight_stats(self.output_size)\n",
        "            \n",
        "            B_init = initialize_feedback_matrix(\n",
        "                output_shape=self.output_size,\n",
        "                input_shape=self.units,\n",
        "                w_mean_next=w_mean_next,\n",
        "                w_std_next=w_std_next,\n",
        "                gamma=self.gamma\n",
        "            )\n",
        "            \n",
        "            self.B = self.add_weight(\n",
        "                name='feedback_matrix',\n",
        "                shape=(self.units, self.output_size),\n",
        "                initializer=keras.initializers.Constant(B_init),\n",
        "                trainable=False  # B is FIXED (never trained)\n",
        "            )\n",
        "            \n",
        "    def call(self, inputs, training=None):\n",
        "        \"\"\"\n",
        "        Forward pass: LIF dynamics over time.\n",
        "        \n",
        "        Args:\n",
        "            inputs: [batch, time_steps, input_dim]\n",
        "        \n",
        "        Returns:\n",
        "            spikes: [batch, time_steps, units]\n",
        "        \"\"\"\n",
        "        batch_size = tf.shape(inputs)[0]\n",
        "        time_steps = tf.shape(inputs)[1]\n",
        "        \n",
        "        # Initialize state\n",
        "        v = tf.zeros((batch_size, self.units))  # Membrane potential\n",
        "        ref_count = tf.zeros((batch_size, self.units))  # Refractory counter\n",
        "        \n",
        "        # Storage for spikes and pre-synaptic currents (needed for DFA)\n",
        "        spikes_array = tf.TensorArray(\n",
        "            dtype=tf.float32,\n",
        "            size=time_steps,\n",
        "            element_shape=(None, self.units)\n",
        "        )\n",
        "        \n",
        "        # We also need to store pre-activation values (a_n) for gradient\n",
        "        # In DFA, we compute e_n = [B_n · e] ⊙ f'(a_n)\n",
        "        # So we need a_n during backward pass\n",
        "        \n",
        "        # Time loop\n",
        "        for t in range(time_steps):\n",
        "            # Get input at time t\n",
        "            x_t = inputs[:, t, :]  # [batch, input_dim]\n",
        "            \n",
        "            # Compute input current (v^(t) from Equation A.1)\n",
        "            i_in = tf.matmul(x_t, self.w) + self.b  # [batch, units]\n",
        "            \n",
        "            # Check refractory period\n",
        "            not_refractory = tf.cast(ref_count <= 0, tf.float32)\n",
        "            \n",
        "            # Update membrane potential (Equation A.2)\n",
        "            # Only update if not in refractory period\n",
        "            v = v * self.alpha + self.beta * i_in\n",
        "            v = v * not_refractory  # Reset to 0 during refractory\n",
        "            \n",
        "            # Generate spikes (Equation A.3)\n",
        "            spikes = tf.cast(v >= self.threshold, tf.float32)\n",
        "            \n",
        "            # Reset mechanism (Equation A.4)\n",
        "            v = v * (1.0 - spikes)  # Set to 0 where spike occurred\n",
        "            \n",
        "            # Update refractory counter\n",
        "            ref_count = ref_count - 1.0  # Decrement\n",
        "            ref_count = tf.where(\n",
        "                spikes > 0,\n",
        "                tf.ones_like(ref_count) * self.ref_steps,  # Reset to ref period\n",
        "                ref_count\n",
        "            )\n",
        "            ref_count = tf.maximum(ref_count, 0.0)  # Clip at 0\n",
        "            \n",
        "            # Store spikes\n",
        "            spikes_array = spikes_array.write(t, spikes)\n",
        "        \n",
        "        # Stack spikes: [time_steps, batch, units] → [batch, time_steps, units]\n",
        "        output_spikes = tf.transpose(spikes_array.stack(), [1, 0, 2])\n",
        "        \n",
        "        # For DFA, we'll handle gradients in a custom training loop\n",
        "        # For now, return spikes\n",
        "        return output_spikes\n",
        "\n",
        "\n",
        "print(\"✓ DFA_LIFLayer class defined\")\n",
        "print(\"\\nKey features:\")\n",
        "print(\"  1. Forward weights W (trainable)\")\n",
        "print(\"  2. Feedback matrix B (fixed, not trainable)\")\n",
        "print(\"  3. Standard LIF dynamics in forward pass\")\n",
        "print(\"  4. DFA error propagation in backward pass (custom training loop)\")"
      ],
      "metadata": {
        "id": "dfa_lif_layer"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "---\n",
        "# CELL 5: Surrogate Gradient Function\n",
        "---\n",
        "\n",
        "## What we're implementing:\n",
        "\n",
        "The **derivative approximation** f'(a) for the spike function.\n",
        "\n",
        "### Why we need this:\n",
        "\n",
        "The spike function is a step function:\n",
        "```\n",
        "spike(v) = 1 if v ≥ threshold\n",
        "           0 if v < threshold\n",
        "```\n",
        "\n",
        "Its true derivative is:\n",
        "```\n",
        "d/dv spike(v) = ∞ at v = threshold\n",
        "                0 everywhere else\n",
        "```\n",
        "\n",
        "This is not useful for training!\n",
        "\n",
        "### Paper's Solution (Appendix D, Equation A.9):\n",
        "\n",
        "Use a smooth approximation of the Dirac delta function:\n",
        "\n",
        "```\n",
        "f'(a) = [h_th · t_ref · τ / (a · (a - h_th))] / [t_ref + τ · log(a / (a - h_th))]²\n",
        "```\n",
        "\n",
        "Where:\n",
        "- **h_th:** Threshold (0.4)\n",
        "- **t_ref:** Refractory time (1ms)\n",
        "- **τ:** Time constant (20ms)\n",
        "- **a:** Pre-activation value (membrane potential)\n",
        "\n",
        "### Properties:\n",
        "- Peaked near threshold\n",
        "- Smooth and differentiable\n",
        "- Always positive (important for paper's PRFS)"
      ],
      "metadata": {
        "id": "cell5"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def surrogate_gradient_exact(a, h_th=0.4, t_ref=1.0, tau=20.0):\n",
        "    \"\"\"\n",
        "    Exact surrogate gradient from paper (Appendix D, Equation A.9).\n",
        "    \n",
        "    This is the smooth approximation of the Dirac delta function\n",
        "    that represents the derivative of the LIF spike function.\n",
        "    \n",
        "    Args:\n",
        "        a: Pre-activation values (membrane potential)\n",
        "        h_th: Spike threshold\n",
        "        t_ref: Refractory period (ms)\n",
        "        tau: Membrane time constant (ms)\n",
        "    \n",
        "    Returns:\n",
        "        Surrogate gradient f'(a)\n",
        "    \"\"\"\n",
        "    # Only compute gradient where a > threshold\n",
        "    # (derivative is 0 when a ≤ threshold)\n",
        "    \n",
        "    # Compute ratio a / (a - h_th)\n",
        "    # Add small epsilon to avoid division by zero\n",
        "    eps = 1e-8\n",
        "    ratio = a / (a - h_th + eps)\n",
        "    \n",
        "    # Compute numerator: h_th · t_ref · τ / [a · (a - h_th)]\n",
        "    numerator = h_th * t_ref * tau / (a * (a - h_th) + eps)\n",
        "    \n",
        "    # Compute denominator: [t_ref + τ · log(ratio)]²\n",
        "    log_term = tf.math.log(ratio + eps)\n",
        "    denominator = (t_ref + tau * log_term) ** 2 + eps\n",
        "    \n",
        "    # Compute gradient\n",
        "    grad = numerator / denominator\n",
        "    \n",
        "    # Only non-zero where a > threshold\n",
        "    grad = tf.where(a > h_th, grad, tf.zeros_like(grad))\n",
        "    \n",
        "    return grad\n",
        "\n",
        "\n",
        "def surrogate_gradient_fast_sigmoid(a, threshold=0.4, alpha=10.0):\n",
        "    \"\"\"\n",
        "    Alternative: Fast sigmoid surrogate gradient.\n",
        "    \n",
        "    This is simpler and commonly used in SNN training.\n",
        "    Formula: 1 / (1 + |alpha * (a - threshold)|)²\n",
        "    \n",
        "    Args:\n",
        "        a: Pre-activation values\n",
        "        threshold: Spike threshold\n",
        "        alpha: Steepness parameter\n",
        "    \n",
        "    Returns:\n",
        "        Surrogate gradient\n",
        "    \"\"\"\n",
        "    shifted = a - threshold\n",
        "    grad = 1.0 / (1.0 + tf.abs(alpha * shifted)) ** 2\n",
        "    return grad\n",
        "\n",
        "\n",
        "# Visualize both surrogate gradients\n",
        "a_values = np.linspace(-2, 2, 1000)\n",
        "threshold = 0.4\n",
        "\n",
        "# Compute gradients\n",
        "grad_exact = surrogate_gradient_exact(\n",
        "    tf.constant(a_values, dtype=tf.float32),\n",
        "    h_th=threshold\n",
        ").numpy()\n",
        "\n",
        "grad_sigmoid = surrogate_gradient_fast_sigmoid(\n",
        "    tf.constant(a_values, dtype=tf.float32),\n",
        "    threshold=threshold\n",
        ").numpy()\n",
        "\n",
        "# Plot\n",
        "plt.figure(figsize=(12, 4))\n",
        "\n",
        "plt.subplot(1, 2, 1)\n",
        "plt.plot(a_values, grad_exact, 'b-', linewidth=2, label=\"Exact (paper)\")\n",
        "plt.axvline(threshold, color='r', linestyle='--', alpha=0.5, label=f\"Threshold={threshold}\")\n",
        "plt.xlabel('Membrane Potential (a)')\n",
        "plt.ylabel(\"f'(a)\")\n",
        "plt.title('Paper\\'s Exact Surrogate Gradient\\n(Equation A.9)')\n",
        "plt.legend()\n",
        "plt.grid(True, alpha=0.3)\n",
        "\n",
        "plt.subplot(1, 2, 2)\n",
        "plt.plot(a_values, grad_sigmoid, 'g-', linewidth=2, label=\"Fast sigmoid\")\n",
        "plt.axvline(threshold, color='r', linestyle='--', alpha=0.5, label=f\"Threshold={threshold}\")\n",
        "plt.xlabel('Membrane Potential (a)')\n",
        "plt.ylabel(\"f'(a)\")\n",
        "plt.title('Fast Sigmoid Surrogate Gradient\\n(Alternative)')\n",
        "plt.legend()\n",
        "plt.grid(True, alpha=0.3)\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "print(\"✓ Surrogate gradient functions defined\")\n",
        "print(\"\\nNote: Both functions are always positive (required for paper's PRFS approach)\")"
      ],
      "metadata": {
        "id": "surrogate_grad"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "---\n",
        "# CELL 6: Custom DFA Training Loop\n",
        "---\n",
        "\n",
        "## What we're implementing:\n",
        "\n",
        "A **custom training loop** that implements DFA gradient computation.\n",
        "\n",
        "### Why custom loop?\n",
        "\n",
        "Keras's `model.fit()` uses standard backpropagation. For DFA, we need to:\n",
        "1. Compute global error at output\n",
        "2. Project it directly to each layer via B\n",
        "3. Compute layer-wise gradients using DFA formula\n",
        "\n",
        "### DFA Training Steps:\n",
        "\n",
        "#### 1. Forward Pass:\n",
        "```python\n",
        "spikes = model(inputs)  # Standard forward\n",
        "```\n",
        "\n",
        "#### 2. Compute Global Error:\n",
        "```python\n",
        "e_global = output - target  # [batch, time, output_dim]\n",
        "```\n",
        "\n",
        "#### 3. For Each Layer:\n",
        "```python\n",
        "# Project global error through fixed B\n",
        "e_projected = e_global @ B^T  # [batch, time, layer_dim]\n",
        "\n",
        "# Multiply by surrogate gradient\n",
        "e_layer = e_projected ⊙ f'(a)  # [batch, time, layer_dim]\n",
        "\n",
        "# Compute weight gradient\n",
        "∂L/∂W = (1/batch) · Σ_t (e_layer^T @ input)\n",
        "```\n",
        "\n",
        "#### 4. Update Weights:\n",
        "```python\n",
        "W = W - learning_rate · ∂L/∂W\n",
        "```\n",
        "\n",
        "### Key Insight:\n",
        "\n",
        "Unlike BP where errors propagate **sequentially** (layer N → N-1 → N-2...):\n",
        "```\n",
        "BP:  Output → Hidden2 → Hidden1\n",
        "           ↓         ↓         ↓\n",
        "         W2^T      W1^T    (slow!)\n",
        "```\n",
        "\n",
        "DFA sends errors **directly in parallel**:\n",
        "```\n",
        "DFA: Output ⇉ B_2 → Hidden2\n",
        "            ⇉ B_1 → Hidden1\n",
        "            (fast! parallelizable!)\n",
        "```"
      ],
      "metadata": {
        "id": "cell6"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class DFATrainer:\n",
        "    \"\"\"\n",
        "    Custom trainer implementing Direct Feedback Alignment.\n",
        "    \n",
        "    This replaces the standard backpropagation in model.fit()\n",
        "    with DFA's direct error injection.\n",
        "    \"\"\"\n",
        "    \n",
        "    def __init__(self, model, learning_rate=0.001, use_exact_gradient=True):\n",
        "        \"\"\"\n",
        "        Args:\n",
        "            model: Keras model with DFA_LIFLayers\n",
        "            learning_rate: Base learning rate\n",
        "            use_exact_gradient: If True, use paper's exact f'; else fast sigmoid\n",
        "        \"\"\"\n",
        "        self.model = model\n",
        "        self.base_lr = learning_rate\n",
        "        self.use_exact_gradient = use_exact_gradient\n",
        "        \n",
        "        # Get DFA layers (layers with feedback matrix B)\n",
        "        self.dfa_layers = [\n",
        "            layer for layer in model.layers \n",
        "            if isinstance(layer, DFA_LIFLayer) and hasattr(layer, 'B')\n",
        "        ]\n",
        "        \n",
        "        # Loss function\n",
        "        self.loss_fn = keras.losses.CategoricalCrossentropy(from_logits=False)\n",
        "        \n",
        "        # Metrics\n",
        "        self.train_loss = keras.metrics.Mean(name='train_loss')\n",
        "        self.train_acc = keras.metrics.CategoricalAccuracy(name='train_acc')\n",
        "        \n",
        "    def compute_spike_rate(self, spikes):\n",
        "        \"\"\"\n",
        "        Convert spike trains to rates (for classification).\n",
        "        Paper uses: output = neuron with highest spike count\n",
        "        \n",
        "        Args:\n",
        "            spikes: [batch, time, output_dim]\n",
        "        \n",
        "        Returns:\n",
        "            rates: [batch, output_dim] (sum over time)\n",
        "        \"\"\"\n",
        "        return tf.reduce_sum(spikes, axis=1)  # Sum over time\n",
        "    \n",
        "    def get_layer_learning_rate(self, layer):\n",
        "        \"\"\"\n",
        "        Paper: \"Learning rate inversely proportional to input dimension\"\n",
        "        (Section B)\n",
        "        \n",
        "        Args:\n",
        "            layer: DFA layer\n",
        "        \n",
        "        Returns:\n",
        "            lr: Adjusted learning rate\n",
        "        \"\"\"\n",
        "        input_dim = layer.w.shape[0]\n",
        "        return self.base_lr / input_dim\n",
        "    \n",
        "    @tf.function\n",
        "    def train_step(self, inputs, targets):\n",
        "        \"\"\"\n",
        "        Single DFA training step.\n",
        "        \n",
        "        Implements the DFA algorithm:\n",
        "        1. Forward pass\n",
        "        2. Compute global error\n",
        "        3. Project error to each layer via B\n",
        "        4. Compute gradients and update weights\n",
        "        \n",
        "        Args:\n",
        "            inputs: [batch, time, input_dim]\n",
        "            targets: [batch, output_dim] (one-hot labels)\n",
        "        \"\"\"\n",
        "        batch_size = tf.shape(inputs)[0]\n",
        "        time_steps = tf.shape(inputs)[1]\n",
        "        \n",
        "        # Track intermediate activations for gradient computation\n",
        "        activations = {}  # Store layer inputs\n",
        "        pre_activations = {}  # Store pre-synaptic currents (for f')\n",
        "        \n",
        "        with tf.GradientTape(persistent=True) as tape:\n",
        "            # FORWARD PASS\n",
        "            # Note: In TensorFlow, we need to manually track intermediate values\n",
        "            # for custom gradient computation\n",
        "            \n",
        "            current_input = inputs\n",
        "            \n",
        "            for i, layer in enumerate(self.model.layers):\n",
        "                if isinstance(layer, DFA_LIFLayer):\n",
        "                    # Store input to this layer (needed for ∂L/∂W = e · x^T)\n",
        "                    activations[layer.name] = current_input\n",
        "                    \n",
        "                    # Store pre-activations (needed for f'(a))\n",
        "                    # This is the weighted input: a = W^T x + b\n",
        "                    # We'll compute this during the layer call\n",
        "                    \n",
        "                # Forward through layer\n",
        "                current_input = layer(current_input, training=True)\n",
        "            \n",
        "            # Get final output spikes\n",
        "            output_spikes = current_input  # [batch, time, output_dim]\n",
        "            \n",
        "            # Convert spikes to rates\n",
        "            output_rates = self.compute_spike_rate(output_spikes)\n",
        "            \n",
        "            # Normalize to probabilities\n",
        "            output_probs = tf.nn.softmax(output_rates)\n",
        "            \n",
        "            # Compute loss\n",
        "            loss = self.loss_fn(targets, output_probs)\n",
        "        \n",
        "        # BACKWARD PASS (DFA)\n",
        "        \n",
        "        # 1. Compute global error at output\n",
        "        # e_global = prediction - target\n",
        "        e_global = output_probs - targets  # [batch, output_dim]\n",
        "        \n",
        "        # Expand to match time dimension\n",
        "        e_global_time = tf.expand_dims(e_global, axis=1)  # [batch, 1, output_dim]\n",
        "        e_global_time = tf.tile(e_global_time, [1, time_steps, 1])  # [batch, time, output_dim]\n",
        "        \n",
        "        # 2. For each DFA layer, compute gradients\n",
        "        for layer in reversed(self.dfa_layers):\n",
        "            # Get feedback matrix B for this layer\n",
        "            B = layer.B  # [layer_units, output_dim]\n",
        "            \n",
        "            # Project global error through B\n",
        "            # e_projected = e_global @ B^T\n",
        "            # Shape: [batch, time, output_dim] @ [output_dim, layer_units]\n",
        "            #      = [batch, time, layer_units]\n",
        "            e_projected = tf.matmul(e_global_time, B, transpose_b=True)\n",
        "            \n",
        "            # Get layer input (stored during forward pass)\n",
        "            layer_input = activations[layer.name]  # [batch, time, input_dim]\n",
        "            \n",
        "            # Compute pre-activation: a = x @ W + b\n",
        "            a = tf.matmul(layer_input, layer.w) + layer.b\n",
        "            # Shape: [batch, time, layer_units]\n",
        "            \n",
        "            # Compute surrogate gradient f'(a)\n",
        "            if self.use_exact_gradient:\n",
        "                f_prime = surrogate_gradient_exact(a, h_th=layer.threshold)\n",
        "            else:\n",
        "                f_prime = surrogate_gradient_fast_sigmoid(a, threshold=layer.threshold)\n",
        "            \n",
        "            # DFA error for this layer: e_n = e_projected ⊙ f'(a)\n",
        "            e_layer = e_projected * f_prime  # [batch, time, layer_units]\n",
        "            \n",
        "            # Compute weight gradient: ∂L/∂W = (1/T) Σ_t (e^T @ x)\n",
        "            # For each time step: e_layer[:, t, :] @ layer_input[:, t, :]\n",
        "            # Average over time and batch\n",
        "            \n",
        "            grad_w = tf.zeros_like(layer.w)\n",
        "            grad_b = tf.zeros_like(layer.b)\n",
        "            \n",
        "            for t in range(time_steps):\n",
        "                # Get values at time t\n",
        "                e_t = e_layer[:, t, :]  # [batch, layer_units]\n",
        "                x_t = layer_input[:, t, :]  # [batch, input_dim]\n",
        "                \n",
        "                # Gradient for weights: ∂L/∂W = x^T @ e\n",
        "                # Shape: [input_dim, batch] @ [batch, layer_units]\n",
        "                #      = [input_dim, layer_units]\n",
        "                grad_w += tf.matmul(x_t, e_t, transpose_a=True)\n",
        "                \n",
        "                # Gradient for bias: ∂L/∂b = sum over batch\n",
        "                grad_b += tf.reduce_sum(e_t, axis=0)\n",
        "            \n",
        "            # Average over time and batch\n",
        "            grad_w = grad_w / (tf.cast(time_steps, tf.float32) * tf.cast(batch_size, tf.float32))\n",
        "            grad_b = grad_b / (tf.cast(time_steps, tf.float32) * tf.cast(batch_size, tf.float32))\n",
        "            \n",
        "            # Get layer-specific learning rate\n",
        "            lr = self.get_layer_learning_rate(layer)\n",
        "            \n",
        "            # Update weights: W = W - lr · ∂L/∂W\n",
        "            layer.w.assign_sub(lr * grad_w)\n",
        "            layer.b.assign_sub(lr * grad_b)\n",
        "        \n",
        "        # Update metrics\n",
        "        self.train_loss.update_state(loss)\n",
        "        self.train_acc.update_state(targets, output_probs)\n",
        "        \n",
        "        return {\n",
        "            'loss': self.train_loss.result(),\n",
        "            'accuracy': self.train_acc.result()\n",
        "        }\n",
        "    \n",
        "    def fit(self, train_data, epochs, verbose=1):\n",
        "        \"\"\"\n",
        "        Train the model using DFA.\n",
        "        \n",
        "        Args:\n",
        "            train_data: tf.data.Dataset or (x, y) tuple\n",
        "            epochs: Number of training epochs\n",
        "            verbose: Verbosity level\n",
        "        \n",
        "        Returns:\n",
        "            history: Training metrics\n",
        "        \"\"\"\n",
        "        history = {'loss': [], 'accuracy': []}\n",
        "        \n",
        "        for epoch in range(epochs):\n",
        "            # Reset metrics\n",
        "            self.train_loss.reset_states()\n",
        "            self.train_acc.reset_states()\n",
        "            \n",
        "            # Training loop\n",
        "            if isinstance(train_data, tuple):\n",
        "                # Simple (x, y) format\n",
        "                x_train, y_train = train_data\n",
        "                # Split into batches manually\n",
        "                batch_size = 32\n",
        "                num_batches = len(x_train) // batch_size\n",
        "                \n",
        "                for batch_idx in range(num_batches):\n",
        "                    start_idx = batch_idx * batch_size\n",
        "                    end_idx = start_idx + batch_size\n",
        "                    \n",
        "                    x_batch = x_train[start_idx:end_idx]\n",
        "                    y_batch = y_train[start_idx:end_idx]\n",
        "                    \n",
        "                    metrics = self.train_step(x_batch, y_batch)\n",
        "            else:\n",
        "                # tf.data.Dataset format\n",
        "                for x_batch, y_batch in train_data:\n",
        "                    metrics = self.train_step(x_batch, y_batch)\n",
        "            \n",
        "            # Record metrics\n",
        "            loss = float(metrics['loss'])\n",
        "            acc = float(metrics['accuracy'])\n",
        "            history['loss'].append(loss)\n",
        "            history['accuracy'].append(acc)\n",
        "            \n",
        "            if verbose:\n",
        "                print(f\"Epoch {epoch+1}/{epochs} - \"\n",
        "                      f\"loss: {loss:.4f} - accuracy: {acc:.4f}\")\n",
        "        \n",
        "        return history\n",
        "\n",
        "\n",
        "print(\"✓ DFATrainer class defined\")\n",
        "print(\"\\nKey differences from standard training:\")\n",
        "print(\"  1. Direct error injection (e_global → B → each layer)\")\n",
        "print(\"  2. Layer-specific learning rates (inversely proportional to input dim)\")\n",
        "print(\"  3. Custom gradient computation (no automatic backprop)\")\n",
        "print(\"  4. Parallelizable error propagation\")"
      ],
      "metadata": {
        "id": "dfa_trainer"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "---\n",
        "# CELL 7: Build and Train DFA-SNN Model\n",
        "---\n",
        "\n",
        "## What we're doing:\n",
        "\n",
        "Putting it all together to train an SNN with DFA on MNIST.\n",
        "\n",
        "### Network Architecture (from paper):\n",
        "\n",
        "```\n",
        "784 (input) → 1000 (hidden) → 10 (output)\n",
        "```\n",
        "\n",
        "### Training Configuration:\n",
        "\n",
        "From Section B of the paper:\n",
        "- **Time steps:** 100ms total (20ms running period + 80ms training period)\n",
        "- **Time step (dt):** 0.25ms → 100 / 0.25 = 400 total steps\n",
        "- **Batch size:** 100\n",
        "- **Epochs:** 20\n",
        "- **Learning rate:** Layer-specific (inversely proportional to input dimension)\n",
        "- **Input encoding:** Direct mapping (no spike encoding - just repeat input)\n",
        "\n",
        "### Expected Performance:\n",
        "\n",
        "From Table 1 in the paper:\n",
        "- **DFA-SNNs:** ~96.75% (best), ~92.09% (average)\n",
        "- **aDFA-SNNs:** ~98.01% (best), ~97.91% (average)\n",
        "\n",
        "### Why DFA works:\n",
        "\n",
        "Even though B is random and fixed:\n",
        "1. **Alignment phenomenon:** B aligns with W^T during training\n",
        "2. **Approximate direction:** B provides roughly correct error direction\n",
        "3. **Robustness:** Network adapts to imperfect feedback"
      ],
      "metadata": {
        "id": "cell7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Load MNIST data\n",
        "(x_train, y_train), (x_test, y_test) = keras.datasets.mnist.load_data()\n",
        "\n",
        "# Normalize\n",
        "x_train = x_train.astype('float32') / 255.0\n",
        "x_test = x_test.astype('float32') / 255.0\n",
        "\n",
        "# Flatten\n",
        "x_train = x_train.reshape(-1, 784)\n",
        "x_test = x_test.reshape(-1, 784)\n",
        "\n",
        "# One-hot encode labels\n",
        "y_train = keras.utils.to_categorical(y_train, 10)\n",
        "y_test = keras.utils.to_categorical(y_test, 10)\n",
        "\n",
        "print(f\"Training samples: {len(x_train)}\")\n",
        "print(f\"Test samples: {len(x_test)}\")\n",
        "print(f\"Input shape: {x_train.shape[1]}\")\n",
        "print(f\"Output classes: {y_train.shape[1]}\")\n",
        "\n",
        "# Paper's configuration\n",
        "TIME_STEPS = 100  # Paper uses 100ms total (simplified from 400 steps)\n",
        "HIDDEN_SIZE = 1000  # Paper uses 1000 hidden neurons\n",
        "OUTPUT_SIZE = 10\n",
        "INPUT_SIZE = 784\n",
        "\n",
        "# For demo, use smaller dataset\n",
        "TRAIN_SAMPLES = 1000  # Use 1000 samples for quick demo\n",
        "x_train_small = x_train[:TRAIN_SAMPLES]\n",
        "y_train_small = y_train[:TRAIN_SAMPLES]\n",
        "\n",
        "# Expand to time dimension (paper uses \"direct mapping\")\n",
        "# Just repeat the input for each time step\n",
        "x_train_spikes = np.tile(\n",
        "    x_train_small[:, np.newaxis, :],  # Add time dim\n",
        "    (1, TIME_STEPS, 1)  # Repeat TIME_STEPS times\n",
        ")\n",
        "\n",
        "print(f\"\\nInput shape with time: {x_train_spikes.shape}\")\n",
        "print(f\"  [batch={x_train_spikes.shape[0]}, \"\n",
        "      f\"time={x_train_spikes.shape[1]}, \"\n",
        "      f\"features={x_train_spikes.shape[2]}]\")\n",
        "\n",
        "# Build DFA-SNN model\n",
        "print(\"\\n\" + \"=\"*60)\n",
        "print(\"BUILDING DFA-SNN MODEL\")\n",
        "print(\"=\"*60)\n",
        "\n",
        "inputs = keras.Input(shape=(TIME_STEPS, INPUT_SIZE))\n",
        "\n",
        "# Hidden layer with DFA\n",
        "hidden = DFA_LIFLayer(\n",
        "    units=HIDDEN_SIZE,\n",
        "    output_size=OUTPUT_SIZE,  # For B matrix initialization\n",
        "    tau=20.0,\n",
        "    dt=0.25,\n",
        "    threshold=0.4,\n",
        "    t_ref=1.0,\n",
        "    use_dfa=True,\n",
        "    gamma=0.0338,\n",
        "    name='hidden_layer'\n",
        ")(inputs)\n",
        "\n",
        "# Output layer (also DFA, but projects to itself)\n",
        "outputs = DFA_LIFLayer(\n",
        "    units=OUTPUT_SIZE,\n",
        "    output_size=OUTPUT_SIZE,\n",
        "    tau=20.0,\n",
        "    dt=0.25,\n",
        "    threshold=0.4,\n",
        "    t_ref=1.0,\n",
        "    use_dfa=False,  # Output layer doesn't need B\n",
        "    name='output_layer'\n",
        ")(hidden)\n",
        "\n",
        "model = keras.Model(inputs=inputs, outputs=outputs)\n",
        "\n",
        "model.summary()\n",
        "\n",
        "# Create DFA trainer\n",
        "print(\"\\n\" + \"=\"*60)\n",
        "print(\"INITIALIZING DFA TRAINER\")\n",
        "print(\"=\"*60)\n",
        "\n",
        "trainer = DFATrainer(\n",
        "    model=model,\n",
        "    learning_rate=0.001,  # Base LR (will be adjusted per layer)\n",
        "    use_exact_gradient=True  # Use paper's exact f'\n",
        ")\n",
        "\n",
        "print(f\"\\nDFA layers found: {len(trainer.dfa_layers)}\")\n",
        "for layer in trainer.dfa_layers:\n",
        "    lr = trainer.get_layer_learning_rate(layer)\n",
        "    print(f\"  {layer.name}:\")\n",
        "    print(f\"    Units: {layer.units}\")\n",
        "    print(f\"    B shape: {layer.B.shape}\")\n",
        "    print(f\"    Learning rate: {lr:.6f}\")\n",
        "\n",
        "# Train with DFA\n",
        "print(\"\\n\" + \"=\"*60)\n",
        "print(\"TRAINING WITH DFA\")\n",
        "print(\"=\"*60)\n",
        "print(\"\\nNote: This is using DIRECT FEEDBACK ALIGNMENT\")\n",
        "print(\"  - Errors bypass layer-by-layer propagation\")\n",
        "print(\"  - Each layer receives error directly from output\")\n",
        "print(\"  - Feedback matrices B are FIXED (not trained)\\n\")\n",
        "\n",
        "history = trainer.fit(\n",
        "    train_data=(x_train_spikes, y_train_small),\n",
        "    epochs=5,  # Quick demo (paper uses 20)\n",
        "    verbose=1\n",
        ")\n",
        "\n",
        "# Plot results\n",
        "plt.figure(figsize=(12, 4))\n",
        "\n",
        "plt.subplot(1, 2, 1)\n",
        "plt.plot(history['loss'], 'b-', linewidth=2)\n",
        "plt.xlabel('Epoch')\n",
        "plt.ylabel('Loss')\n",
        "plt.title('Training Loss (DFA-SNN)')\n",
        "plt.grid(True, alpha=0.3)\n",
        "\n",
        "plt.subplot(1, 2, 2)\n",
        "plt.plot(history['accuracy'], 'g-', linewidth=2)\n",
        "plt.xlabel('Epoch')\n",
        "plt.ylabel('Accuracy')\n",
        "plt.title('Training Accuracy (DFA-SNN)')\n",
        "plt.grid(True, alpha=0.3)\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "print(\"\\n✓ DFA-SNN training complete!\")\n",
        "print(f\"\\nFinal metrics:\")\n",
        "print(f\"  Loss: {history['loss'][-1]:.4f}\")\n",
        "print(f\"  Accuracy: {history['accuracy'][-1]:.4f}\")\n",
        "print(f\"\\nPaper's reported performance (MNIST):\")\n",
        "print(f\"  DFA-SNNs: ~96.75% (best), ~92.09% (average)\")\n",
        "print(f\"  aDFA-SNNs: ~98.01% (best), ~97.91% (average)\")"
      ],
      "metadata": {
        "id": "train_model"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "---\n",
        "# Summary: What We Implemented\n",
        "---\n",
        "\n",
        "## 1. LIF Neuron Dynamics\n",
        "- Membrane potential leakage\n",
        "- Spike threshold\n",
        "- Reset mechanism\n",
        "- Refractory period\n",
        "\n",
        "## 2. Fixed Random Feedback Matrices (B)\n",
        "- Initialized using paper's formula\n",
        "- NOT trainable (fixed throughout training)\n",
        "- Projects global error to each layer\n",
        "\n",
        "## 3. Surrogate Gradient\n",
        "- Smooth approximation of spike derivative\n",
        "- Enables gradient-based learning\n",
        "- Paper's exact formula or fast sigmoid\n",
        "\n",
        "## 4. DFA Training Algorithm\n",
        "- Direct error injection (no layer-by-layer propagation)\n",
        "- Custom gradient computation\n",
        "- Layer-specific learning rates\n",
        "\n",
        "---\n",
        "\n",
        "## Key Insights:\n",
        "\n",
        "### Why DFA is Better for SNNs:\n",
        "\n",
        "1. **Biologically Plausible**\n",
        "   - No symmetric feedback weights\n",
        "   - Local learning rules\n",
        "   - Matches brain's structure\n",
        "\n",
        "2. **Hardware Friendly**\n",
        "   - Simpler to implement on chips\n",
        "   - No need to store W^T\n",
        "   - Parallelizable\n",
        "\n",
        "3. **Gradient-Free**\n",
        "   - Works with non-differentiable spikes\n",
        "   - No complex surrogate design needed\n",
        "   - More robust\n",
        "\n",
        "### The \"Magic\" of DFA:\n",
        "\n",
        "Even though B is random:\n",
        "- It provides approximate error direction\n",
        "- Network learns to align with feedback\n",
        "- Performance comparable to BP!\n",
        "\n",
        "---\n",
        "\n",
        "## Next Steps:\n",
        "\n",
        "1. **Augmented DFA (aDFA):**\n",
        "   - Replace f' with arbitrary function g\n",
        "   - Even more flexible\n",
        "   - Better performance (98%+ on MNIST)\n",
        "\n",
        "2. **Hyperparameter Tuning:**\n",
        "   - Genetic algorithm for g parameters\n",
        "   - Optimize B scale factor γ\n",
        "   - Network architecture search\n",
        "\n",
        "3. **Deeper Networks:**\n",
        "   - Multiple hidden layers\n",
        "   - Convolutional SNNs\n",
        "   - More complex tasks\n",
        "\n",
        "---\n",
        "\n",
        "## References:\n",
        "\n",
        "Zhang et al. (2024). \"Training Spiking Neural Networks via Augmented Direct Feedback Alignment\"\n",
        "- arXiv:2409.07776\n",
        "- Key equations: A.1-A.9, Appendices A-K\n"
      ],
      "metadata": {
        "id": "summary"
      }
    }
  ]
}
