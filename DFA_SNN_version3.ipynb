{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "A100",
      "authorship_tag": "ABX9TyPIWg/lkQasnGaZgSS88E9c",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/JoshuaNalla/SNN-with-IFA/blob/main/DFA_SNN_version3.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "RAKaR4LGncaI"
      },
      "outputs": [],
      "source": [
        "# ============================================================================\n",
        "# CELL 1: IMPORTS AND POISSON ENCODING\n",
        "# ============================================================================\n",
        "\n",
        "# Import core libraries\n",
        "import math\n",
        "import numpy as np\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "from torchvision import datasets, transforms\n",
        "from torch.utils.data import DataLoader\n",
        "\n",
        "# for evaluation purposes, importing scikit.metric\n",
        "from sklearn.metrics import confusion_matrix\n",
        "\n",
        "def poisson_encoding(x, time_steps, max_rate=0.8):\n",
        "    \"\"\"\n",
        "    Convert static inputs to temporal spike trains via Poisson process.\n",
        "\n",
        "    Args:\n",
        "        x: [batch, features] - static input (pixel intensities in [0,1])\n",
        "        time_steps: number of time steps\n",
        "        max_rate: maximum firing rate (probability per time step)\n",
        "\n",
        "    Returns:\n",
        "        x_time: [batch, time_steps, features] - temporal spike trains\n",
        "    \"\"\"\n",
        "    batch_size = x.shape[0]\n",
        "    features = x.shape[1]\n",
        "    x_time = torch.zeros(batch_size, time_steps, features, device=x.device)\n",
        "\n",
        "    for t in range(time_steps):\n",
        "        # Generate spikes with probability proportional to input intensity\n",
        "        spikes = (torch.rand_like(x) < (x * max_rate)).float()\n",
        "        x_time[:, t, :] = spikes\n",
        "\n",
        "    return x_time"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# ============================================================================\n",
        "# CELL 2: UPDATED SURROGATE GRADIENTS\n",
        "# ============================================================================\n",
        "\n",
        "def surrogate_gradient_exact(a, h_th=0.1, t_ref=1.0, tau=20.0):\n",
        "    \"\"\"\n",
        "    Exact surrogate gradient from paper (Appendix D, Equation A.9).\n",
        "    PyTorch version.\n",
        "    \"\"\"\n",
        "    eps = 1e-8\n",
        "\n",
        "    # Compute ratio a / (a - h_th)\n",
        "    ratio = a / (a - h_th + eps)\n",
        "\n",
        "    # Numerator: h_th * t_ref * tau / [a * (a - h_th)]\n",
        "    numerator = h_th * t_ref * tau / (a * (a - h_th) + eps)\n",
        "\n",
        "    # Denominator: [t_ref + tau * log(ratio)]^2\n",
        "    log_term = torch.log(ratio + eps)\n",
        "    denominator = (t_ref + tau * log_term) ** 2 + eps\n",
        "\n",
        "    grad = numerator / denominator\n",
        "\n",
        "    # Only non-zero where a > threshold\n",
        "    grad = torch.where(a > h_th, grad, torch.zeros_like(grad))\n",
        "\n",
        "    return grad\n",
        "\n",
        "\n",
        "def surrogate_gradient_fast_sigmoid(a, threshold=0.1, alpha=10):\n",
        "    \"\"\"\n",
        "    Fast sigmoid surrogate gradient (PyTorch version).\n",
        "    Increased alpha from 5 to 10 for steeper gradients.\n",
        "    \"\"\"\n",
        "    shifted = a - threshold\n",
        "    grad = 1.0 / (1.0 + torch.abs(alpha * shifted)) ** 2\n",
        "    return grad\n",
        "\n"
      ],
      "metadata": {
        "id": "_uffwnA7n4m0"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ============================================================================\n",
        "# CELL 3: DFA LIF LAYER (unchanged but included for completeness)\n",
        "# ============================================================================\n",
        "\n",
        "class DFA_LIFLayer(nn.Module):\n",
        "    \"\"\"\n",
        "    LIF layer that outputs spikes over time.\n",
        "    Has fixed random feedback matrix B for DFA training.\n",
        "    \"\"\"\n",
        "    def __init__(self, in_features, units, output_size,\n",
        "                 tau=20.0, dt=0.25, threshold=0.1, t_ref=1.0,\n",
        "                 use_dfa=True, gamma=0.0338):\n",
        "        super().__init__()\n",
        "        self.in_features = in_features\n",
        "        self.units = units\n",
        "        self.output_size = output_size\n",
        "\n",
        "        self.tau = tau\n",
        "        self.dt = dt\n",
        "        self.threshold = threshold\n",
        "        self.t_ref = t_ref\n",
        "        self.use_dfa = use_dfa\n",
        "        self.gamma = gamma\n",
        "\n",
        "        # Trainable weights\n",
        "        self.w = nn.Parameter(torch.empty(in_features, units))\n",
        "        self.b = nn.Parameter(torch.zeros(units))\n",
        "        nn.init.constant_(self.b, 0.2)\n",
        "\n",
        "        # Init weights\n",
        "        nn.init.xavier_uniform_(self.w)\n",
        "\n",
        "        # Refractory steps\n",
        "        self.ref_steps = int(round(self.t_ref / self.dt))\n",
        "\n",
        "        # Fixed random feedback matrix B\n",
        "        if use_dfa:\n",
        "            B_init = torch.randn(units, output_size) * self.gamma\n",
        "            self.register_buffer(\"B\", B_init)\n",
        "\n",
        "    def forward(self, inputs):\n",
        "        \"\"\"\n",
        "        inputs: [batch, time, in_features]\n",
        "        returns spikes: [batch, time, units]\n",
        "        \"\"\"\n",
        "        batch_size, time_steps, _ = inputs.shape\n",
        "\n",
        "        v = torch.zeros(batch_size, self.units, device=inputs.device)\n",
        "        ref_count = torch.zeros(batch_size, self.units, device=inputs.device)\n",
        "\n",
        "        spikes_out = []\n",
        "\n",
        "        alpha = 1.0 - (self.dt / self.tau)\n",
        "        beta  = self.dt / self.tau\n",
        "\n",
        "        for t in range(time_steps):\n",
        "            x_t = inputs[:, t, :]\n",
        "\n",
        "            I_t = (x_t @ self.w) + self.b\n",
        "            v = alpha * v + beta * I_t\n",
        "\n",
        "            # Spike generation\n",
        "            spike = (v >= self.threshold).float()\n",
        "\n",
        "            # Apply refractory period\n",
        "            in_ref = (ref_count > 0).float()\n",
        "            spike = spike * (1.0 - in_ref)\n",
        "\n",
        "            # Reset voltage for spiking neurons\n",
        "            v = v * (1.0 - spike)\n",
        "\n",
        "            # Update refractory counter\n",
        "            ref_count = torch.where(spike > 0.5,\n",
        "                                   torch.full_like(ref_count, float(self.ref_steps)),\n",
        "                                   ref_count)\n",
        "            ref_count = torch.clamp(ref_count - 1.0, min=0.0)\n",
        "\n",
        "            spikes_out.append(spike)\n",
        "\n",
        "        spikes_out = torch.stack(spikes_out, dim=1)\n",
        "        return spikes_out"
      ],
      "metadata": {
        "id": "Nn4VNK5-n8sh"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ============================================================================\n",
        "# CELL 4: UPDATED DFA-SNN MODEL WITH BETTER THRESHOLDS\n",
        "# ============================================================================\n",
        "\n",
        "class DFASNN(nn.Module):\n",
        "    def __init__(self, time_steps, input_size, hidden_size1, hidden_size2, hidden_size3, output_size):\n",
        "        super().__init__()\n",
        "        self.time_steps = time_steps\n",
        "\n",
        "        # 1st hidden layer (DFA) - consistent threshold\n",
        "        self.hidden_layer1 = DFA_LIFLayer(\n",
        "            in_features=input_size,\n",
        "            units=hidden_size1,\n",
        "            output_size=output_size,\n",
        "            tau=20.0,\n",
        "            dt=0.25,\n",
        "            threshold=0.05,  # Consistent threshold\n",
        "            t_ref=1.0,\n",
        "            use_dfa=True,\n",
        "            gamma=0.0338\n",
        "        )\n",
        "\n",
        "        # 2nd hidden layer (DFA)\n",
        "        self.hidden_layer2 = DFA_LIFLayer(\n",
        "            in_features=hidden_size1,\n",
        "            units=hidden_size2,\n",
        "            output_size=output_size,\n",
        "            tau=20.0,\n",
        "            dt=0.25,\n",
        "            threshold=0.02,  # Consistent threshold\n",
        "            t_ref=1.0,\n",
        "            use_dfa=True,\n",
        "            gamma=0.0338\n",
        "        )\n",
        "\n",
        "        # 3rd hidden layer (DFA)\n",
        "        self.hidden_layer3 = DFA_LIFLayer(\n",
        "            in_features=hidden_size2,\n",
        "            units=hidden_size3,\n",
        "            output_size=output_size,\n",
        "            tau=20.0,\n",
        "            dt=0.25,\n",
        "            threshold=0.01,  # Consistent threshold\n",
        "            t_ref=1.0,\n",
        "            use_dfa=True,\n",
        "            gamma=0.0338\n",
        "        )\n",
        "\n",
        "        # Output layer (no DFA)\n",
        "        self.output_layer = DFA_LIFLayer(\n",
        "            in_features=hidden_size3,\n",
        "            units=output_size,\n",
        "            output_size=output_size,\n",
        "            tau=20.0,\n",
        "            dt=0.25,\n",
        "            threshold=0.15,  # Slightly higher for output\n",
        "            t_ref=1.0,\n",
        "            use_dfa=False\n",
        "        )\n",
        "\n",
        "    def forward(self, x_time):\n",
        "        \"\"\"\n",
        "        x_time: [batch, time, 784]\n",
        "        returns:\n",
        "          h1_spikes: [batch, time, hidden_size1]\n",
        "          h2_spikes: [batch, time, hidden_size2]\n",
        "          h3_spikes: [batch, time, hidden_size3]\n",
        "          y_spikes:  [batch, time, output_size]\n",
        "        \"\"\"\n",
        "        h1_spikes = self.hidden_layer1(x_time)\n",
        "        h2_spikes = self.hidden_layer2(h1_spikes)\n",
        "        h3_spikes = self.hidden_layer3(h2_spikes)\n",
        "        y_spikes = self.output_layer(h3_spikes)\n",
        "        return h1_spikes, h2_spikes, h3_spikes, y_spikes\n",
        "\n"
      ],
      "metadata": {
        "id": "3K5o0lqPoEGy"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ============================================================================\n",
        "# CELL 5: UPDATED DFA TRAINER WITH LEARNING RATE DECAY\n",
        "# ============================================================================\n",
        "\n",
        "class DFATrainerTorch:\n",
        "    def __init__(self, model, learning_rate=0.1, use_exact_gradient=False,\n",
        "                 lr_decay=0.95, decay_every=10):\n",
        "        self.model = model\n",
        "        self.lr = learning_rate\n",
        "        self.initial_lr = learning_rate\n",
        "        self.lr_decay = lr_decay\n",
        "        self.decay_every = decay_every\n",
        "        self.use_exact_gradient = use_exact_gradient\n",
        "        self.epoch_counter = 0\n",
        "\n",
        "    @staticmethod\n",
        "    def compute_spike_rate(spikes):\n",
        "        return spikes.sum(dim=1)  # [batch, units]\n",
        "\n",
        "    def step_lr(self):\n",
        "        \"\"\"Decay learning rate periodically\"\"\"\n",
        "        self.epoch_counter += 1\n",
        "        if self.epoch_counter % self.decay_every == 0:\n",
        "            self.lr *= self.lr_decay\n",
        "            print(f\"  → Learning rate decayed to {self.lr:.6f}\")\n",
        "\n",
        "    def train_step(self, x_time, y_onehot):\n",
        "        \"\"\"\n",
        "        x_time:   [b,t,784]\n",
        "        y_onehot: [b,10]\n",
        "        returns: (loss_float, acc_float)\n",
        "        \"\"\"\n",
        "        self.model.train()\n",
        "        x_time = x_time.to(device)\n",
        "        y_onehot = y_onehot.to(device)\n",
        "\n",
        "        # Forward (3 hidden layers)\n",
        "        h1_spikes, h2_spikes, h3_spikes, y_spikes = self.model(x_time)\n",
        "        out_rates = self.compute_spike_rate(y_spikes)\n",
        "\n",
        "        # Temperature scaling for better gradients\n",
        "        out_rates_norm = out_rates / (out_rates.sum(dim=1, keepdim=True) + 1e-9)\n",
        "        out_probs = F.softmax(out_rates_norm / 0.5, dim=1)\n",
        "\n",
        "        eps = 1e-9\n",
        "        loss = -(y_onehot * torch.log(out_probs + eps)).sum(dim=1).mean()\n",
        "\n",
        "        # Global error (broadcast to time)\n",
        "        e_global = out_probs - y_onehot\n",
        "        e_time = e_global.unsqueeze(1).repeat(1, y_spikes.size(1), 1)\n",
        "\n",
        "        bsz, T, _ = y_spikes.shape\n",
        "        denom = float(bsz * T)\n",
        "\n",
        "        with torch.no_grad():\n",
        "            # =========================================================\n",
        "            # 1) OUTPUT LAYER UPDATE (uses real output error)\n",
        "            # =========================================================\n",
        "            out_layer = self.model.output_layer\n",
        "            out_input = h3_spikes\n",
        "\n",
        "            a_out = (out_input @ out_layer.w) + out_layer.b\n",
        "\n",
        "            if self.use_exact_gradient:\n",
        "                fprime_out = surrogate_gradient_exact(\n",
        "                    a_out, h_th=out_layer.threshold, t_ref=out_layer.t_ref, tau=out_layer.tau\n",
        "                )\n",
        "            else:\n",
        "                fprime_out = surrogate_gradient_fast_sigmoid(\n",
        "                    a_out, threshold=out_layer.threshold, alpha=10.0\n",
        "                )\n",
        "\n",
        "            e_out = e_time * fprime_out\n",
        "\n",
        "            grad_w_out = torch.einsum(\"bti,btj->ij\", out_input, e_out) / denom\n",
        "            grad_b_out = e_out.sum(dim=(0, 1)) / denom\n",
        "\n",
        "            out_layer.w -= self.lr * grad_w_out\n",
        "            out_layer.b -= self.lr * grad_b_out\n",
        "\n",
        "            # =========================================================\n",
        "            # 2) HIDDEN LAYER 3 DFA UPDATE\n",
        "            # =========================================================\n",
        "            hid3_layer = self.model.hidden_layer3\n",
        "\n",
        "            e_proj3 = torch.einsum(\"bto,ho->bth\", e_time, hid3_layer.B)\n",
        "            a_hid3 = (h2_spikes @ hid3_layer.w) + hid3_layer.b\n",
        "\n",
        "            if self.use_exact_gradient:\n",
        "                fprime_hid3 = surrogate_gradient_exact(\n",
        "                    a_hid3, h_th=hid3_layer.threshold, t_ref=hid3_layer.t_ref, tau=hid3_layer.tau\n",
        "                )\n",
        "            else:\n",
        "                fprime_hid3 = surrogate_gradient_fast_sigmoid(\n",
        "                    a_hid3, threshold=hid3_layer.threshold, alpha=10.0\n",
        "                )\n",
        "\n",
        "            e_hid3 = e_proj3 * fprime_hid3\n",
        "\n",
        "            grad_w_hid3 = torch.einsum(\"bti,btj->ij\", h2_spikes, e_hid3) / denom\n",
        "            grad_b_hid3 = e_hid3.sum(dim=(0, 1)) / denom\n",
        "\n",
        "            hid3_layer.w -= self.lr * grad_w_hid3\n",
        "            hid3_layer.b -= self.lr * grad_b_hid3\n",
        "\n",
        "            # =========================================================\n",
        "            # 3) HIDDEN LAYER 2 DFA UPDATE\n",
        "            # =========================================================\n",
        "            hid2_layer = self.model.hidden_layer2\n",
        "\n",
        "            e_proj2 = torch.einsum(\"bto,ho->bth\", e_time, hid2_layer.B)\n",
        "            a_hid2 = (h1_spikes @ hid2_layer.w) + hid2_layer.b\n",
        "\n",
        "            if self.use_exact_gradient:\n",
        "                fprime_hid2 = surrogate_gradient_exact(\n",
        "                    a_hid2, h_th=hid2_layer.threshold, t_ref=hid2_layer.t_ref, tau=hid2_layer.tau\n",
        "                )\n",
        "            else:\n",
        "                fprime_hid2 = surrogate_gradient_fast_sigmoid(\n",
        "                    a_hid2, threshold=hid2_layer.threshold, alpha=10.0\n",
        "                )\n",
        "\n",
        "            e_hid2 = e_proj2 * fprime_hid2\n",
        "\n",
        "            grad_w_hid2 = torch.einsum(\"bti,btj->ij\", h1_spikes, e_hid2) / denom\n",
        "            grad_b_hid2 = e_hid2.sum(dim=(0, 1)) / denom\n",
        "\n",
        "            hid2_layer.w -= self.lr * grad_w_hid2\n",
        "            hid2_layer.b -= self.lr * grad_b_hid2\n",
        "\n",
        "            # =========================================================\n",
        "            # 4) HIDDEN LAYER 1 DFA UPDATE\n",
        "            # =========================================================\n",
        "            hid1_layer = self.model.hidden_layer1\n",
        "\n",
        "            e_proj1 = torch.einsum(\"bto,ho->bth\", e_time, hid1_layer.B)\n",
        "            a_hid1 = (x_time @ hid1_layer.w) + hid1_layer.b\n",
        "\n",
        "            if self.use_exact_gradient:\n",
        "                fprime_hid1 = surrogate_gradient_exact(\n",
        "                    a_hid1, h_th=hid1_layer.threshold, t_ref=hid1_layer.t_ref, tau=hid1_layer.tau\n",
        "                )\n",
        "            else:\n",
        "                fprime_hid1 = surrogate_gradient_fast_sigmoid(\n",
        "                    a_hid1, threshold=hid1_layer.threshold, alpha=10.0\n",
        "                )\n",
        "\n",
        "            e_hid1 = e_proj1 * fprime_hid1\n",
        "\n",
        "            grad_w_hid1 = torch.einsum(\"bti,btj->ij\", x_time, e_hid1) / denom\n",
        "            grad_b_hid1 = e_hid1.sum(dim=(0, 1)) / denom\n",
        "\n",
        "            hid1_layer.w -= self.lr * grad_w_hid1\n",
        "            hid1_layer.b -= self.lr * grad_b_hid1\n",
        "\n",
        "        # Accuracy\n",
        "        preds = torch.argmax(out_probs, dim=1)\n",
        "        true = torch.argmax(y_onehot, dim=1)\n",
        "        acc = (preds == true).float().mean().item()\n",
        "\n",
        "        return loss.item(), acc\n",
        "\n"
      ],
      "metadata": {
        "id": "2UtrLAVHoL0l"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ============================================================================\n",
        "# CELL 6: TRAINING SETUP WITH ALL FIXES\n",
        "# ============================================================================\n",
        "\n",
        "import numpy as np\n",
        "import torch\n",
        "import torch.nn.functional as F\n",
        "from torchvision import datasets, transforms\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# ----------------------------\n",
        "# Load MNIST data (PyTorch)\n",
        "# ----------------------------\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "print(f\"Using device: {device}\")\n",
        "\n",
        "transform = transforms.Compose([\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Lambda(lambda x: x.view(-1))\n",
        "])\n",
        "\n",
        "train_ds_full = datasets.MNIST(root=\"./data\", train=True, download=True, transform=transform)\n",
        "test_ds_full  = datasets.MNIST(root=\"./data\", train=False, download=True, transform=transform)\n",
        "\n",
        "# Convert to tensors\n",
        "x_train = torch.stack([train_ds_full[i][0] for i in range(len(train_ds_full))]).float()\n",
        "y_train_int = torch.tensor([train_ds_full[i][1] for i in range(len(train_ds_full))], dtype=torch.long)\n",
        "\n",
        "x_test  = torch.stack([test_ds_full[i][0] for i in range(len(test_ds_full))]).float()\n",
        "y_test_int  = torch.tensor([test_ds_full[i][1] for i in range(len(test_ds_full))], dtype=torch.long)\n",
        "\n",
        "# One-hot encode labels\n",
        "y_train = F.one_hot(y_train_int, num_classes=10).float()\n",
        "y_test  = F.one_hot(y_test_int,  num_classes=10).float()\n",
        "\n",
        "print(f\"Training samples: {len(x_train)}\")\n",
        "print(f\"Test samples: {len(x_test)}\")\n",
        "\n",
        "# ----------------------------\n",
        "# Hyperparams\n",
        "# ----------------------------\n",
        "TIME_STEPS = 25\n",
        "HIDDEN_SIZE1 = 1000\n",
        "HIDDEN_SIZE2 = 250\n",
        "HIDDEN_SIZE3 = 100\n",
        "OUTPUT_SIZE = 10\n",
        "INPUT_SIZE = 784\n",
        "\n",
        "TRAIN_SAMPLES = 60000\n",
        "VAL_FRAC = 0.10\n",
        "\n",
        "EPOCHS = 50\n",
        "BATCH_SIZE = 128\n",
        "EVAL_BATCH = 256\n",
        "\n",
        "# ----------------------------\n",
        "# CRITICAL FIX: Use Poisson encoding instead of static repetition\n",
        "# ----------------------------\n",
        "print(\"\\n\" + \"=\"*60)\n",
        "print(\"ENCODING INPUTS WITH POISSON PROCESS\")\n",
        "print(\"=\"*60)\n",
        "\n",
        "x_train_small = x_train[:TRAIN_SAMPLES]\n",
        "y_train_small = y_train[:TRAIN_SAMPLES]\n",
        "\n",
        "# Poisson encoding with max_rate=0.8\n",
        "print(\"Encoding training data...\")\n",
        "x_train_spikes = poisson_encoding(x_train_small, TIME_STEPS, max_rate=0.8)\n",
        "\n",
        "print(f\"Input shape with time: {tuple(x_train_spikes.shape)}\")\n",
        "print(f\"  [batch={x_train_spikes.shape[0]}, time={x_train_spikes.shape[1]}, features={x_train_spikes.shape[2]}]\")\n",
        "\n",
        "# ----------------------------\n",
        "# Build DFA-SNN model\n",
        "# ----------------------------\n",
        "print(\"\\n\" + \"=\"*60)\n",
        "print(\"BUILDING DFA-SNN MODEL\")\n",
        "print(\"=\"*60)\n",
        "\n",
        "model = DFASNN(\n",
        "    time_steps=TIME_STEPS,\n",
        "    input_size=INPUT_SIZE,\n",
        "    hidden_size1=HIDDEN_SIZE1,\n",
        "    hidden_size2=HIDDEN_SIZE2,\n",
        "    hidden_size3=HIDDEN_SIZE3,\n",
        "    output_size=OUTPUT_SIZE\n",
        ").to(device)\n",
        "\n",
        "# CRITICAL FIX: Consistent bias initialization\n",
        "with torch.no_grad():\n",
        "    model.hidden_layer1.b.fill_(0.5)  # Consistent\n",
        "    model.hidden_layer2.b.fill_(0.5)  # Consistent\n",
        "    model.hidden_layer3.b.fill_(0.5)  # Consistent\n",
        "    model.output_layer.b.fill_(1.0)   # Slightly higher for output\n",
        "\n",
        "print(model)\n",
        "\n",
        "# ----------------------------\n",
        "# Create DFA trainer with LR decay\n",
        "# ----------------------------\n",
        "print(\"\\n\" + \"=\"*60)\n",
        "print(\"INITIALIZING DFA TRAINER\")\n",
        "print(\"=\"*60)\n",
        "\n",
        "trainer = DFATrainerTorch(\n",
        "    model=model,\n",
        "    learning_rate=0.1,\n",
        "    use_exact_gradient=False,  # Use fast sigmoid\n",
        "    lr_decay=0.95,\n",
        "    decay_every=10\n",
        ")\n",
        "\n",
        "print(f\"\\nInitial learning rate: {trainer.lr:.6f}\")\n",
        "print(f\"LR decay factor: {trainer.lr_decay}\")\n",
        "print(f\"Decay every: {trainer.decay_every} epochs\")\n",
        "print(f\"Using fast sigmoid surrogate (alpha=10)\")\n",
        "\n",
        "# ----------------------------\n",
        "# Train/Val split\n",
        "# ----------------------------\n",
        "N = x_train_spikes.shape[0]\n",
        "perm = torch.randperm(N)\n",
        "\n",
        "val_n = int(N * VAL_FRAC)\n",
        "val_idx = perm[:val_n]\n",
        "train_idx = perm[val_n:]\n",
        "\n",
        "x_tr = x_train_spikes[train_idx]\n",
        "y_tr = y_train_small[train_idx]\n",
        "x_val = x_train_spikes[val_idx]\n",
        "y_val = y_train_small[val_idx]\n",
        "\n",
        "print(f\"\\nTrain samples: {x_tr.shape[0]}\")\n",
        "print(f\"Val samples:   {x_val.shape[0]}\")\n",
        "\n",
        "# ----------------------------\n",
        "# Prepare test set with Poisson encoding\n",
        "# ----------------------------\n",
        "print(\"Encoding test data...\")\n",
        "x_test_spikes = poisson_encoding(x_test, TIME_STEPS, max_rate=0.8)\n",
        "print(f\"Test spikes shape: {tuple(x_test_spikes.shape)}\")\n",
        "\n",
        "# ----------------------------\n",
        "# Evaluation helper\n",
        "# ----------------------------\n",
        "@torch.no_grad()\n",
        "def evaluate(model, x_time, y_onehot, batch_size=256):\n",
        "    model.eval()\n",
        "    n = x_time.shape[0]\n",
        "    total_loss = 0.0\n",
        "    total_acc = 0.0\n",
        "    nb = 0\n",
        "    eps = 1e-9\n",
        "\n",
        "    for i in range(0, n, batch_size):\n",
        "        xb = x_time[i:i+batch_size].to(device)\n",
        "        yb = y_onehot[i:i+batch_size].to(device)\n",
        "\n",
        "        h1_spikes, h2_spikes, h3_spikes, y_spikes = model(xb)\n",
        "\n",
        "        out_rates = trainer.compute_spike_rate(y_spikes)\n",
        "        out_rates_norm = out_rates / (out_rates.sum(dim=1, keepdim=True) + eps)\n",
        "        out_probs = F.softmax(out_rates_norm / 0.5, dim=1)\n",
        "\n",
        "        loss = -(yb * torch.log(out_probs + eps)).sum(dim=1).mean()\n",
        "\n",
        "        preds = torch.argmax(out_probs, dim=1)\n",
        "        true  = torch.argmax(yb, dim=1)\n",
        "        acc = (preds == true).float().mean().item()\n",
        "\n",
        "        total_loss += loss.item()\n",
        "        total_acc  += acc\n",
        "        nb += 1\n",
        "\n",
        "    return total_loss / nb, total_acc / nb\n",
        "\n",
        "# ----------------------------\n",
        "# Training loop with val/test each epoch\n",
        "# ----------------------------\n",
        "print(\"\\n\" + \"=\"*60)\n",
        "print(\"STARTING TRAINING\")\n",
        "print(\"=\"*60)\n",
        "\n",
        "history = {\n",
        "    \"train_loss\": [], \"train_acc\": [],\n",
        "    \"val_loss\": [], \"val_acc\": [],\n",
        "    \"test_loss\": [], \"test_acc\": []\n",
        "}\n",
        "\n",
        "best_test_acc = 0.0\n",
        "\n",
        "for ep in range(EPOCHS):\n",
        "    # --- train one epoch ---\n",
        "    model.train()\n",
        "    perm = torch.randperm(x_tr.shape[0])\n",
        "\n",
        "    ep_loss = 0.0\n",
        "    ep_acc = 0.0\n",
        "    nb = 0\n",
        "\n",
        "    for i in range(0, x_tr.shape[0], BATCH_SIZE):\n",
        "        idx = perm[i:i+BATCH_SIZE]\n",
        "        loss, acc = trainer.train_step(x_tr[idx], y_tr[idx])\n",
        "        ep_loss += loss\n",
        "        ep_acc += acc\n",
        "        nb += 1\n",
        "\n",
        "    train_loss = ep_loss / nb\n",
        "    train_acc  = ep_acc / nb\n",
        "\n",
        "    # --- eval ---\n",
        "    val_loss, val_acc = evaluate(model, x_val, y_val, batch_size=EVAL_BATCH)\n",
        "    test_loss, test_acc = evaluate(model, x_test_spikes, y_test, batch_size=EVAL_BATCH)\n",
        "\n",
        "    history[\"train_loss\"].append(train_loss)\n",
        "    history[\"train_acc\"].append(train_acc)\n",
        "    history[\"val_loss\"].append(val_loss)\n",
        "    history[\"val_acc\"].append(val_acc)\n",
        "    history[\"test_loss\"].append(test_loss)\n",
        "    history[\"test_acc\"].append(test_acc)\n",
        "\n",
        "    # Track best\n",
        "    if test_acc > best_test_acc:\n",
        "        best_test_acc = test_acc\n",
        "\n",
        "    print(f\"Epoch {ep+1:3d}/{EPOCHS} | \"\n",
        "          f\"train loss {train_loss:.4f} acc {train_acc:.4f} | \"\n",
        "          f\"val loss {val_loss:.4f} acc {val_acc:.4f} | \"\n",
        "          f\"test loss {test_loss:.4f} acc {test_acc:.4f}\")\n",
        "\n",
        "    # Decay learning rate\n",
        "    trainer.step_lr()\n",
        "\n",
        "print(f\"\\n{'='*60}\")\n",
        "print(f\"TRAINING COMPLETE - Best test accuracy: {best_test_acc:.4f} ({best_test_acc*100:.2f}%)\")\n",
        "print(f\"{'='*60}\")\n",
        "\n",
        "# ----------------------------\n",
        "# Plots: Accuracy + Loss\n",
        "# ----------------------------\n",
        "epochs = np.arange(1, EPOCHS + 1)\n",
        "\n",
        "plt.figure(figsize=(14, 5))\n",
        "\n",
        "plt.subplot(1, 2, 1)\n",
        "plt.plot(epochs, history[\"train_acc\"], linewidth=2, label=\"train acc\", alpha=0.7)\n",
        "plt.plot(epochs, history[\"val_acc\"], linewidth=2, label=\"val acc\", alpha=0.7)\n",
        "plt.plot(epochs, history[\"test_acc\"], linewidth=2, label=\"test acc\", alpha=0.9)\n",
        "plt.axhline(y=best_test_acc, color='r', linestyle='--', alpha=0.5, label=f'best test: {best_test_acc:.4f}')\n",
        "plt.xlabel(\"Epoch\", fontsize=12)\n",
        "plt.ylabel(\"Accuracy\", fontsize=12)\n",
        "plt.title(\"Accuracy vs Epochs (DFA-SNN with Poisson Encoding)\", fontsize=13)\n",
        "plt.grid(True, alpha=0.3)\n",
        "plt.ylim(0, 1.0)\n",
        "plt.legend()\n",
        "\n",
        "plt.subplot(1, 2, 2)\n",
        "plt.plot(epochs, history[\"train_loss\"], linewidth=2, label=\"train loss\", alpha=0.7)\n",
        "plt.plot(epochs, history[\"val_loss\"], linewidth=2, label=\"val loss\", alpha=0.7)\n",
        "plt.plot(epochs, history[\"test_loss\"], linewidth=2, label=\"test loss\", alpha=0.9)\n",
        "plt.xlabel(\"Epoch\", fontsize=12)\n",
        "plt.ylabel(\"Loss\", fontsize=12)\n",
        "plt.title(\"Loss vs Epochs (DFA-SNN with Poisson Encoding)\", fontsize=13)\n",
        "plt.grid(True, alpha=0.3)\n",
        "plt.legend()\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "O3kopaq5oRVa",
        "outputId": "784720f1-7bed-4103-afee-af6bd877aa78"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Using device: cuda\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 9.91M/9.91M [00:00<00:00, 45.7MB/s]\n",
            "100%|██████████| 28.9k/28.9k [00:00<00:00, 1.13MB/s]\n",
            "100%|██████████| 1.65M/1.65M [00:00<00:00, 10.5MB/s]\n",
            "100%|██████████| 4.54k/4.54k [00:00<00:00, 8.99MB/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Training samples: 60000\n",
            "Test samples: 10000\n",
            "\n",
            "============================================================\n",
            "ENCODING INPUTS WITH POISSON PROCESS\n",
            "============================================================\n",
            "Encoding training data...\n",
            "Input shape with time: (60000, 25, 784)\n",
            "  [batch=60000, time=25, features=784]\n",
            "\n",
            "============================================================\n",
            "BUILDING DFA-SNN MODEL\n",
            "============================================================\n",
            "DFASNN(\n",
            "  (hidden_layer1): DFA_LIFLayer()\n",
            "  (hidden_layer2): DFA_LIFLayer()\n",
            "  (hidden_layer3): DFA_LIFLayer()\n",
            "  (output_layer): DFA_LIFLayer()\n",
            ")\n",
            "\n",
            "============================================================\n",
            "INITIALIZING DFA TRAINER\n",
            "============================================================\n",
            "\n",
            "Initial learning rate: 0.100000\n",
            "LR decay factor: 0.95\n",
            "Decay every: 10 epochs\n",
            "Using fast sigmoid surrogate (alpha=10)\n",
            "\n",
            "Train samples: 54000\n",
            "Val samples:   6000\n",
            "Encoding test data...\n",
            "Test spikes shape: (10000, 25, 784)\n",
            "\n",
            "============================================================\n",
            "STARTING TRAINING\n",
            "============================================================\n",
            "Epoch   1/50 | train loss 2.3024 acc 0.0992 | val loss 2.3034 acc 0.0938 | test loss 2.3030 acc 0.0972\n",
            "Epoch   2/50 | train loss 2.3025 acc 0.1049 | val loss 2.3019 acc 0.1168 | test loss 2.3019 acc 0.1138\n",
            "Epoch   3/50 | train loss 2.3034 acc 0.1103 | val loss 2.3024 acc 0.1168 | test loss 2.3024 acc 0.1138\n",
            "Epoch   4/50 | train loss 2.3034 acc 0.1106 | val loss 2.3024 acc 0.1168 | test loss 2.3026 acc 0.1138\n",
            "Epoch   5/50 | train loss 2.3028 acc 0.1107 | val loss 2.3034 acc 0.0938 | test loss 2.3033 acc 0.0972\n",
            "Epoch   6/50 | train loss 2.3025 acc 0.1065 | val loss 2.3031 acc 0.0938 | test loss 2.3031 acc 0.0972\n",
            "Epoch   7/50 | train loss 2.3048 acc 0.0922 | val loss 2.3081 acc 0.0278 | test loss 2.3083 acc 0.0271\n",
            "Epoch   8/50 | train loss 2.3040 acc 0.0899 | val loss 2.3026 acc 0.0938 | test loss 2.3027 acc 0.0970\n",
            "Epoch   9/50 | train loss 2.3030 acc 0.0993 | val loss 2.3026 acc 0.0938 | test loss 2.3026 acc 0.0972\n",
            "Epoch  10/50 | train loss 2.3029 acc 0.0991 | val loss 2.3026 acc 0.0936 | test loss 2.3026 acc 0.0969\n",
            "  → Learning rate decayed to 0.095000\n",
            "Epoch  11/50 | train loss 2.3030 acc 0.0990 | val loss 2.3027 acc 0.0938 | test loss 2.3026 acc 0.0974\n",
            "Epoch  12/50 | train loss 2.3029 acc 0.1000 | val loss 2.3042 acc 0.0948 | test loss 2.3033 acc 0.0994\n",
            "Epoch  13/50 | train loss 2.3016 acc 0.1172 | val loss 2.3012 acc 0.1168 | test loss 2.3023 acc 0.1138\n",
            "Epoch  14/50 | train loss 2.3020 acc 0.1377 | val loss 2.3065 acc 0.1382 | test loss 2.3067 acc 0.1431\n",
            "Epoch  15/50 | train loss 2.3063 acc 0.1276 | val loss 2.3069 acc 0.1168 | test loss 2.3073 acc 0.1138\n",
            "Epoch  16/50 | train loss 2.3074 acc 0.1175 | val loss 2.3076 acc 0.1168 | test loss 2.3093 acc 0.1138\n",
            "Epoch  17/50 | train loss 2.3100 acc 0.1073 | val loss 2.3115 acc 0.0836 | test loss 2.3131 acc 0.0809\n",
            "Epoch  18/50 | train loss 2.3109 acc 0.0863 | val loss 2.3092 acc 0.1168 | test loss 2.3104 acc 0.1138\n",
            "Epoch  19/50 | train loss 2.3103 acc 0.0645 | val loss 2.3102 acc 0.0427 | test loss 2.3106 acc 0.0377\n",
            "Epoch  20/50 | train loss 2.3094 acc 0.0485 | val loss 2.3104 acc 0.0629 | test loss 2.3106 acc 0.0613\n",
            "  → Learning rate decayed to 0.090250\n",
            "Epoch  21/50 | train loss 2.3069 acc 0.0366 | val loss 2.3080 acc 0.0304 | test loss 2.3073 acc 0.0322\n",
            "Epoch  22/50 | train loss 2.3065 acc 0.0390 | val loss 2.3081 acc 0.0320 | test loss 2.3070 acc 0.0322\n",
            "Epoch  23/50 | train loss 2.3073 acc 0.0524 | val loss 2.3084 acc 0.0675 | test loss 2.3077 acc 0.0652\n",
            "Epoch  24/50 | train loss 2.3067 acc 0.0929 | val loss 2.3066 acc 0.1067 | test loss 2.3054 acc 0.1084\n",
            "Epoch  25/50 | train loss 2.3032 acc 0.1245 | val loss 2.3020 acc 0.1316 | test loss 2.3024 acc 0.1247\n",
            "Epoch  26/50 | train loss 2.3006 acc 0.1366 | val loss 2.3015 acc 0.1320 | test loss 2.3016 acc 0.1303\n",
            "Epoch  27/50 | train loss 2.3023 acc 0.1242 | val loss 2.3028 acc 0.1143 | test loss 2.3018 acc 0.1197\n",
            "Epoch  28/50 | train loss 2.3020 acc 0.1141 | val loss 2.3031 acc 0.1029 | test loss 2.3020 acc 0.1045\n",
            "Epoch  29/50 | train loss 2.3027 acc 0.1043 | val loss 2.3037 acc 0.0975 | test loss 2.3030 acc 0.0996\n",
            "Epoch  30/50 | train loss 2.3030 acc 0.1014 | val loss 2.3031 acc 0.0971 | test loss 2.3024 acc 0.1008\n",
            "  → Learning rate decayed to 0.085737\n",
            "Epoch  31/50 | train loss 2.3023 acc 0.1013 | val loss 2.3024 acc 0.0959 | test loss 2.3017 acc 0.0983\n",
            "Epoch  32/50 | train loss 2.3021 acc 0.1005 | val loss 2.3029 acc 0.0918 | test loss 2.3025 acc 0.0952\n",
            "Epoch  33/50 | train loss 2.3024 acc 0.0982 | val loss 2.3020 acc 0.0936 | test loss 2.3009 acc 0.0978\n",
            "Epoch  34/50 | train loss 2.3007 acc 0.1011 | val loss 2.3002 acc 0.0982 | test loss 2.2994 acc 0.1012\n"
          ]
        }
      ]
    }
  ]
}