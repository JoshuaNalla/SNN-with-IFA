{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Spiking Neural Network (SNN) - PyTorch Translation\n",
        "\n",
        "This notebook translates a TensorFlow/Keras SNN implementation to PyTorch.\n",
        "Each cell includes detailed explanations of the translation process and significance."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Cell 1: Import Libraries\n",
        "\n",
        "**Translation Details:**\n",
        "- `tensorflow` â†’ `torch` (main deep learning framework)\n",
        "- `keras` â†’ `torch.nn` (neural network modules)\n",
        "- `numpy` â†’ kept the same (numerical operations)\n",
        "- `matplotlib.pyplot` â†’ kept the same (visualization)\n",
        "\n",
        "**Significance:**\n",
        "PyTorch uses a more explicit, imperative programming style compared to Keras's declarative approach.\n",
        "We'll need to handle autograd and custom gradients differently."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from torch.utils.data import DataLoader, TensorDataset\n",
        "\n",
        "# Set device (GPU if available)\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "print(f\"Using device: {device}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Cell 2: Define Surrogate Gradient Spike Function\n",
        "\n",
        "**Translation Details:**\n",
        "- TensorFlow's `@tf.custom_gradient` â†’ PyTorch's `torch.autograd.Function`\n",
        "- `tf.greater()` â†’ simple comparison `>`\n",
        "- `tf.cast()` â†’ `.float()`\n",
        "- TensorFlow's nested gradient function â†’ PyTorch's `backward()` method\n",
        "\n",
        "**Significance:**\n",
        "This is the **CRITICAL** component for training SNNs:\n",
        "- **Forward pass:** Uses Heaviside step function (hard threshold) to generate actual spikes\n",
        "- **Backward pass:** Uses a smooth \"fast sigmoid\" surrogate gradient to enable gradient flow\n",
        "- **Why needed:** The Heaviside function has zero gradient everywhere (except undefined at threshold),\n",
        "  making it impossible to train with standard backpropagation. The surrogate gradient solves this!\n",
        "\n",
        "**Mathematical Background:**\n",
        "- Forward: spike = 1 if v > threshold, else 0\n",
        "- Backward: âˆ‚spike/âˆ‚v â‰ˆ 1/(1 + |Î±(v - threshold)|)Â² (fast sigmoid derivative)\n",
        "- Î± = 10 controls the steepness of the surrogate gradient"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "class SpikeFunction(torch.autograd.Function):\n",
        "    \"\"\"\n",
        "    Custom autograd function for spike generation with surrogate gradient.\n",
        "    \n",
        "    This is the key innovation that makes SNNs trainable with backprop!\n",
        "    \"\"\"\n",
        "    \n",
        "    @staticmethod\n",
        "    def forward(ctx, v, threshold):\n",
        "        \"\"\"\n",
        "        Forward pass: Generate actual spikes using hard threshold (Heaviside).\n",
        "        \n",
        "        Args:\n",
        "            v: Membrane potential tensor [batch, neurons]\n",
        "            threshold: Spike threshold value\n",
        "            \n",
        "        Returns:\n",
        "            spikes: Binary spike tensor (1 if v > threshold, else 0)\n",
        "        \"\"\"\n",
        "        # Save for backward pass\n",
        "        ctx.save_for_backward(v)\n",
        "        ctx.threshold = threshold\n",
        "        \n",
        "        # Generate spikes: 1 if voltage exceeds threshold, 0 otherwise\n",
        "        spikes = (v > threshold).float()\n",
        "        return spikes\n",
        "    \n",
        "    @staticmethod\n",
        "    def backward(ctx, grad_output):\n",
        "        \"\"\"\n",
        "        Backward pass: Use fast sigmoid surrogate gradient.\n",
        "        \n",
        "        This is what enables training! We replace the non-differentiable\n",
        "        Heaviside function with a smooth approximation during backprop.\n",
        "        \n",
        "        Args:\n",
        "            grad_output: Gradient flowing back from next layer\n",
        "            \n",
        "        Returns:\n",
        "            grad_v: Gradient with respect to membrane potential\n",
        "            grad_threshold: None (threshold is constant)\n",
        "        \"\"\"\n",
        "        v, = ctx.saved_tensors\n",
        "        threshold = ctx.threshold\n",
        "        \n",
        "        # Surrogate gradient parameters\n",
        "        alpha = 10.0  # Steepness of surrogate gradient\n",
        "        \n",
        "        # Compute distance from threshold\n",
        "        v_shifted = v - threshold\n",
        "        \n",
        "        # Fast sigmoid surrogate gradient: 1 / (1 + |alpha * (v - threshold)|)^2\n",
        "        surrogate_grad = 1.0 / (1.0 + torch.abs(alpha * v_shifted)) ** 2\n",
        "        \n",
        "        # Apply chain rule: multiply by incoming gradient\n",
        "        grad_v = grad_output * surrogate_grad\n",
        "        \n",
        "        # Threshold is constant, so no gradient\n",
        "        return grad_v, None\n",
        "\n",
        "# Create a functional interface for easier use\n",
        "spike_fn = SpikeFunction.apply\n",
        "\n",
        "print(\"âœ“ Surrogate gradient spike function defined!\")\n",
        "print(\"Forward: Hard threshold (Heaviside step function)\")\n",
        "print(\"Backward: Smooth fast sigmoid gradient (Î±=10)\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Cell 3: Define LIF (Leaky Integrate-and-Fire) Layer\n",
        "\n",
        "**Translation Details:**\n",
        "- `keras.layers.Layer` â†’ `nn.Module`\n",
        "- `self.add_weight()` â†’ `nn.Parameter()` or `nn.Linear()`\n",
        "- `tf.while_loop()` â†’ standard Python `for` loop (PyTorch handles dynamic graphs automatically)\n",
        "- `tf.TensorArray` â†’ Python list (PyTorch is more flexible with dynamic operations)\n",
        "- `build()` method â†’ handled in `__init__()` in PyTorch\n",
        "\n",
        "**Significance:**\n",
        "The LIF neuron is a simplified biological neuron model:\n",
        "1. **Leaky Integration:** Membrane potential decays over time (models ion channels)\n",
        "2. **Input Current:** Weighted sum of inputs charges the neuron\n",
        "3. **Spike Generation:** When voltage crosses threshold, neuron fires\n",
        "4. **Reset:** After spiking, voltage resets to zero (refractory period)\n",
        "\n",
        "**Equation:** dv/dt = -v/Ï„ + I_in\n",
        "- Ï„ (tau): time constant controlling decay rate\n",
        "- Discrete approximation: v = v*(1-1/Ï„) + I_in"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "class LIFLayer(nn.Module):\n",
        "    \"\"\"\n",
        "    Leaky Integrate-and-Fire neuron layer with surrogate gradient training.\n",
        "    \n",
        "    This layer simulates biological spiking neurons that:\n",
        "    - Integrate input over time (like dendrites collecting signals)\n",
        "    - Leak charge gradually (like ion channels)\n",
        "    - Fire spikes when voltage exceeds threshold (like axon action potentials)\n",
        "    - Reset after spiking (like refractory period)\n",
        "    \"\"\"\n",
        "    \n",
        "    def __init__(self, input_dim, output_dim, tau=20.0, threshold=1.0):\n",
        "        \"\"\"\n",
        "        Initialize the LIF layer.\n",
        "        \n",
        "        Args:\n",
        "            input_dim: Number of input features\n",
        "            output_dim: Number of neurons in this layer\n",
        "            tau: Membrane time constant (higher = slower decay)\n",
        "            threshold: Voltage threshold for spike generation\n",
        "        \"\"\"\n",
        "        super(LIFLayer, self).__init__()\n",
        "        \n",
        "        # Store neuron parameters\n",
        "        self.input_dim = input_dim\n",
        "        self.output_dim = output_dim\n",
        "        self.tau = tau\n",
        "        self.threshold = threshold\n",
        "        \n",
        "        # Create fully connected layer (weight matrix + bias)\n",
        "        # PyTorch's Linear layer handles both W and b\n",
        "        self.fc = nn.Linear(input_dim, output_dim)\n",
        "        \n",
        "        # Initialize weights using Xavier/Glorot uniform (same as TensorFlow default)\n",
        "        nn.init.xavier_uniform_(self.fc.weight)\n",
        "        nn.init.zeros_(self.fc.bias)\n",
        "    \n",
        "    def forward(self, x):\n",
        "        \"\"\"\n",
        "        Forward pass: Simulate LIF neuron dynamics over time.\n",
        "        \n",
        "        Args:\n",
        "            x: Input spike trains [batch, time_steps, input_dim]\n",
        "            \n",
        "        Returns:\n",
        "            Output spike trains [batch, time_steps, output_dim]\n",
        "        \"\"\"\n",
        "        batch_size, time_steps, _ = x.shape\n",
        "        \n",
        "        # Initialize membrane potential to zero\n",
        "        v = torch.zeros(batch_size, self.output_dim, device=x.device)\n",
        "        \n",
        "        # Store output spikes for each time step\n",
        "        spikes_list = []\n",
        "        \n",
        "        # Simulate neuron dynamics over time\n",
        "        # PyTorch handles dynamic graphs, so we can use regular for loops!\n",
        "        for t in range(time_steps):\n",
        "            # Extract input at current time step\n",
        "            x_t = x[:, t, :]  # Shape: [batch, input_dim]\n",
        "            \n",
        "            # Compute input current: I = W*x + b\n",
        "            i_in = self.fc(x_t)  # Shape: [batch, output_dim]\n",
        "            \n",
        "            # Update membrane potential with leaky integration\n",
        "            # v(t+1) = v(t) * (1 - 1/tau) + I_in(t)\n",
        "            # The (1 - 1/tau) term causes exponential decay\n",
        "            v = v * (1.0 - 1.0/self.tau) + i_in\n",
        "            \n",
        "            # Generate spikes using surrogate gradient function\n",
        "            # Forward: hard threshold, Backward: smooth gradient\n",
        "            spike = spike_fn(v, self.threshold)\n",
        "            \n",
        "            # Reset membrane potential for neurons that spiked\n",
        "            # v = v * (1 - spike) sets spiking neurons to 0\n",
        "            v = v * (1.0 - spike)\n",
        "            \n",
        "            # Store spikes for this time step\n",
        "            spikes_list.append(spike)\n",
        "        \n",
        "        # Stack spikes along time dimension\n",
        "        # List of [batch, output_dim] â†’ [batch, time_steps, output_dim]\n",
        "        spikes = torch.stack(spikes_list, dim=1)\n",
        "        \n",
        "        return spikes\n",
        "\n",
        "print(\"âœ“ LIF Layer defined!\")\n",
        "print(\"Key features:\")\n",
        "print(\"  - Leaky integration (exponential decay with tau={:.1f})\".format(20.0))\n",
        "print(\"  - Spike generation at threshold={:.1f}\".format(1.0))\n",
        "print(\"  - Hard reset after spiking\")\n",
        "print(\"  - Surrogate gradient for backprop\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Cell 4: Build 2-Layer SNN Model\n",
        "\n",
        "**Translation Details:**\n",
        "- Keras functional API â†’ PyTorch `nn.Module` with `nn.Sequential`\n",
        "- `keras.Input()` â†’ handled implicitly in PyTorch's forward pass\n",
        "- `keras.Model()` â†’ custom `nn.Module` class\n",
        "\n",
        "**Significance:**\n",
        "This creates a simple feedforward SNN with:\n",
        "- **Input layer:** Receives spike-encoded MNIST images [batch, 20, 784]\n",
        "- **Hidden layer:** 128 LIF neurons for feature extraction\n",
        "- **Output layer:** 10 LIF neurons (one per digit class)\n",
        "\n",
        "The network processes temporal information (20 time steps) to classify digits.\n",
        "Unlike standard ANNs, SNNs process sequences of spikes over time!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "class SpikingNN(nn.Module):\n",
        "    \"\"\"\n",
        "    2-layer Spiking Neural Network for MNIST classification.\n",
        "    \n",
        "    Architecture:\n",
        "    Input [784] â†’ LIF Hidden [128] â†’ LIF Output [10]\n",
        "    \n",
        "    All layers process spike trains over time (temporal coding).\n",
        "    \"\"\"\n",
        "    \n",
        "    def __init__(self, input_dim=784, hidden_dim=128, output_dim=10, tau=20.0, threshold=1.0):\n",
        "        \"\"\"\n",
        "        Initialize the spiking neural network.\n",
        "        \n",
        "        Args:\n",
        "            input_dim: Input feature dimension (784 for flattened MNIST)\n",
        "            hidden_dim: Number of neurons in hidden layer\n",
        "            output_dim: Number of output classes\n",
        "            tau: Membrane time constant for all neurons\n",
        "            threshold: Spike threshold for all neurons\n",
        "        \"\"\"\n",
        "        super(SpikingNN, self).__init__()\n",
        "        \n",
        "        # First LIF layer (input â†’ hidden)\n",
        "        self.lif1 = LIFLayer(input_dim, hidden_dim, tau=tau, threshold=threshold)\n",
        "        \n",
        "        # Second LIF layer (hidden â†’ output)\n",
        "        self.lif2 = LIFLayer(hidden_dim, output_dim, tau=tau, threshold=threshold)\n",
        "    \n",
        "    def forward(self, x):\n",
        "        \"\"\"\n",
        "        Forward pass through the spiking network.\n",
        "        \n",
        "        Args:\n",
        "            x: Input spike trains [batch, time_steps, input_dim]\n",
        "            \n",
        "        Returns:\n",
        "            Output spike trains [batch, time_steps, output_dim]\n",
        "        \"\"\"\n",
        "        # Pass through first LIF layer\n",
        "        h = self.lif1(x)  # [batch, time_steps, hidden_dim]\n",
        "        \n",
        "        # Pass through second LIF layer\n",
        "        out = self.lif2(h)  # [batch, time_steps, output_dim]\n",
        "        \n",
        "        return out\n",
        "\n",
        "# Create model instance\n",
        "model = SpikingNN(input_dim=784, hidden_dim=128, output_dim=10).to(device)\n",
        "\n",
        "# Print model summary\n",
        "print(\"âœ“ Spiking Neural Network created!\")\n",
        "print(\"\\nModel Architecture:\")\n",
        "print(model)\n",
        "print(\"\\nTotal parameters:\", sum(p.numel() for p in model.parameters()))\n",
        "print(\"Trainable parameters:\", sum(p.numel() for p in model.parameters() if p.requires_grad))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Cell 5: Rate Encoding Function\n",
        "\n",
        "**Translation Details:**\n",
        "- `np.random.rand()` â†’ kept the same (NumPy is fine for preprocessing)\n",
        "- `np.newaxis` â†’ same in NumPy\n",
        "- Logic remains identical\n",
        "\n",
        "**Significance:**\n",
        "**Rate coding** converts continuous values to spike trains:\n",
        "- Higher pixel intensity â†’ Higher firing rate (more spikes)\n",
        "- Lower pixel intensity â†’ Lower firing rate (fewer spikes)\n",
        "\n",
        "**How it works:**\n",
        "1. Generate random numbers uniformly in [0, 1] for each pixel at each time step\n",
        "2. If random < (pixel_value * max_rate), generate a spike\n",
        "3. Result: Pixel value of 0.8 â†’ ~80% chance of spike per time step\n",
        "\n",
        "**Example:** \n",
        "- Dark pixel (0.1) â†’ ~1-2 spikes over 20 time steps\n",
        "- Bright pixel (0.9) â†’ ~14-16 spikes over 20 time steps\n",
        "\n",
        "This is one way neurons encode information in the brain!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def rate_encode(data, time_steps, max_rate=1.0):\n",
        "    \"\"\"\n",
        "    Convert static data to spike trains using rate coding.\n",
        "    \n",
        "    Rate coding: Encode continuous values as firing rates (spikes/time).\n",
        "    Higher values â†’ More spikes (mimics biological neural coding).\n",
        "    \n",
        "    Args:\n",
        "        data: Input data [batch, features] with values in [0, 1]\n",
        "        time_steps: Number of time steps for spike train\n",
        "        max_rate: Maximum firing rate (probability per time step)\n",
        "        \n",
        "    Returns:\n",
        "        Spike trains [batch, time_steps, features] with binary spikes (0 or 1)\n",
        "    \"\"\"\n",
        "    # Ensure data is in valid range [0, 1]\n",
        "    data = np.clip(data, 0, 1)\n",
        "    \n",
        "    # Generate random numbers for probabilistic spike generation\n",
        "    # Shape: [batch, time_steps, features]\n",
        "    rand = np.random.rand(data.shape[0], time_steps, data.shape[1])\n",
        "    \n",
        "    # Generate spike if random < pixel_value * max_rate\n",
        "    # data[:, np.newaxis, :] broadcasts to [batch, time_steps, features]\n",
        "    # Higher pixel values â†’ Higher spike probability\n",
        "    spikes = (rand < data[:, np.newaxis, :] * max_rate).astype(np.float32)\n",
        "    \n",
        "    return spikes\n",
        "\n",
        "print(\"âœ“ Rate encoding function defined!\")\n",
        "print(\"\\nExample: pixel value 0.8 with max_rate=1.0 over 20 time steps\")\n",
        "print(\"  â†’ Expected ~16 spikes (0.8 * 20 = 16)\")\n",
        "print(\"  â†’ Each time step has 80% chance of spike\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Cell 6: Load and Prepare MNIST Dataset\n",
        "\n",
        "**Translation Details:**\n",
        "- `keras.datasets.mnist` â†’ `torchvision.datasets.MNIST`\n",
        "- `keras.utils.to_categorical()` â†’ PyTorch keeps labels as integers (cross entropy handles it)\n",
        "- Manual normalization â†’ same approach\n",
        "\n",
        "**Significance:**\n",
        "MNIST preprocessing steps:\n",
        "1. **Flatten:** 28Ã—28 images â†’ 784-dimensional vectors\n",
        "2. **Normalize:** Pixel values [0, 255] â†’ [0, 1] (helps with training stability)\n",
        "3. **Subset:** Use 1000 samples for faster training/testing\n",
        "4. **One-hot encoding:** Convert labels to probability distributions\n",
        "\n",
        "**Note:** We keep labels as one-hot for compatibility with the original implementation,\n",
        "though PyTorch typically uses integer labels."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Import torchvision for MNIST dataset\n",
        "from torchvision import datasets, transforms\n",
        "\n",
        "# Load MNIST dataset\n",
        "# Note: torchvision automatically downloads to './data' directory\n",
        "train_dataset = datasets.MNIST(root='./data', train=True, download=True)\n",
        "test_dataset = datasets.MNIST(root='./data', train=False, download=True)\n",
        "\n",
        "# Extract data as numpy arrays\n",
        "x_train = train_dataset.data.numpy()\n",
        "y_train = train_dataset.targets.numpy()\n",
        "x_test = test_dataset.data.numpy()\n",
        "y_test = test_dataset.targets.numpy()\n",
        "\n",
        "# Flatten images from 28Ã—28 to 784-dimensional vectors\n",
        "x_train = x_train.reshape(-1, 784).astype('float32')  # [60000, 784]\n",
        "x_test = x_test.reshape(-1, 784).astype('float32')    # [10000, 784]\n",
        "\n",
        "# Normalize pixel values to [0, 1] range\n",
        "# Dividing by 255 (max pixel value) scales to unit interval\n",
        "x_train = x_train / 255.0\n",
        "x_test = x_test / 255.0\n",
        "\n",
        "# Convert labels to one-hot encoding\n",
        "# Example: label 3 â†’ [0, 0, 0, 1, 0, 0, 0, 0, 0, 0]\n",
        "def to_categorical(y, num_classes=10):\n",
        "    \"\"\"Convert integer labels to one-hot encoded vectors.\"\"\"\n",
        "    one_hot = np.zeros((y.shape[0], num_classes))\n",
        "    one_hot[np.arange(y.shape[0]), y] = 1\n",
        "    return one_hot.astype('float32')\n",
        "\n",
        "y_train = to_categorical(y_train, 10)  # [60000, 10]\n",
        "y_test = to_categorical(y_test, 10)    # [10000, 10]\n",
        "\n",
        "# Use smaller subset for faster training (you can increase this)\n",
        "num_samples = 1000\n",
        "x_train = x_train[:num_samples]\n",
        "y_train = y_train[:num_samples]\n",
        "\n",
        "print(f\"âœ“ MNIST dataset loaded and preprocessed!\")\n",
        "print(f\"\\nTraining data shape: {x_train.shape}\")\n",
        "print(f\"Training labels shape: {y_train.shape}\")\n",
        "print(f\"Test data shape: {x_test.shape}\")\n",
        "print(f\"Test labels shape: {y_test.shape}\")\n",
        "print(f\"\\nData range: [{x_train.min():.2f}, {x_train.max():.2f}]\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Cell 7: Encode Data as Spike Trains\n",
        "\n",
        "**Translation Details:**\n",
        "- Same logic as TensorFlow version\n",
        "- No translation needed (uses NumPy)\n",
        "\n",
        "**Significance:**\n",
        "This is where we convert static images to temporal spike patterns!\n",
        "\n",
        "**Before encoding:** Each image is 784 continuous values in [0, 1]\n",
        "**After encoding:** Each image is 20 time steps of 784 binary spikes\n",
        "\n",
        "**Visualization:**\n",
        "```\n",
        "Original pixel (value=0.8):\n",
        "[0.8]\n",
        "\n",
        "After rate encoding (20 time steps):\n",
        "t=0: [1]\n",
        "t=1: [1]\n",
        "t=2: [0]  â† No spike (random < 0.8 failed)\n",
        "t=3: [1]\n",
        "...\n",
        "Total: ~16 spikes out of 20 time steps (80% firing rate)\n",
        "```\n",
        "\n",
        "**Why 20 time steps?**\n",
        "- Trade-off between temporal resolution and computational cost\n",
        "- More time steps â†’ Better encoding but slower training\n",
        "- 20 is a reasonable balance for MNIST"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Define temporal parameters\n",
        "time_steps = 20  # Number of simulation time steps\n",
        "\n",
        "# Convert training data to spike trains using rate coding\n",
        "print(\"Encoding training data...\")\n",
        "x_train_spikes = rate_encode(x_train, time_steps, max_rate=0.8)\n",
        "print(f\"âœ“ Training spike trains shape: {x_train_spikes.shape}\")\n",
        "print(f\"  [batch, time_steps, features] = {x_train_spikes.shape}\")\n",
        "\n",
        "# Convert test data to spike trains\n",
        "print(\"\\nEncoding test data...\")\n",
        "x_test_spikes = rate_encode(x_test, time_steps, max_rate=0.8)\n",
        "print(f\"âœ“ Test spike trains shape: {x_test_spikes.shape}\")\n",
        "\n",
        "# Verify spike encoding\n",
        "avg_spikes_per_pixel = x_train_spikes.mean(axis=1).mean()  # Average across time and batch\n",
        "print(f\"\\nAverage spikes per pixel: {avg_spikes_per_pixel:.3f}\")\n",
        "print(f\"Expected: ~{0.8 * x_train.mean():.3f} (max_rate * avg_pixel_value)\")\n",
        "\n",
        "# Convert to PyTorch tensors\n",
        "x_train_spikes = torch.FloatTensor(x_train_spikes)\n",
        "y_train = torch.FloatTensor(y_train)\n",
        "x_test_spikes = torch.FloatTensor(x_test_spikes)\n",
        "y_test = torch.FloatTensor(y_test)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Cell 8: Define Loss Function and Metrics\n",
        "\n",
        "**Translation Details:**\n",
        "- Custom TensorFlow loss function â†’ Custom PyTorch function\n",
        "- `tf.reduce_sum()` â†’ `torch.sum()`\n",
        "- `tf.nn.softmax()` â†’ `F.softmax()`\n",
        "- `keras.losses.categorical_crossentropy()` â†’ `F.cross_entropy()`\n",
        "\n",
        "**Significance:**\n",
        "**Spike-based Classification:**\n",
        "Standard ANNs output probabilities directly. SNNs output spike trains!\n",
        "\n",
        "**How we convert spikes to predictions:**\n",
        "1. **Sum spikes over time:** Count total spikes per output neuron\n",
        "2. **Apply softmax:** Convert spike counts to probability distribution\n",
        "3. **Compute cross-entropy:** Compare to true labels\n",
        "\n",
        "**Example:**\n",
        "```\n",
        "Output spikes for digit 3:\n",
        "Neuron 0: [0,0,1,0,0,...] â†’ 3 total spikes\n",
        "Neuron 1: [0,1,0,0,1,...] â†’ 5 total spikes\n",
        "Neuron 2: [0,0,0,1,0,...] â†’ 4 total spikes\n",
        "Neuron 3: [1,1,1,1,1,...] â†’ 18 total spikes â† HIGHEST!\n",
        "...\n",
        "\n",
        "Spike counts: [3, 5, 4, 18, 2, 1, 3, 4, 2, 1]\n",
        "After softmax: [0.01, 0.02, 0.01, 0.92, 0.00, ...] â† Neuron 3 has highest probability\n",
        "Prediction: Class 3 âœ“\n",
        "```\n",
        "\n",
        "**Why sum over time?**\n",
        "- Integrates temporal information\n",
        "- Neuron that fires most frequently over the trial period wins\n",
        "- Mimics rate coding in biological neurons"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def spike_categorical_crossentropy(y_pred, y_true):\n",
        "    \"\"\"\n",
        "    Loss function for spike-based classification.\n",
        "    \n",
        "    Converts spike trains to class probabilities by:\n",
        "    1. Summing spikes over time (rate coding)\n",
        "    2. Applying softmax normalization\n",
        "    3. Computing cross-entropy loss\n",
        "    \n",
        "    Args:\n",
        "        y_pred: Predicted spike trains [batch, time_steps, num_classes]\n",
        "        y_true: True labels (one-hot) [batch, num_classes]\n",
        "        \n",
        "    Returns:\n",
        "        Scalar loss value\n",
        "    \"\"\"\n",
        "    # Sum spikes across time dimension to get total spike counts\n",
        "    spike_counts = torch.sum(y_pred, dim=1)  # [batch, num_classes]\n",
        "    \n",
        "    # Apply softmax to convert spike counts to probabilities\n",
        "    # This normalizes so that probabilities sum to 1\n",
        "    probs = F.softmax(spike_counts, dim=-1)\n",
        "    \n",
        "    # Compute categorical cross-entropy loss\n",
        "    # -sum(y_true * log(y_pred))\n",
        "    loss = F.cross_entropy(probs, y_true)\n",
        "    \n",
        "    return loss\n",
        "\n",
        "\n",
        "def spike_categorical_accuracy(y_pred, y_true):\n",
        "    \"\"\"\n",
        "    Accuracy metric for spike-based classification.\n",
        "    \n",
        "    Args:\n",
        "        y_pred: Predicted spike trains [batch, time_steps, num_classes]\n",
        "        y_true: True labels (one-hot) [batch, num_classes]\n",
        "        \n",
        "    Returns:\n",
        "        Accuracy as a percentage\n",
        "    \"\"\"\n",
        "    # Sum spikes over time to get spike counts per class\n",
        "    spike_counts = torch.sum(y_pred, dim=1)  # [batch, num_classes]\n",
        "    \n",
        "    # Get predicted class (neuron with most spikes)\n",
        "    y_pred_class = torch.argmax(spike_counts, dim=-1)  # [batch]\n",
        "    \n",
        "    # Get true class from one-hot labels\n",
        "    y_true_class = torch.argmax(y_true, dim=-1)  # [batch]\n",
        "    \n",
        "    # Compute accuracy\n",
        "    correct = (y_pred_class == y_true_class).float()\n",
        "    accuracy = correct.mean() * 100.0  # Convert to percentage\n",
        "    \n",
        "    return accuracy\n",
        "\n",
        "print(\"âœ“ Loss function and metrics defined!\")\n",
        "print(\"\\nLoss: Spike-based categorical cross-entropy\")\n",
        "print(\"  1. Sum spikes over time â†’ spike counts\")\n",
        "print(\"  2. Apply softmax â†’ probabilities\")\n",
        "print(\"  3. Compute cross-entropy loss\")\n",
        "print(\"\\nMetric: Spike-based accuracy\")\n",
        "print(\"  - Neuron with most spikes wins\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Cell 9: Training Setup\n",
        "\n",
        "**Translation Details:**\n",
        "- `model.compile()` â†’ Manual training loop in PyTorch\n",
        "- `keras.optimizers.Adam()` â†’ `torch.optim.Adam()`\n",
        "- `model.fit()` â†’ Custom training loop with DataLoader\n",
        "\n",
        "**Significance:**\n",
        "PyTorch requires more explicit control over training:\n",
        "1. **DataLoader:** Handles batching and shuffling\n",
        "2. **Optimizer:** Updates weights based on gradients\n",
        "3. **Training loop:** Manually iterate through epochs and batches\n",
        "\n",
        "**Training process:**\n",
        "```\n",
        "For each epoch:\n",
        "  For each batch:\n",
        "    1. Forward pass: model(x) â†’ predictions\n",
        "    2. Compute loss: loss_fn(predictions, targets)\n",
        "    3. Backward pass: loss.backward() â†’ compute gradients\n",
        "    4. Update weights: optimizer.step()\n",
        "    5. Reset gradients: optimizer.zero_grad()\n",
        "```\n",
        "\n",
        "**Key difference from TensorFlow:**\n",
        "- TensorFlow/Keras: High-level `fit()` handles everything\n",
        "- PyTorch: Explicit control over each training step"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Training hyperparameters\n",
        "batch_size = 32\n",
        "epochs = 10\n",
        "learning_rate = 0.001\n",
        "validation_split = 0.2\n",
        "\n",
        "# Split training data into train and validation sets\n",
        "num_train = int(len(x_train_spikes) * (1 - validation_split))\n",
        "indices = torch.randperm(len(x_train_spikes))\n",
        "\n",
        "train_indices = indices[:num_train]\n",
        "val_indices = indices[num_train:]\n",
        "\n",
        "x_train_split = x_train_spikes[train_indices]\n",
        "y_train_split = y_train[train_indices]\n",
        "x_val = x_train_spikes[val_indices]\n",
        "y_val = y_train[val_indices]\n",
        "\n",
        "# Create DataLoaders for batching\n",
        "train_dataset = TensorDataset(x_train_split, y_train_split)\n",
        "val_dataset = TensorDataset(x_val, y_val)\n",
        "\n",
        "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
        "val_loader = DataLoader(val_dataset, batch_size=batch_size, shuffle=False)\n",
        "\n",
        "# Initialize optimizer\n",
        "optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)\n",
        "\n",
        "print(\"âœ“ Training setup complete!\")\n",
        "print(f\"\\nTraining samples: {len(x_train_split)}\")\n",
        "print(f\"Validation samples: {len(x_val)}\")\n",
        "print(f\"Batch size: {batch_size}\")\n",
        "print(f\"Learning rate: {learning_rate}\")\n",
        "print(f\"Epochs: {epochs}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Cell 10: Training Loop\n",
        "\n",
        "**Translation Details:**\n",
        "- `model.fit()` â†’ Custom training loop\n",
        "- Automatic progress bars â†’ Manual progress tracking\n",
        "- Built-in validation â†’ Manual validation loop\n",
        "\n",
        "**Significance:**\n",
        "This is the core training loop that:\n",
        "1. **Trains the model** using surrogate gradient descent\n",
        "2. **Validates performance** on held-out data\n",
        "3. **Tracks metrics** (loss and accuracy)\n",
        "\n",
        "**What's happening during training:**\n",
        "- **Forward pass:** Spikes propagate through network (LIF dynamics)\n",
        "- **Loss computation:** Compare spike-based predictions to labels\n",
        "- **Backward pass:** Gradients flow back using surrogate gradients!\n",
        "- **Weight update:** Adjust synaptic weights to improve predictions\n",
        "\n",
        "**The magic:** Even though forward pass uses discontinuous Heaviside function,\n",
        "backward pass uses smooth surrogate gradient, enabling training!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Training history\n",
        "history = {\n",
        "    'train_loss': [],\n",
        "    'train_acc': [],\n",
        "    'val_loss': [],\n",
        "    'val_acc': []\n",
        "}\n",
        "\n",
        "print(\"Starting training...\\n\")\n",
        "print(\"=\" * 70)\n",
        "\n",
        "# Training loop\n",
        "for epoch in range(epochs):\n",
        "    # ==================== Training Phase ====================\n",
        "    model.train()  # Set model to training mode\n",
        "    \n",
        "    train_loss = 0.0\n",
        "    train_correct = 0\n",
        "    train_total = 0\n",
        "    \n",
        "    for batch_idx, (x_batch, y_batch) in enumerate(train_loader):\n",
        "        # Move data to device (GPU if available)\n",
        "        x_batch = x_batch.to(device)\n",
        "        y_batch = y_batch.to(device)\n",
        "        \n",
        "        # Zero gradients from previous iteration\n",
        "        optimizer.zero_grad()\n",
        "        \n",
        "        # Forward pass: Get spike predictions\n",
        "        # x_batch: [batch, time_steps, input_dim]\n",
        "        # outputs: [batch, time_steps, output_dim]\n",
        "        outputs = model(x_batch)\n",
        "        \n",
        "        # Compute loss\n",
        "        loss = spike_categorical_crossentropy(outputs, y_batch)\n",
        "        \n",
        "        # Backward pass: Compute gradients using surrogate gradient\n",
        "        loss.backward()\n",
        "        \n",
        "        # Update weights\n",
        "        optimizer.step()\n",
        "        \n",
        "        # Track metrics\n",
        "        train_loss += loss.item() * x_batch.size(0)\n",
        "        \n",
        "        # Compute accuracy\n",
        "        spike_counts = torch.sum(outputs, dim=1)\n",
        "        pred_class = torch.argmax(spike_counts, dim=-1)\n",
        "        true_class = torch.argmax(y_batch, dim=-1)\n",
        "        train_correct += (pred_class == true_class).sum().item()\n",
        "        train_total += x_batch.size(0)\n",
        "    \n",
        "    # Calculate average training metrics\n",
        "    train_loss = train_loss / train_total\n",
        "    train_acc = 100.0 * train_correct / train_total\n",
        "    \n",
        "    # ==================== Validation Phase ====================\n",
        "    model.eval()  # Set model to evaluation mode\n",
        "    \n",
        "    val_loss = 0.0\n",
        "    val_correct = 0\n",
        "    val_total = 0\n",
        "    \n",
        "    with torch.no_grad():  # Disable gradient computation for validation\n",
        "        for x_batch, y_batch in val_loader:\n",
        "            x_batch = x_batch.to(device)\n",
        "            y_batch = y_batch.to(device)\n",
        "            \n",
        "            # Forward pass\n",
        "            outputs = model(x_batch)\n",
        "            \n",
        "            # Compute loss\n",
        "            loss = spike_categorical_crossentropy(outputs, y_batch)\n",
        "            val_loss += loss.item() * x_batch.size(0)\n",
        "            \n",
        "            # Compute accuracy\n",
        "            spike_counts = torch.sum(outputs, dim=1)\n",
        "            pred_class = torch.argmax(spike_counts, dim=-1)\n",
        "            true_class = torch.argmax(y_batch, dim=-1)\n",
        "            val_correct += (pred_class == true_class).sum().item()\n",
        "            val_total += x_batch.size(0)\n",
        "    \n",
        "    # Calculate average validation metrics\n",
        "    val_loss = val_loss / val_total\n",
        "    val_acc = 100.0 * val_correct / val_total\n",
        "    \n",
        "    # Store history\n",
        "    history['train_loss'].append(train_loss)\n",
        "    history['train_acc'].append(train_acc)\n",
        "    history['val_loss'].append(val_loss)\n",
        "    history['val_acc'].append(val_acc)\n",
        "    \n",
        "    # Print epoch results\n",
        "    print(f\"Epoch {epoch+1}/{epochs}:\")\n",
        "    print(f\"  Train Loss: {train_loss:.4f} | Train Acc: {train_acc:.2f}%\")\n",
        "    print(f\"  Val Loss:   {val_loss:.4f} | Val Acc:   {val_acc:.2f}%\")\n",
        "    print(\"-\" * 70)\n",
        "\n",
        "print(\"\\nâœ“ Training complete!\")\n",
        "print(f\"\\nFinal validation accuracy: {history['val_acc'][-1]:.2f}%\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Cell 11: Visualize Training History\n",
        "\n",
        "**Significance:**\n",
        "Plotting training curves helps us understand:\n",
        "- **Convergence:** Is the model learning?\n",
        "- **Overfitting:** Is validation performance degrading while training improves?\n",
        "- **Stability:** Are the metrics smooth or noisy?\n",
        "\n",
        "**What to look for:**\n",
        "- Train loss should decrease over time\n",
        "- Validation loss should track training loss (if not, overfitting)\n",
        "- Accuracy should increase and plateau\n",
        "- Gap between train/val suggests overfitting or underfitting"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Plot training history\n",
        "fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(14, 5))\n",
        "\n",
        "# Plot loss\n",
        "ax1.plot(history['train_loss'], label='Training Loss', marker='o')\n",
        "ax1.plot(history['val_loss'], label='Validation Loss', marker='s')\n",
        "ax1.set_xlabel('Epoch')\n",
        "ax1.set_ylabel('Loss')\n",
        "ax1.set_title('Training and Validation Loss')\n",
        "ax1.legend()\n",
        "ax1.grid(True, alpha=0.3)\n",
        "\n",
        "# Plot accuracy\n",
        "ax2.plot(history['train_acc'], label='Training Accuracy', marker='o')\n",
        "ax2.plot(history['val_acc'], label='Validation Accuracy', marker='s')\n",
        "ax2.set_xlabel('Epoch')\n",
        "ax2.set_ylabel('Accuracy (%)')\n",
        "ax2.set_title('Training and Validation Accuracy')\n",
        "ax2.legend()\n",
        "ax2.grid(True, alpha=0.3)\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "print(\"\\nðŸ“Š Training curves plotted!\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Cell 12: Test Set Evaluation\n",
        "\n",
        "**Significance:**\n",
        "Final evaluation on held-out test set to assess generalization.\n",
        "This tells us how well the SNN performs on completely unseen data."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Evaluate on test set\n",
        "model.eval()\n",
        "\n",
        "test_dataset = TensorDataset(x_test_spikes, y_test)\n",
        "test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False)\n",
        "\n",
        "test_loss = 0.0\n",
        "test_correct = 0\n",
        "test_total = 0\n",
        "\n",
        "print(\"Evaluating on test set...\\n\")\n",
        "\n",
        "with torch.no_grad():\n",
        "    for x_batch, y_batch in test_loader:\n",
        "        x_batch = x_batch.to(device)\n",
        "        y_batch = y_batch.to(device)\n",
        "        \n",
        "        # Forward pass\n",
        "        outputs = model(x_batch)\n",
        "        \n",
        "        # Compute loss\n",
        "        loss = spike_categorical_crossentropy(outputs, y_batch)\n",
        "        test_loss += loss.item() * x_batch.size(0)\n",
        "        \n",
        "        # Compute accuracy\n",
        "        spike_counts = torch.sum(outputs, dim=1)\n",
        "        pred_class = torch.argmax(spike_counts, dim=-1)\n",
        "        true_class = torch.argmax(y_batch, dim=-1)\n",
        "        test_correct += (pred_class == true_class).sum().item()\n",
        "        test_total += x_batch.size(0)\n",
        "\n",
        "test_loss = test_loss / test_total\n",
        "test_acc = 100.0 * test_correct / test_total\n",
        "\n",
        "print(\"=\" * 50)\n",
        "print(\"TEST SET RESULTS\")\n",
        "print(\"=\" * 50)\n",
        "print(f\"Test Loss: {test_loss:.4f}\")\n",
        "print(f\"Test Accuracy: {test_acc:.2f}%\")\n",
        "print(\"=\" * 50)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Summary: Key Translation Points\n",
        "\n",
        "### 1. **Surrogate Gradient Function**\n",
        "- **TensorFlow:** `@tf.custom_gradient` decorator\n",
        "- **PyTorch:** `torch.autograd.Function` class with `forward()` and `backward()`\n",
        "\n",
        "### 2. **Custom Layers**\n",
        "- **TensorFlow:** Inherit from `keras.layers.Layer`, implement `build()` and `call()`\n",
        "- **PyTorch:** Inherit from `nn.Module`, implement `__init__()` and `forward()`\n",
        "\n",
        "### 3. **Dynamic Loops**\n",
        "- **TensorFlow:** `tf.while_loop()` with `TensorArray`\n",
        "- **PyTorch:** Regular Python `for` loops (dynamic graph!)\n",
        "\n",
        "### 4. **Model Building**\n",
        "- **TensorFlow:** Functional API with `keras.Input()` and `keras.Model()`\n",
        "- **PyTorch:** Custom `nn.Module` with sequential layers\n",
        "\n",
        "### 5. **Training**\n",
        "- **TensorFlow:** High-level `model.compile()` and `model.fit()`\n",
        "- **PyTorch:** Manual training loop with `optimizer.step()`\n",
        "\n",
        "### Key Advantages of PyTorch Version:\n",
        "1. **More explicit:** Easier to debug and understand what's happening\n",
        "2. **More flexible:** Simpler to modify training loop or add custom behavior\n",
        "3. **Dynamic graphs:** No need for `tf.TensorArray` or complex control flow\n",
        "4. **Cleaner code:** Python loops instead of graph operations\n",
        "\n",
        "### Preserved Functionality:\n",
        "- âœ“ Surrogate gradient training\n",
        "- âœ“ LIF neuron dynamics\n",
        "- âœ“ Rate-based spike encoding\n",
        "- âœ“ Spike-based classification\n",
        "- âœ“ All hyperparameters and architecture"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.0"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 4
}
